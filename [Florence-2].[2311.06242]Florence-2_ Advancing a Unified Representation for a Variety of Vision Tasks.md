# Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks

## 0. Abstract

| ã€æ¦‚è¿°ã€‘åŸæ–‡ | ã€æ¦‚è¿°ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We introduce Florence-2, a novel vision foundation model with a unified, prompt-based representation for a variety of computer vision and vision-language tasks. While existing large vision models excel in transfer learning, they struggle to perform a diversity of tasks with simple instructions, a capability that implies handling the complexity of various spatial hierarchy and semantic granularity. Florence-2 was designed to take text-prompt as task instructions and generate desirable results in text forms, whether it be captioning, object detection, grounding or segmentation. This multi-task learning setup demands large-scale, high-quality annotated data. To this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive visual annotations on 126 million images, using an iterative strategy of automated image annotation and model refinement. We adopted a sequence-to-sequence structure to train Florence-2 to perform versatile and comprehensive vision tasks. Extensive evaluations on numerous tasks demonstrated Florence-2 to be a strong vision foundation model contender with unprecedented zero-shot and fine-tuning capabilities. | âœ… We introduce Florence-2, a novel vision foundation model with a unified, prompt-based representation for a variety of computer vision and vision-language tasks. While existing large vision models excel in transfer learning, they struggle to perform a diversity of tasks with simple instructions, a capability that implies handling the complexity of various spatial hierarchy and semantic granularity. Florence-2 was designed to take text-prompt as task instructions and generate desirable results in text forms, whether it be captioning, object detection, grounding or segmentation. This multi-task learning setup demands large-scale, high-quality annotated data. To this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive visual annotations on 126 million images, using an iterative strategy of automated image annotation and model refinement. We adopted a sequence-to-sequence structure to train Florence-2 to perform versatile and comprehensive vision tasks. Extensive evaluations on numerous tasks demonstrated Florence-2 to be a strong vision foundation model contender with unprecedented zero-shot and fine-tuning capabilities. |

## 1 Introduction

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/x1.png)

| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 1:  We aim to build a vision foundation model to enable extensive perception capabilities including spatial hierarchy and semantic granularity. To achieve this, a single unified model Florence-2 is pre-trained on our FLD-5B dataset encompassing a total of 5.4B comprehensive annotations across 126M images, which are collected by our Florence data engine.  | âœ… Figure 1:  We aim to build a vision foundation model to enable extensive perception capabilities including spatial hierarchy and semantic granularity. To achieve this, a single unified model Florence-2 is pre-trained on our FLD-5B dataset encompassing a total of 5.4B comprehensive annotations across 126M images, which are collected by our Florence data engine.  |

| ã€ç¬¬1èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬1èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… In the realm of Artificial General Intelligence (AGI) systems, there has been a notable shift towards utilizing pre-trained, versatile representations, acknowledged for task-agnostic benefits accross diverse applications. | âœ… åœ¨é€šç”¨äººå·¥æ™ºèƒ½ (AGI) ç³»ç»Ÿé¢†åŸŸï¼Œäººä»¬å·²ç»æ˜æ˜¾è½¬å‘ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„ã€å¤šåŠŸèƒ½çš„è¡¨ç¤ºå½¢å¼ï¼Œè¿™ç§è¡¨ç¤ºå½¢å¼å› å…¶åœ¨ä¸åŒåº”ç”¨ä¸­å…·æœ‰ä¸ä»»åŠ¡æ— å…³çš„ä¼˜åŠ¿è€Œå—åˆ°è®¤å¯ã€‚ |
| âœ… This trend is evident in natural language processing (NLP), where advanced models ( **1. On the opportunities and risks of foundation models.** ï½œ **2. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.** ï½œ **3. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019.** ï½œ **4. Exploring the limits of transfer learning with a unified text-to-text transformer.** ï½œ **5. Language models are few-shot learners.** ï½œ **6. Language models are unsupervised multitask learners.** ) show adaptability with comprehensive knowledge spanning various domains and tasks with simple instructions. | âœ… è¿™ä¸€è¶‹åŠ¿åœ¨è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ä¸­å¾ˆæ˜æ˜¾ï¼Œå…¶ä¸­é«˜çº§æ¨¡å‹ ( **1. On the opportunities and risks of foundation models.** ï½œ **2. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.** ï½œ **3. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019.** ï½œ **4. Exploring the limits of transfer learning with a unified text-to-text transformer.** ï½œ **5. Language models are few-shot learners.** ï½œ **6. Language models are unsupervised multitask learners.** ) è¡¨ç°å‡ºé€‚åº”æ€§ï¼Œå…·æœ‰æ¶µç›–å„ä¸ªé¢†åŸŸå’Œä»»åŠ¡çš„å…¨é¢çŸ¥è¯†ä»¥åŠç®€å•çš„æŒ‡ä»¤ã€‚ |
| âœ… The success of NLP motivates a parallel approach in computer vision. | âœ… NLP çš„æˆåŠŸæ¿€å‘äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å¹¶è¡Œæ–¹æ³•ã€‚ |

| ã€ç¬¬1èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬1èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Universal representation for diverse vision-related tasks presents unique challenges, notably the need for comprehensive perceptual abilities. | âœ… ä¸å„ç§è§†è§‰ç›¸å…³çš„ä»»åŠ¡çš„é€šç”¨è¡¨ç¤ºæå‡ºäº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯éœ€è¦å…¨é¢çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚ |
| âœ… Unlike NLP, which deals mainly with text, computer vision requires handling intricate visual data like object location, masked contours, and attributes. | âœ… ä¸ä¸»è¦å¤„ç†æ–‡æœ¬çš„ NLP ä¸åŒï¼Œè®¡ç®—æœºè§†è§‰éœ€è¦å¤„ç†å¤æ‚çš„è§†è§‰æ•°æ®ï¼Œå¦‚å¯¹è±¡ä½ç½®ã€è’™ç‰ˆè½®å»“å’Œå±æ€§ã€‚ |
| âœ… Attaining universal representation in computer vision demands adept management of a spectrum of complex tasks, organized two-dimensionally as illustrated in FigureÂ 1 : | âœ… è¦å®ç°è®¡ç®—æœºè§†è§‰çš„é€šç”¨è¡¨ç¤ºï¼Œéœ€è¦ç†Ÿç»ƒåœ°ç®¡ç†ä¸€ç³»åˆ—å¤æ‚çš„ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡ä»¥äºŒç»´å½¢å¼ç»„ç»‡ï¼Œå¦‚ FigureÂ 1 æ‰€ç¤ºï¼š |

| ã€ç¬¬1èŠ‚ï¼Œç¬¬3æ®µã€‘åŸæ–‡ | ã€ç¬¬1èŠ‚ï¼Œç¬¬3æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Spatial Hierarchy : The model must discern spatial details across varying scales, understanding image-level concepts and fine-grained pixel specifics. | âœ… Spatial Hierarchyï¼šæ¨¡å‹å¿…é¡»è¾¨åˆ«ä¸åŒå°ºåº¦çš„ç©ºé—´ç»†èŠ‚ï¼Œç†è§£å›¾åƒçº§æ¦‚å¿µå’Œç»†ç²’åº¦åƒç´ ç»†èŠ‚ã€‚ |
| âœ… Accommodating the intricate spatial hierarchy within vision demands the modelâ€™s proficiency in handling diverse levels of granularity. | âœ… é€‚åº”è§†è§‰å†…å¤æ‚çš„ç©ºé—´å±‚æ¬¡è¦æ±‚æ¨¡å‹èƒ½å¤Ÿç†Ÿç»ƒåœ°å¤„ç†ä¸åŒç²’åº¦çº§åˆ«ã€‚ |

| ã€ç¬¬1èŠ‚ï¼Œç¬¬4æ®µã€‘åŸæ–‡ | ã€ç¬¬1èŠ‚ï¼Œç¬¬4æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Semantic Granularity : Universal representation in computer vision should span a spectrum of semantic granularity. | âœ… Semantic Granularityï¼šè®¡ç®—æœºè§†è§‰ä¸­çš„é€šç”¨è¡¨ç¤ºåº”è¯¥æ¶µç›–è¯­ä¹‰ç²’åº¦çš„èŒƒå›´ã€‚ |
| âœ… The model transitions from high-level captions to nuanced descriptions, enabling versatile understanding for diverse applications. | âœ… è¯¥æ¨¡å‹ä»é«˜çº§æ ‡é¢˜è¿‡æ¸¡åˆ°ç»†è‡´å…¥å¾®çš„æè¿°ï¼Œä¸ºä¸åŒçš„åº”ç”¨æä¾›å¤šç§ç†è§£ã€‚ |

| ã€ç¬¬1èŠ‚ï¼Œç¬¬5æ®µã€‘åŸæ–‡ | ã€ç¬¬1èŠ‚ï¼Œç¬¬5æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… This pursuit is characterized by distinctiveness and substantial challenges. | âœ… è¿™ä¸€è¿½æ±‚å…·æœ‰ç‹¬ç‰¹æ€§å’Œé‡å¤§æŒ‘æˆ˜æ€§ã€‚ |
| âœ… A key hurdle is the scarcity of comprehensive visual annotations , hindering the development of a foundational model capable of capturing the intricate nuances of spatial hierarchy and semantic granularity. | âœ… ä¸€ä¸ªå…³é”®çš„éšœç¢æ˜¯ comprehensive visual annotations çš„ç¨€ç¼ºæ€§ï¼Œè¿™é˜»ç¢äº†èƒ½å¤Ÿæ•æ‰ç©ºé—´å±‚æ¬¡å’Œè¯­ä¹‰ç²’åº¦çš„å¤æ‚ç»†å¾®å·®åˆ«çš„åŸºç¡€æ¨¡å‹çš„å¼€å‘ã€‚ |
| âœ… Existing datasets, such as ImageNet ( **Imagenet: A large-scale hierarchical image database.** ) , COCO ( **Microsoft coco: Common objects in context.** ) , and Flickr30k Entities ( **Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.** ) , tailored for specialized applications, are extensively labeled by humans. | âœ… ç°æœ‰çš„æ•°æ®é›†ï¼Œä¾‹å¦‚ ImageNet ( **Imagenet: A large-scale hierarchical image database.** )ã€COCO ( **Microsoft coco: Common objects in context.** ) å’Œ Flickr30k Entities ( **Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.** )ï¼Œéƒ½æ˜¯ä¸ºä¸“é—¨åº”ç”¨è€Œå®šåˆ¶çš„ï¼Œå¹¶ç”±äººå·¥è¿›è¡Œå¤§é‡æ ‡è®°ã€‚ |
| âœ… To overcome this constraint, it is imperative to generate extensive annotations for each image on a larger scale. | âœ… ä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œå¿…é¡»ä¸ºæ›´å¤§è§„æ¨¡çš„æ¯å¹…å›¾åƒç”Ÿæˆå¤§é‡æ³¨é‡Šã€‚ |

| ã€ç¬¬1èŠ‚ï¼Œç¬¬6æ®µã€‘åŸæ–‡ | ã€ç¬¬1èŠ‚ï¼Œç¬¬6æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Another challenge is the absence of a unified pre-training framework with a singular network architecture that seamlessly integrates spatial hierarchy and semantic granularity in computer vision. | âœ… å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ç¼ºå°‘ä¸€ä¸ªèƒ½å¤Ÿæ— ç¼é›†æˆè®¡ç®—æœºè§†è§‰ä¸­çš„ç©ºé—´å±‚æ¬¡å’Œè¯­ä¹‰ç²’åº¦çš„ unified pre-training framework with a singular network architectureã€‚ |
| âœ… Traditional models excel in tasks like object detection ( **1. Mask r-cnn.** ï½œ **2. Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** ) , semantic segmentation ( **1. Masked-attention mask transformer for universal image segmentation.** ï½œ **2. Unified perceptual parsing for scene understanding.** ) , and image captioning ( **1. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.** ï½œ **2. Git: A generative image-to-text transformer for vision and language, 2022.** ) with task-specific design. | âœ… ä¼ ç»Ÿæ¨¡å‹é€šè¿‡é’ˆå¯¹ä»»åŠ¡çš„ç‰¹å®šè®¾è®¡ï¼Œåœ¨å¯¹è±¡æ£€æµ‹ ( **1. Mask r-cnn.** ï½œ **2. Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** )ã€è¯­ä¹‰åˆ†å‰² ( **1. Masked-attention mask transformer for universal image segmentation.** ï½œ **2. Unified perceptual parsing for scene understanding.** ) å’Œå›¾åƒå­—å¹• ( **1. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.** ï½œ **2. Git: A generative image-to-text transformer for vision and language, 2022.** ) ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚ |
| âœ… However, it is essential to develop a comprehensive, unified model that is capable of adapting across various vision tasks in a task-agnostic manner, even accommodating new tasks with minimal or no task-specific fine-tuning. | âœ… ç„¶è€Œï¼Œå¿…é¡»å¼€å‘ä¸€ä¸ªå…¨é¢ã€ç»Ÿä¸€çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»¥ä¸ä»»åŠ¡æ— å…³çš„æ–¹å¼é€‚åº”å„ç§è§†è§‰ä»»åŠ¡ï¼Œç”šè‡³å¯ä»¥é€šè¿‡æœ€å°‘æˆ–æ²¡æœ‰ç‰¹å®šäºä»»åŠ¡çš„å¾®è°ƒæ¥é€‚åº”æ–°ä»»åŠ¡ã€‚ |

| ã€ç¬¬1èŠ‚ï¼Œç¬¬7æ®µã€‘åŸæ–‡ | ã€ç¬¬1èŠ‚ï¼Œç¬¬7æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… The model Florence Â  ( **Florence: A new foundation model for computer vision.** ) pioneers the integration of spatial, temporal, and multi-modal aspects in computer vision through unified pre-training and network architecture. | âœ… æ¨¡å‹ Florence Â  ( **Florence: A new foundation model for computer vision.** ) é€šè¿‡ç»Ÿä¸€çš„é¢„è®­ç»ƒå’Œç½‘ç»œæ¶æ„ï¼Œç‡å…ˆå®ç°äº†è®¡ç®—æœºè§†è§‰ä¸­ç©ºé—´ã€æ—¶é—´å’Œå¤šæ¨¡æ€æ–¹é¢çš„æ•´åˆã€‚ |
| âœ… The first evolutionary version ( **Florence: A new foundation model for computer vision.** ) excels in transfer learning via pre-training with noisy text-image pairs and task-specific fine-tuning using specialized adapters. | âœ… ç¬¬ä¸€ä¸ªè¿›åŒ–ç‰ˆæœ¬ ( **Florence: A new foundation model for computer vision.** ) é€šè¿‡ä½¿ç”¨å˜ˆæ‚çš„æ–‡æœ¬-å›¾åƒå¯¹è¿›è¡Œé¢„è®­ç»ƒä»¥åŠä½¿ç”¨ä¸“é—¨çš„é€‚é…å™¨è¿›è¡Œç‰¹å®šä»»åŠ¡çš„å¾®è°ƒï¼Œåœ¨è¿ç§»å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ |
| âœ… However, it relies on large task-specific datasets and adapters, leaving gaps in addressing the above dual key challenges. | âœ… ç„¶è€Œï¼Œå®ƒä¾èµ–äºå¤§å‹ç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†å’Œé€‚é…å™¨ï¼Œåœ¨è§£å†³ä¸Šè¿°åŒé”®æŒ‘æˆ˜æ–¹é¢å­˜åœ¨å·®è·ã€‚ |

| ã€ç¬¬1èŠ‚ï¼Œç¬¬8æ®µã€‘åŸæ–‡ | ã€ç¬¬1èŠ‚ï¼Œç¬¬8æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… In this paper, we introduce Florence-2 , a universal backbone achieved through multitask learning with extensive visual annotations. | âœ… åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† Florence-2ï¼Œä¸€ç§é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ å’Œå¤§é‡è§†è§‰æ³¨é‡Šå®ç°çš„é€šç”¨ä¸»å¹²ã€‚ |
| âœ… This results in a unified, prompt-based representation for diverse vision tasks, effectively addressing the challenges of limited comprehensive data and the absence of a unified architecture. | âœ… è¿™ä½¿å¾—é’ˆå¯¹ä¸åŒè§†è§‰ä»»åŠ¡çš„è¡¨ç¤ºå…·æœ‰ç»Ÿä¸€æ€§ã€åŸºäºæç¤ºæ€§ï¼Œä»è€Œæœ‰æ•ˆåœ°è§£å†³äº†ç»¼åˆæ•°æ®æœ‰é™å’Œç¼ºä¹ç»Ÿä¸€æ¶æ„çš„æŒ‘æˆ˜ã€‚ |

| ã€ç¬¬1èŠ‚ï¼Œç¬¬9æ®µã€‘åŸæ–‡ | ã€ç¬¬1èŠ‚ï¼Œç¬¬9æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Multitask learning necessitates large-scale, high-quality annotated data. | âœ… å¤šä»»åŠ¡å­¦ä¹ éœ€è¦å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ³¨é‡Šæ•°æ®ã€‚ |
| âœ… Our data engine, instead of relying on labor-intensive manual annotation, autonomously generates a comprehensive visual dataset called FLD-5B , encompassing a total of 5.4B annotations for 126M images. | âœ… æˆ‘ä»¬çš„æ•°æ®å¼•æ“ä¸å†ä¾èµ–åŠ³åŠ¨å¯†é›†å‹çš„äººå·¥æ³¨é‡Šï¼Œè€Œæ˜¯è‡ªä¸»ç”Ÿæˆä¸€ä¸ªåä¸º FLD-5B çš„ç»¼åˆè§†è§‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å« 126M å¼ å›¾åƒçš„æ€»å…± 54 äº¿æ¡æ³¨é‡Šã€‚ |
| âœ… This engine consists of two efficient processing modules. | âœ… è¯¥å¼•æ“ç”±ä¸¤ä¸ªé«˜æ•ˆçš„å¤„ç†æ¨¡å—ç»„æˆã€‚ |
| âœ… The first module uses specialized models to collaboratively and autonomously annotate images, moving away from the traditional single and manual annotation approach. | âœ… ç¬¬ä¸€ä¸ªæ¨¡å—ä½¿ç”¨ä¸“é—¨çš„æ¨¡å‹æ¥åä½œå’Œè‡ªä¸»åœ°æ³¨é‡Šå›¾åƒï¼Œæ‘†è„±ä¼ ç»Ÿçš„å•ä¸€å’Œæ‰‹åŠ¨æ³¨é‡Šæ–¹æ³•ã€‚ |
| âœ… Multiple models work together to reach a consensus, reminiscent of the wisdom of crowds concept ( **1. The wisdom of the crowd in combinatorial problems.** ï½œ **2. Wisdom of the crowd.** ï½œ **3. Power of the few vs. wisdom of the crowd: Wikipedia and the rise of the bourgeoisie.** ) , ensuring a more reliable and unbiased image understanding. | âœ… å¤šä¸ªæ¨¡å‹å…±åŒåŠªåŠ›è¾¾æˆå…±è¯†ï¼Œè®©äººè”æƒ³åˆ°ç¾¤ä½“æ™ºæ…§æ¦‚å¿µ( **1. The wisdom of the crowd in combinatorial problems.** ï½œ **2. Wisdom of the crowd.** ï½œ **3. Power of the few vs. wisdom of the crowd: Wikipedia and the rise of the bourgeoisie.** )ï¼Œç¡®ä¿æ›´å¯é ã€æ›´å…¬æ­£çš„å›¾åƒç†è§£ã€‚ |
| âœ… The second module iteratively refines and filters these automated annotations using well-trained foundational models. | âœ… ç¬¬äºŒä¸ªæ¨¡å—ä½¿ç”¨è®­ç»ƒæœ‰ç´ çš„åŸºç¡€æ¨¡å‹è¿­ä»£åœ°ç»†åŒ–å’Œè¿‡æ»¤è¿™äº›è‡ªåŠ¨æ³¨é‡Šã€‚ |

| ã€ç¬¬1èŠ‚ï¼Œç¬¬10æ®µã€‘åŸæ–‡ | ã€ç¬¬1èŠ‚ï¼Œç¬¬10æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… By utilizing this extensive dataset, our model employs a sequence-to-sequence (seq2seq) architecture ( **1. Sequence to sequence learning with neural networks.** ï½œ **2. Learning phrase representations using rnn encoder-decoder for statistical machine translation.** ï½œ **3. Exploring the limits of transfer learning with a unified text-to-text transformer.** ï½œ **4. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.** ) , which integrates an image encoder and a multi-modality encoder-decoder. | âœ… é€šè¿‡åˆ©ç”¨è¿™ä¸ªå¹¿æ³›çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨äº†åºåˆ—åˆ°åºåˆ—ï¼ˆseq2seqï¼‰æ¶æ„ ( **1. Sequence to sequence learning with neural networks.** ï½œ **2. Learning phrase representations using rnn encoder-decoder for statistical machine translation.** ï½œ **3. Exploring the limits of transfer learning with a unified text-to-text transformer.** ï½œ **4. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.** )ï¼Œå®ƒé›†æˆäº†å›¾åƒç¼–ç å™¨å’Œå¤šæ¨¡æ€ç¼–ç å™¨-è§£ç å™¨ã€‚ |
| âœ… This design accommodates a spectrum of vision tasks without the need for task-specific architectural modifications, aligning with the ethos of the NLP community for versatile model development with a consistent underlying structure. | âœ… è¯¥è®¾è®¡é€‚ç”¨äºä¸€ç³»åˆ—è§†è§‰ä»»åŠ¡ï¼Œè€Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œæ¶æ„ä¿®æ”¹ï¼Œè¿™ç¬¦åˆ NLP ç¤¾åŒºå…³äºå…·æœ‰ä¸€è‡´åº•å±‚ç»“æ„çš„å¤šåŠŸèƒ½æ¨¡å‹å¼€å‘çš„ç²¾ç¥ã€‚ |
| âœ… All annotations in the dataset FLD-5B , are uniformly standardized into textual outputs, facilitating a unified multi-task learning approach with consistent optimization with the same loss function as the objective. | âœ… æ•°æ®é›† FLD-5B ä¸­çš„æ‰€æœ‰æ³¨é‡Šéƒ½è¢«ç»Ÿä¸€æ ‡å‡†åŒ–ä¸ºæ–‡æœ¬è¾“å‡ºï¼Œä»è€Œä¿ƒè¿›äº†ç»Ÿä¸€çš„å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ï¼Œå¹¶ä»¥ç›¸åŒçš„æŸå¤±å‡½æ•°ä½œä¸ºç›®æ ‡è¿›è¡Œä¸€è‡´çš„ä¼˜åŒ–ã€‚ |
| âœ… The outcome is a versatile vision foundation model, Florence-2 , capable of performing a variety of tasks, such as object detection, captioning, and grounding, all within a single model governed by a uniform set of parameters. | âœ… æœ€ç»ˆæˆæœæ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½è§†è§‰åŸºç¡€æ¨¡å‹ Florence-2ï¼Œå®ƒèƒ½å¤Ÿæ‰§è¡Œå„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚å¯¹è±¡æ£€æµ‹ã€å­—å¹•å’ŒåŸºç¡€ï¼Œæ‰€æœ‰è¿™äº›éƒ½åœ¨ç”±ç»Ÿä¸€çš„ä¸€ç»„å‚æ•°æ§åˆ¶çš„å•ä¸ªæ¨¡å‹ä¸­å®Œæˆã€‚ |
| âœ… Task activation is achieved through textual prompts, reflecting the approach used by Large Language Models (LLMs) ( **Language models are unsupervised multitask learners.** ) . | âœ… ä»»åŠ¡æ¿€æ´»æ˜¯é€šè¿‡æ–‡æœ¬æç¤ºå®ç°çš„ï¼Œåæ˜ äº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ( **Language models are unsupervised multitask learners.** ) ä½¿ç”¨çš„æ–¹æ³•ã€‚ |

| ã€ç¬¬1èŠ‚ï¼Œç¬¬11æ®µã€‘åŸæ–‡ | ã€ç¬¬1èŠ‚ï¼Œç¬¬11æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Our approach attains a universal representation, demonstrating broad applicability across various visual tasks. | âœ… æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†é€šç”¨è¡¨ç¤ºï¼Œå±•ç¤ºäº†åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚ |
| âœ… Key results include: | âœ… ä¸»è¦æˆæœåŒ…æ‹¬ï¼š |

| ã€ç¬¬1èŠ‚ï¼Œç¬¬12æ®µã€‘åŸæ–‡ | ã€ç¬¬1èŠ‚ï¼Œç¬¬12æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… As a versatile vision foundation model, Florence-2 achieves new state-of-the-art zero-shot performance in tasks such as captioning on COCO ( **Microsoft coco: Common objects in context.** ) , visual grounding on Flick30k ( **Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.** ) , and referring expression comprehension on RefCOCO/+/g ( **1. Referitgame: Referring to objects in photographs of natural scenes.** ï½œ **2. Modeling context in referring expressions.** ï½œ **3. Generation and comprehension of unambiguous object descriptions.** ) . | âœ… ä½œä¸ºä¸€ä¸ªå¤šåŠŸèƒ½è§†è§‰åŸºç¡€æ¨¡å‹ï¼ŒFlorence-2 åœ¨ COCO ( **Microsoft coco: Common objects in context.** ) ä¸Šçš„å­—å¹•ã€Flick30k ( **Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.** ) ä¸Šçš„è§†è§‰åŸºç¡€ã€ä»¥åŠ RefCOCO/+/g ( **1. Referitgame: Referring to objects in photographs of natural scenes.** ï½œ **2. Modeling context in referring expressions.** ï½œ **3. Generation and comprehension of unambiguous object descriptions.** ) ä¸Šçš„æŒ‡ç§°è¡¨è¾¾ç†è§£ç­‰ä»»åŠ¡ä¸­å®ç°äº†æ–°çš„æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ |

| ã€ç¬¬1èŠ‚ï¼Œç¬¬13æ®µã€‘åŸæ–‡ | ã€ç¬¬1èŠ‚ï¼Œç¬¬13æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… After fine-tuning with public human-annotated data, Florence-2 , despite its compact size, competes with larger specialist models. | âœ… åœ¨ä½¿ç”¨å…¬å…±çš„äººå·¥æ³¨é‡Šæ•°æ®è¿›è¡Œå¾®è°ƒåï¼ŒFlorence-2 å°½ç®¡ä½“ç§¯å°ï¼Œä½†ä»èƒ½ä¸æ›´å¤§çš„ä¸“ä¸šæ¨¡å‹ç›¸åª²ç¾ã€‚ |
| âœ… Notably, the fine-tuned Florence-2 establishes new state-of-the-art results on the benchmarks on RefCOCO/+/g. | âœ… å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç»è¿‡å¾®è°ƒçš„ Florence-2 åœ¨ RefCOCO/+/g çš„åŸºå‡†ä¸Šå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚ |

| ã€ç¬¬1èŠ‚ï¼Œç¬¬14æ®µã€‘åŸæ–‡ | ã€ç¬¬1èŠ‚ï¼Œç¬¬14æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… The pre-trained Florence-2 backbone enhances performance on downstream tasks, e.g. | âœ… é¢„å…ˆè®­ç»ƒçš„ Florence-2 ä¸»å¹²å¢å¼ºäº†ä¸‹æ¸¸ä»»åŠ¡ e.g çš„æ€§èƒ½ã€‚ |
| âœ…  COCO object detection and instance segmentation, and ADE20K semantic segmentation, surpassing both supervised and self-supervised models. | âœ…  COCO å¯¹è±¡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²ï¼Œä»¥åŠ ADE20K è¯­ä¹‰åˆ†å‰²ï¼Œè¶…è¶Šäº†ç›‘ç£å’Œè‡ªç›‘ç£æ¨¡å‹ã€‚ |
| âœ… Compared to pre-trained models on ImageNet, ours improves training efficiency by 4  $\times$  and achieves substantial improvements of 6.9, 5.5, and 5.9 points on COCO ( **Microsoft coco: Common objects in context.** ) and ADE20K ( **Scene parsing through ade20k dataset.** ) datasets, using Mask-RCNN ( **Mask r-cnn.** ) , DINO ( **Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** ) , and UperNet ( **Unified perceptual parsing for scene understanding.** ) frameworks respectively. | âœ… ä¸ ImageNet ä¸Šçš„é¢„è®­ç»ƒæ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æé«˜äº† 4  $\times$  çš„è®­ç»ƒæ•ˆç‡ï¼Œå¹¶åœ¨ COCO ( **Microsoft coco: Common objects in context.** ) å’Œ ADE20K ( **Scene parsing through ade20k dataset.** ) æ•°æ®é›†ä¸Šåˆ†åˆ«ä½¿ç”¨ Mask-RCNN ( **Mask r-cnn.** )ã€DINO ( **Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** ) å’Œ UperNet ( **Unified perceptual parsing for scene understanding.** ) æ¡†æ¶å–å¾—äº† 6.9ã€5.5 å’Œ 5.9 ç‚¹çš„å¤§å¹…æå‡ã€‚ |

## 2 Rethinking Vision Model Pre-training

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/x2.png)

| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 2:  Florence-2  consists of an image encoder and standard multi-modality encoder-decoder. We train Florence-2 on our FLD-5B data in a unified multitask learning paradigm, resulting in a generaslist vision foundation model, which can perform various vision tasks. | âœ… Figure 2:  Florence-2  consists of an image encoder and standard multi-modality encoder-decoder. We train Florence-2 on our FLD-5B data in a unified multitask learning paradigm, resulting in a generaslist vision foundation model, which can perform various vision tasks. |

| ã€ç¬¬2èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬2èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… In pursuit of a versatile vision foundation model, we revisit three predominant pre-training paradigms: supervised ( e.g. | âœ… ä¸ºäº†è¿½æ±‚å¤šåŠŸèƒ½çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†ä¸‰ç§ä¸»è¦çš„é¢„è®­ç»ƒèŒƒå¼ï¼šç›‘ç£ï¼ˆe.gï¼‰ã€‚ |
| âœ…  , ImageNet classification ( **Imagenet: A large-scale hierarchical image database.** ) ), self-supervised ( e.g. | âœ… ã€ImageNet åˆ†ç±» ( **Imagenet: A large-scale hierarchical image database.** )ï¼‰ã€è‡ªç›‘ç£ï¼ˆe.gï¼‰ã€‚ |
| âœ…  , SimCLR ( **A simple framework for contrastive learning of visual representations.** ) , MoCo ( **Momentum contrast for unsupervised visual representation learning.** ) , BEiT ( **BEiT: BERT pre-training of image transformers.** ) , MAE ( **Masked autoencoders are scalable vision learners.** ) ), and weakly supervised ( e.g. | âœ… ã€SimCLR ( **A simple framework for contrastive learning of visual representations.** )ã€MoCo ( **Momentum contrast for unsupervised visual representation learning.** )ã€BEiT ( **BEiT: BERT pre-training of image transformers.** )ã€MAE ( **Masked autoencoders are scalable vision learners.** )ï¼‰å’Œå¼±ç›‘ç£ï¼ˆe.gï¼‰ã€‚ |
| âœ…  , CLIP ( **Learning transferable visual models from natural language supervision.** ) , Florence ( **Florence: A new foundation model for computer vision.** ) , SAM ( **Segment anything.** ) ). | âœ… ã€CLIP ( **Learning transferable visual models from natural language supervision.** )ã€ä½›ç½—ä¼¦è¨ ( **Florence: A new foundation model for computer vision.** )ã€SAM ( **Segment anything.** )ï¼‰ã€‚ |
| âœ… Each paradigm captures unique aspects of visual data but is inherently limited by the constraints of single-task learning frameworks. | âœ… æ¯ä¸ªèŒƒå¼éƒ½æ•æ‰è§†è§‰æ•°æ®çš„ç‹¬ç‰¹æ–¹é¢ï¼Œä½†æœ¬è´¨ä¸Šå—åˆ°å•ä»»åŠ¡å­¦ä¹ æ¡†æ¶çš„é™åˆ¶ã€‚ |
| âœ… Supervised pre-training excels in object recognition but lacks adaptability ( **Imagenet classification with deep convolutional neural networks.** ) ; self-supervised algorithms reveal intricate features but may overemphasize certain attributes ( **Unsupervised learning of visual features by contrasting cluster assignments.** ) ; weakly supervised methods leverage unstructured textual annotations but yield only image-level understanding ( **Learning transferable visual models from natural language supervision.** ). | âœ… ç›‘ç£é¢„è®­ç»ƒåœ¨ç‰©ä½“è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹é€‚åº”æ€§ ( **Imagenet classification with deep convolutional neural networks.** )ï¼›è‡ªç›‘ç£ç®—æ³•æ­ç¤ºå¤æ‚çš„ç‰¹å¾ï¼Œä½†å¯èƒ½ä¼šè¿‡åˆ†å¼ºè°ƒæŸäº›å±æ€§ ( **Unsupervised learning of visual features by contrasting cluster assignments.** )ï¼›å¼±ç›‘ç£æ–¹æ³•åˆ©ç”¨éç»“æ„åŒ–æ–‡æœ¬æ³¨é‡Šï¼Œä½†åªèƒ½äº§ç”Ÿå›¾åƒçº§åˆ«çš„ç†è§£ ( **Learning transferable visual models from natural language supervision.** )ã€‚ |
| âœ… To build a unified vision foundation model suitable for various applications, we must explore innovative pre-training strategies that overcome single-task limitations and integrate both textual and visual semantics. | âœ… ä¸ºäº†æ„å»ºé€‚ç”¨äºå„ç§åº”ç”¨çš„ç»Ÿä¸€è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬å¿…é¡»æ¢ç´¢åˆ›æ–°çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œä»¥å…‹æœå•ä»»åŠ¡é™åˆ¶å¹¶æ•´åˆæ–‡æœ¬å’Œè§†è§‰è¯­ä¹‰ã€‚ |

| ã€ç¬¬2èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬2èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Image understanding necessitates capturing multiple levels of granularity, from global semantics to local details, and comprehending spatial relationships between objects and entities in their semantic context. | âœ… å›¾åƒç†è§£éœ€è¦æ•æ‰å¤šå±‚æ¬¡çš„ç²’åº¦ï¼Œä»å…¨å±€è¯­ä¹‰åˆ°å±€éƒ¨ç»†èŠ‚ï¼Œå¹¶ç†è§£è¯­ä¹‰ç¯å¢ƒä¸­å¯¹è±¡å’Œå®ä½“ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€‚ |
| âœ… To address these core aspects of image understanding, our approach incorporates a diverse set of annotations, effectively capturing visual understanding nuances and bridging the gap between vision and language understanding. | âœ… ä¸ºäº†è§£å†³å›¾åƒç†è§£çš„è¿™äº›æ ¸å¿ƒæ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å¤šç§æ³¨é‡Šï¼Œæœ‰æ•ˆåœ°æ•æ‰è§†è§‰ç†è§£çš„ç»†å¾®å·®åˆ«å¹¶å¼¥åˆè§†è§‰å’Œè¯­è¨€ç†è§£ä¹‹é—´çš„å·®è·ã€‚ |

### 2.1 Comprehensive Multitask Learning

| ã€ç¬¬2.1èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬2.1èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… To develop a versatile vision foundation model, we formulate a range of multitask learning objectives, each tailored to address specific aspects of visual comprehension. | âœ… ä¸ºäº†å¼€å‘å¤šåŠŸèƒ½çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ¶å®šäº†ä¸€ç³»åˆ—å¤šä»»åŠ¡å­¦ä¹ ç›®æ ‡ï¼Œæ¯ä¸ªç›®æ ‡éƒ½é’ˆå¯¹è§†è§‰ç†è§£çš„ç‰¹å®šæ–¹é¢è¿›è¡Œå®šåˆ¶ã€‚ |
| âœ… These objectives align with our predefined criteria: spatial hierarchy and semantic granularity, inspired by recent research on multitask learning ( **1. Flamingo: a visual language model for few-shot learning.** ï½œ **2. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework, 2022.** ï½œ **3. Unified-io: A unified model for vision, language, and multi-modal tasks, 2022.** ï½œ **4. Pali: A jointly-scaled multilingual language-image model, 2022.** ï½œ **5. Pali-x: On scaling up a multilingual vision and language model.** ï½œ **6. Pali-3 vision language models: Smaller, faster, stronger, 2023.** ). | âœ… è¿™äº›ç›®æ ‡ä¸æˆ‘ä»¬é¢„å®šä¹‰çš„æ ‡å‡†ä¸€è‡´ï¼šç©ºé—´å±‚æ¬¡å’Œè¯­ä¹‰ç²’åº¦ï¼Œå—åˆ°æœ€è¿‘å¯¹å¤šä»»åŠ¡å­¦ä¹  ( **1. Flamingo: a visual language model for few-shot learning.** ï½œ **2. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework, 2022.** ï½œ **3. Unified-io: A unified model for vision, language, and multi-modal tasks, 2022.** ï½œ **4. Pali: A jointly-scaled multilingual language-image model, 2022.** ï½œ **5. Pali-x: On scaling up a multilingual vision and language model.** ï½œ **6. Pali-3 vision language models: Smaller, faster, stronger, 2023.** ) çš„ç ”ç©¶çš„å¯å‘ã€‚ |
| âœ… Our multitask learning approach incorporates three distinct learning objectives, each addressing a different level of granularity and semantic understanding: | âœ… æˆ‘ä»¬çš„å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•åŒ…å«ä¸‰ä¸ªä¸åŒçš„å­¦ä¹ ç›®æ ‡ï¼Œæ¯ä¸ªç›®æ ‡é’ˆå¯¹ä¸åŒçº§åˆ«çš„ç²’åº¦å’Œè¯­ä¹‰ç†è§£ï¼š |

| ã€ç¬¬2.1èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬2.1èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Image-level understanding tasks capture high-level semantics and foster a comprehensive understanding of images through linguistic descriptions ( **1. Microsoft coco captions: Data collection and evaluation server.** ï½œ **2. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.** ï½œ **3. Imagenet: A large-scale hierarchical image database.** ï½œ **4. A hierarchical approach for generating descriptive image paragraphs.** ). | âœ… Image-level understanding ä»»åŠ¡æ•è·é«˜çº§è¯­ä¹‰å¹¶é€šè¿‡è¯­è¨€æè¿° ( **1. Microsoft coco captions: Data collection and evaluation server.** ï½œ **2. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.** ï½œ **3. Imagenet: A large-scale hierarchical image database.** ï½œ **4. A hierarchical approach for generating descriptive image paragraphs.** ) ä¿ƒè¿›å¯¹å›¾åƒçš„å…¨é¢ç†è§£ã€‚ |
| âœ… They enable the model to comprehend the overall context of an image and grasp semantic relationships and contextual nuances in the language domain. | âœ… å®ƒä»¬ä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£å›¾åƒçš„æ•´ä½“èƒŒæ™¯å¹¶æŒæ¡è¯­è¨€é¢†åŸŸä¸­çš„è¯­ä¹‰å…³ç³»å’Œä¸Šä¸‹æ–‡ç»†å¾®å·®åˆ«ã€‚ |
| âœ… Exemplar tasks include image classification, captioning, and visual question answering. | âœ… ç¤ºä¾‹ä»»åŠ¡åŒ…æ‹¬å›¾åƒåˆ†ç±»ã€å­—å¹•å’Œè§†è§‰é—®ç­”ã€‚ |

| ã€ç¬¬2.1èŠ‚ï¼Œç¬¬3æ®µã€‘åŸæ–‡ | ã€ç¬¬2.1èŠ‚ï¼Œç¬¬3æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Region/pixel-level recognition tasks facilitate detailed object and entity localization within images, capturing relationships between objects and their spatial context. | âœ… Region/pixel-level recognition ä»»åŠ¡æœ‰åŠ©äºåœ¨å›¾åƒä¸­å®ç°è¯¦ç»†çš„å¯¹è±¡å’Œå®ä½“å®šä½ï¼Œæ•æ‰å¯¹è±¡ä¸å…¶ç©ºé—´ç¯å¢ƒä¹‹é—´çš„å…³ç³»ã€‚ |
| âœ… Tasks include object detection, segmentation, and referring expression comprehension. | âœ… ä»»åŠ¡åŒ…æ‹¬å¯¹è±¡æ£€æµ‹ã€åˆ†å‰²å’ŒæŒ‡ç§°è¡¨è¾¾ç†è§£ã€‚ |

| ã€ç¬¬2.1èŠ‚ï¼Œç¬¬4æ®µã€‘åŸæ–‡ | ã€ç¬¬2.1èŠ‚ï¼Œç¬¬4æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Fine-grained visual-semantic alignment tasks require fine-grained understanding of both text and image. | âœ… Fine-grained visual-semantic alignment ä»»åŠ¡éœ€è¦å¯¹æ–‡æœ¬å’Œå›¾åƒè¿›è¡Œç»†ç²’åº¦çš„ç†è§£ã€‚ |
| âœ… It involves locating the image regions that correspond to the text phrases, such as objects, attributes, or relations. | âœ… å®ƒæ¶‰åŠå®šä½ä¸æ–‡æœ¬çŸ­è¯­ï¼ˆä¾‹å¦‚å¯¹è±¡ã€å±æ€§æˆ–å…³ç³»ï¼‰ç›¸å¯¹åº”çš„å›¾åƒåŒºåŸŸã€‚ |
| âœ… These tasks challenge the ability to capture the local details of visual entities and their semantic contexts, as well as the interactions between textual and visual elements. | âœ… è¿™äº›ä»»åŠ¡æŒ‘æˆ˜äº†æ•æ‰è§†è§‰å®ä½“çš„å±€éƒ¨ç»†èŠ‚åŠå…¶è¯­ä¹‰èƒŒæ™¯ä»¥åŠæ–‡æœ¬å’Œè§†è§‰å…ƒç´ ä¹‹é—´äº¤äº’çš„èƒ½åŠ›ã€‚ |

| ã€ç¬¬2.1èŠ‚ï¼Œç¬¬5æ®µã€‘åŸæ–‡ | ã€ç¬¬2.1èŠ‚ï¼Œç¬¬5æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… By combining these three learning objectives in a multitask learning framework, our foundation model learns to handle different levels of detail and semantic understanding. | âœ… é€šè¿‡åœ¨å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ä¸­ç»“åˆè¿™ä¸‰ä¸ªå­¦ä¹ ç›®æ ‡ï¼Œæˆ‘ä»¬çš„åŸºç¡€æ¨¡å‹å¯ä»¥å­¦ä¹ å¤„ç†ä¸åŒçº§åˆ«çš„ç»†èŠ‚å’Œè¯­ä¹‰ç†è§£ã€‚ |
| âœ… This strategic alignment enables our model to deal with various spatial details, distinguish levels of detail in understanding, and go beyond surface-level recognitionâ€”ultimately learning a universal representation for vision understanding. | âœ… è¿™ç§æˆ˜ç•¥åè°ƒä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå¤„ç†å„ç§ç©ºé—´ç»†èŠ‚ï¼ŒåŒºåˆ†ç†è§£ä¸­çš„ç»†èŠ‚å±‚æ¬¡ï¼Œå¹¶è¶…è¶Šè¡¨é¢å±‚æ¬¡çš„è¯†åˆ«â€”â€”æœ€ç»ˆå­¦ä¹ è§†è§‰ç†è§£çš„é€šç”¨è¡¨ç¤ºã€‚ |

## 3 Model

| ã€ç¬¬3èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬3èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We present the foundation model Florence-2 , designed for universal representation learning, capable of handling various vision tasks with a single set of weights and a unified architecture. | âœ… æˆ‘ä»¬æå‡ºäº†åŸºç¡€æ¨¡å‹ Florence-2ï¼Œä¸“ä¸ºé€šç”¨è¡¨ç¤ºå­¦ä¹ è€Œè®¾è®¡ï¼Œèƒ½å¤Ÿä½¿ç”¨ä¸€ç»„æƒé‡å’Œç»Ÿä¸€çš„æ¶æ„å¤„ç†å„ç§è§†è§‰ä»»åŠ¡ã€‚ |
| âœ… As depicted in FigureÂ 2 , Florence-2 employs a sequence-to-sequence learning paradigm ( **Attention is all you need.** ) , integrating all tasks, described in SectionÂ 2 , under a common language modeling objective. | âœ… å¦‚ FigureÂ 2 æ‰€ç¤ºï¼ŒFlorence-2 é‡‡ç”¨åºåˆ—åˆ°åºåˆ—å­¦ä¹ èŒƒå¼ ( **Attention is all you need.** )ï¼Œå°† SectionÂ 2 ä¸­æè¿°çš„æ‰€æœ‰ä»»åŠ¡æ•´åˆåœ¨ä¸€ä¸ªå…±åŒçš„è¯­è¨€å»ºæ¨¡ç›®æ ‡ä¹‹ä¸‹ã€‚ |
| âœ… The model takes images coupled with task-prompt as task instructions, and generates the desirable results in text forms. | âœ… è¯¥æ¨¡å‹ä»¥å›¾åƒåŠ ä¸Šä»»åŠ¡æç¤ºä½œä¸ºä»»åŠ¡æŒ‡ä»¤ï¼Œä»¥æ–‡æœ¬å½¢å¼ç”Ÿæˆæ‰€éœ€çš„ç»“æœã€‚ |
| âœ… It uses a vision encoder to convert images into visual token embeddings, which are then concatenated with text embeddings and processed by a transformer-based multi-modal encoder-decoder to generate the response. | âœ… å®ƒä½¿ç”¨è§†è§‰ç¼–ç å™¨å°†å›¾åƒè½¬æ¢ä¸ºè§†è§‰æ ‡è®°åµŒå…¥ï¼Œç„¶åå°†å…¶ä¸æ–‡æœ¬åµŒå…¥è¿æ¥ï¼Œå¹¶ç”±åŸºäºå˜å‹å™¨çš„å¤šæ¨¡æ€ç¼–ç å™¨è§£ç å™¨å¤„ç†ä»¥ç”Ÿæˆå“åº”ã€‚ |
| âœ… In the following sections, we will provide a detailed explanation of each model component. | âœ… åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹æ¯ä¸ªæ¨¡å‹ç»„ä»¶è¿›è¡Œè¯¦ç»†çš„è§£é‡Šã€‚ |

#### 3.1 Task formulation.

| ã€ç¬¬3.1èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬3.1èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We adopt a sequence-to-sequence framework ( **1. Attention is all you need.** ï½œ **2. Unified-io: A unified model for vision, language, and multi-modal tasks, 2022.** ï½œ **3. Pali: A jointly-scaled multilingual language-image model, 2022.** ï½œ **4. Pix2seq: A language modeling framework for object detection, 2022.** ) to address various vision tasks in a unified manner. | âœ… æˆ‘ä»¬é‡‡ç”¨åºåˆ—åˆ°åºåˆ—æ¡†æ¶( **1. Attention is all you need.** ï½œ **2. Unified-io: A unified model for vision, language, and multi-modal tasks, 2022.** ï½œ **3. Pali: A jointly-scaled multilingual language-image model, 2022.** ï½œ **4. Pix2seq: A language modeling framework for object detection, 2022.** )æ¥ç»Ÿä¸€å¤„ç†å„ç§è§†è§‰ä»»åŠ¡ã€‚ |
| âœ… As shown in TableÂ 13 , we formulate each task as a translation problem: Given an input image and a task-specific prompt, we generate the corresponding output response. | âœ… å¦‚ TableÂ 13 æ‰€ç¤ºï¼Œæˆ‘ä»¬å°†æ¯ä¸ªä»»åŠ¡åˆ¶å®šä¸ºä¸€ä¸ªç¿»è¯‘é—®é¢˜ï¼šç»™å®šä¸€ä¸ªè¾“å…¥å›¾åƒå’Œä¸€ä¸ªç‰¹å®šäºä»»åŠ¡çš„æç¤ºï¼Œæˆ‘ä»¬ç”Ÿæˆç›¸åº”çš„è¾“å‡ºå“åº”ã€‚ |
| âœ… Depending on the task, the prompt and response can be either text or region: | âœ… æ ¹æ®ä»»åŠ¡ï¼Œæç¤ºå’Œå“åº”å¯ä»¥æ˜¯æ–‡æœ¬æˆ–åŒºåŸŸï¼š |

| ã€ç¬¬3.1èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬3.1èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Text : When the prompt or answer is plain text without special formatting, we maintain it in our final sequence-to-sequence format. | âœ… Textï¼šå½“æç¤ºæˆ–ç­”æ¡ˆæ˜¯æ²¡æœ‰ç‰¹æ®Šæ ¼å¼çš„çº¯æ–‡æœ¬æ—¶ï¼Œæˆ‘ä»¬ä¼šå°†å…¶ä¿ç•™åœ¨æœ€ç»ˆçš„åºåˆ—åˆ°åºåˆ—æ ¼å¼ä¸­ã€‚ |

| ã€ç¬¬3.1èŠ‚ï¼Œç¬¬3æ®µã€‘åŸæ–‡ | ã€ç¬¬3.1èŠ‚ï¼Œç¬¬3æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Region : For region-specific tasks, we add location tokens to the tokenizerâ€™s vocabulary list, representing quantized coordinates. | âœ… Regionï¼šå¯¹äºç‰¹å®šåŒºåŸŸçš„ä»»åŠ¡ï¼Œæˆ‘ä»¬å°†ä½ç½®æ ‡è®°æ·»åŠ åˆ°æ ‡è®°å™¨çš„è¯æ±‡åˆ—è¡¨ä¸­ï¼Œè¡¨ç¤ºé‡åŒ–åæ ‡ã€‚ |
| âœ… We create  $1,000$  bins, similar to ( **1. Pix2seq: A language modeling framework for object detection, 2022.** ï½œ **2. Unified-io: A unified model for vision, language, and multi-modal tasks, 2022.** ï½œ **3. A unified sequence interface for vision tasks.** ï½œ **4. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework, 2022.** ) , and represent regions using formats tailored to task requirements: | âœ… æˆ‘ä»¬åˆ›å»ºç±»ä¼¼äº ( **1. Pix2seq: A language modeling framework for object detection, 2022.** ï½œ **2. Unified-io: A unified model for vision, language, and multi-modal tasks, 2022.** ï½œ **3. A unified sequence interface for vision tasks.** ï½œ **4. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework, 2022.** ) çš„  $1,000$  ç®±ï¼Œå¹¶ä½¿ç”¨æ ¹æ®ä»»åŠ¡è¦æ±‚å®šåˆ¶çš„æ ¼å¼æ¥è¡¨ç¤ºåŒºåŸŸï¼š | | ã€ç¬¬3.1èŠ‚ï¼Œç¬¬4æ®µã€‘åŸæ–‡ | ã€ç¬¬3.1èŠ‚ï¼Œç¬¬4æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Box representation (x0,y0,x1,y1)subscriptğ‘¥0subscriptğ‘¦0subscriptğ‘¥1subscriptğ‘¦1(x_{0},y_{0},x_{1},y_{1}) : Utilized in tasks such as object detection and dense region captioning, with location tokens corresponding to the box coordinates. | âœ… Box representation (x0,y0,x1,y1)subscriptğ‘¥0subscriptğ‘¦0subscriptğ‘¥1subscriptğ‘¦1(x_{0},y_{0},x_{1},y_{1})ï¼šç”¨äºå¯¹è±¡æ£€æµ‹å’Œå¯†é›†åŒºåŸŸå­—å¹•ç­‰ä»»åŠ¡ï¼Œä½ç½®æ ‡è®°ä¸æ¡†åæ ‡ç›¸å¯¹åº”ã€‚ |
| âœ… The location tokens are the coordinates of the top-left and bottom-right corners of the box. | âœ… ä½ç½®æ ‡è®°æ˜¯æ¡†å·¦ä¸Šè§’å’Œå³ä¸‹è§’çš„åæ ‡ã€‚ | | ã€ç¬¬3.1èŠ‚ï¼Œç¬¬5æ®µã€‘åŸæ–‡ | ã€ç¬¬3.1èŠ‚ï¼Œç¬¬5æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Quad box representation (x0,y0,â€¦,x3,y3)subscriptğ‘¥0subscriptğ‘¦0â€¦subscriptğ‘¥3subscriptğ‘¦3(x_{0},y_{0},...,x_{3},y_{3}) : For text detection and recognition tasks, using location tokens for each coordinate of the quadrilateral enclosing the text. | âœ… Quad box representation (x0,y0,â€¦,x3,y3)subscriptğ‘¥0subscriptğ‘¦0â€¦subscriptğ‘¥3subscriptğ‘¦3(x_{0},y_{0},...,x_{3},y_{3})ï¼šå¯¹äºæ–‡æœ¬æ£€æµ‹å’Œè¯†åˆ«ä»»åŠ¡ï¼Œä½¿ç”¨ä½ç½®æ ‡è®°æ¥è¡¨ç¤ºåŒ…å›´æ–‡æœ¬çš„å››è¾¹å½¢çš„æ¯ä¸ªåæ ‡ã€‚ |
| âœ… The location tokens are the coordinates of each corner of the quad box, starting from the top-left and going clockwise. | âœ… ä½ç½®æ ‡è®°æ˜¯å››è¾¹å½¢æ¡†æ¯ä¸ªè§’çš„åæ ‡ï¼Œä»å·¦ä¸Šè§’å¼€å§‹é¡ºæ—¶é’ˆæ—‹è½¬ã€‚ | | ã€ç¬¬3.1èŠ‚ï¼Œç¬¬6æ®µã€‘åŸæ–‡ | ã€ç¬¬3.1èŠ‚ï¼Œç¬¬6æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Polygon Representation (x0,y0,â€¦,xn,yn)subscriptğ‘¥0subscriptğ‘¦0â€¦subscriptğ‘¥ğ‘›subscriptğ‘¦ğ‘›(x_{0},y_{0},...,x_{n},y_{n}) : For referring segmentation tasks, with location tokens representing the vertices of the polygon. | âœ… Polygon Representation (x0,y0,â€¦,xn,yn)subscriptğ‘¥0subscriptğ‘¦0â€¦subscriptğ‘¥ğ‘›subscriptğ‘¦ğ‘›(x_{0},y_{0},...,x_{n},y_{n})ï¼šç”¨äºå¼•ç”¨åˆ†å‰²ä»»åŠ¡ï¼Œå…¶ä¸­ä½ç½®æ ‡è®°ä»£è¡¨å¤šè¾¹å½¢çš„é¡¶ç‚¹ã€‚ |
| âœ… The location tokens are the coordinates of the vertices of the polygon, in clockwise order. | âœ… ä½ç½®æ ‡è®°æ˜¯å¤šè¾¹å½¢é¡¶ç‚¹çš„åæ ‡ï¼ŒæŒ‰é¡ºæ—¶é’ˆé¡ºåºæ’åˆ—ã€‚ |

| ã€ç¬¬3.1èŠ‚ï¼Œç¬¬7æ®µã€‘åŸæ–‡ | ã€ç¬¬3.1èŠ‚ï¼Œç¬¬7æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… By extending the tokenizerâ€™s vocabulary to include location tokens, we enable the model to process region-specific information in a unified learning format. | âœ… é€šè¿‡æ‰©å±•æ ‡è®°å™¨çš„è¯æ±‡è¡¨ä»¥åŒ…å«ä½ç½®æ ‡è®°ï¼Œæˆ‘ä»¬ä½¿æ¨¡å‹èƒ½å¤Ÿä»¥ç»Ÿä¸€çš„å­¦ä¹ æ ¼å¼å¤„ç†ç‰¹å®šäºåŒºåŸŸçš„ä¿¡æ¯ã€‚ |
| âœ… This eliminates the need to design task-specific heads for different tasks and allows for a more data-centric approach. | âœ… è¿™æ ·å°±æ— éœ€ä¸ºä¸åŒä»»åŠ¡è®¾è®¡ç‰¹å®šä»»åŠ¡çš„å¤´éƒ¨ï¼Œè€Œå¯ä»¥é‡‡ç”¨æ›´åŠ ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹æ³•ã€‚ |

#### 3.2 Vision encoder.

| ã€ç¬¬3.2èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬3.2èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We employ DaViT ( **Davit: Dual attention vision transformers.** ) as the vision encoder. | âœ… æˆ‘ä»¬é‡‡ç”¨ DaViT ( **Davit: Dual attention vision transformers.** ) ä½œä¸ºè§†è§‰ç¼–ç å™¨ã€‚ |
| âœ… It processes an input image  $\mathbf{I}\in\mathbb{R}^{H\times W\times 3}$  (with  $H$  and  $W$  denoting height and width, respectively) into flattened visual token embeddings  $\mathbf{V}\in\mathbb{R}^{N_{v}\times D_{v}}$  , where  $N_{v}$  and  $D_{v}$  represent the number and dimensionality of vision tokens, respectively. | âœ… å®ƒå°†è¾“å…¥å›¾åƒ  $\mathbf{I}\in\mathbb{R}^{H\times W\times 3}$ ï¼ˆ $H$  å’Œ  $W$  åˆ†åˆ«è¡¨ç¤ºé«˜åº¦å’Œå®½åº¦ï¼‰å¤„ç†æˆæ‰å¹³çš„è§†è§‰æ ‡è®°åµŒå…¥  $\mathbf{V}\in\mathbb{R}^{N_{v}\times D_{v}}$ ï¼Œå…¶ä¸­  $N_{v}$  å’Œ  $D_{v}$  åˆ†åˆ«è¡¨ç¤ºè§†è§‰æ ‡è®°çš„æ•°é‡å’Œç»´æ•°ã€‚ |

#### 3.3 Multi-modality encoder decoder.

| ã€ç¬¬3.3èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬3.3èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We use a standard encoder-decoder transformer architecture to process visual and language token embeddings. | âœ… æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çš„ç¼–ç å™¨-è§£ç å™¨è½¬æ¢å™¨æ¶æ„æ¥å¤„ç†è§†è§‰å’Œè¯­è¨€æ ‡è®°åµŒå…¥ã€‚ |
| âœ… We first obtain prompt text embeddings  $\mathbf{T}_{prompt}\in\mathbf{R}^{N_{t}\times D}$  using our extended language tokenizer and word embedding layer ( **Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019.** ). | âœ… æˆ‘ä»¬é¦–å…ˆä½¿ç”¨æ‰©å±•çš„è¯­è¨€æ ‡è®°å™¨å’Œè¯åµŒå…¥å±‚ ( **Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019.** ) è·å¾—æç¤ºæ–‡æœ¬åµŒå…¥  $\mathbf{T}_{prompt}\in\mathbf{R}^{N_{t}\times D}$ ã€‚ |
| âœ… Then, we concatenate vision token embeddings with prompt embeddings to form the multi-modality encoder module input,  $\mathbf{X}=[\mathbf{V}^{\prime},\mathbf{T}_{prompt}]$  , where  $\mathbf{V}^{\prime}\in\mathbb{R}^{N_{v}\times D}$  is obtained by applying a linear projection and LayerNorm layer ( **Layer normalization, 2016.** ) to  $\mathbf{V}$  for dimensionality alignment. | âœ… ç„¶åï¼Œæˆ‘ä»¬å°†è§†è§‰æ ‡è®°åµŒå…¥ä¸æç¤ºåµŒå…¥è¿æ¥èµ·æ¥ä»¥å½¢æˆå¤šæ¨¡æ€ç¼–ç å™¨æ¨¡å—è¾“å…¥  $\mathbf{X}=[\mathbf{V}^{\prime},\mathbf{T}_{prompt}]$ ï¼Œå…¶ä¸­  $\mathbf{V}^{\prime}\in\mathbb{R}^{N_{v}\times D}$  æ˜¯é€šè¿‡åº”ç”¨çº¿æ€§æŠ•å½±å’Œ LayerNorm å±‚ ( **Layer normalization, 2016.** ) åˆ°  $\mathbf{V}$  è¿›è¡Œç»´åº¦å¯¹é½è·å¾—çš„ã€‚ |

#### 3.4 Optimization objective.

| ã€ç¬¬3.4èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬3.4èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Given the input  $x$  combined from the image and the prompt, and the target  $y$  , we use the standard language modeling with cross-entropy loss for all the tasks. | âœ… ç»™å®šç”±å›¾åƒå’Œæç¤ºç»„åˆè€Œæˆçš„è¾“å…¥  $x$  ä»¥åŠç›®æ ‡  $y$ ï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰ä»»åŠ¡ä½¿ç”¨å…·æœ‰äº¤å‰ç†µæŸå¤±çš„æ ‡å‡†è¯­è¨€å»ºæ¨¡ã€‚ |

**å…¬å¼(1):** 
$$ \mathcal{L}=-\sum_{i=1}^{|y|}logP_{\theta}(y_{i}|y_{<i},x) $$

| ã€ç¬¬3.4èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬3.4èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… where  $\theta$  are the network parameters,  $ \vert y \vert $  is the number of target tokens. | âœ… å…¶ä¸­ $\theta$ æ˜¯ç½‘ç»œå‚æ•°ï¼Œ $ \vert y \vert $ æ˜¯ç›®æ ‡ä»¤ç‰Œçš„æ•°é‡ã€‚ |

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/x3.png)

| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 3:  Florence-2  data engine consists of three essential phrases: (1) initial annotation employing specialist models, (2) data filtering to correct errors and remove irrelevant annotations, and (3) an iterative process for data refinement. Our final dataset (FLD-5B) of over 5B annotations contains 126M images, 500M text annotations, 1.3B region-text annotations, and 3.6B text-phrase-region annotations. | âœ… Figure 3:  Florence-2  data engine consists of three essential phrases: (1) initial annotation employing specialist models, (2) data filtering to correct errors and remove irrelevant annotations, and (3) an iterative process for data refinement. Our final dataset (FLD-5B) of over 5B annotations contains 126M images, 500M text annotations, 1.3B region-text annotations, and 3.6B text-phrase-region annotations. |

## 4 Data Engine

| ã€ç¬¬4èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬4èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… To train our Florence-2 model, we require a comprehensive, large-scale, high-quality multitask dataset encompassing various image data aspects. | âœ… ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„ Florence-2 æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¶µç›–å„ç§å›¾åƒæ•°æ®æ–¹é¢çš„å…¨é¢ã€å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„å¤šä»»åŠ¡æ•°æ®é›†ã€‚ |
| âœ… Given the scarcity of such data, we have developed a new multitask image dataset. | âœ… é‰´äºæ­¤ç±»æ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ–°çš„å¤šä»»åŠ¡å›¾åƒæ•°æ®é›†ã€‚ |
| âœ… This dataset FLD-5B includes 126M images, 500M text annotations, and 1.3B text-region annotations, and 3.6B text-phrase-region annotations across different tasks. | âœ… è¯¥æ•°æ®é›†FLD-5BåŒ…æ‹¬è·¨ä¸åŒä»»åŠ¡çš„126Må›¾åƒã€500Mæ–‡æœ¬æ³¨é‡Šã€1.3Bæ–‡æœ¬åŒºåŸŸæ³¨é‡Šå’Œ3.6Bæ–‡æœ¬çŸ­è¯­åŒºåŸŸæ³¨é‡Šã€‚ |
| âœ… We extensively explain our data collection and annotation procedures, encompassing adaptations for various annotation types. | âœ… æˆ‘ä»¬å¹¿æ³›è§£é‡Šäº†æˆ‘ä»¬çš„æ•°æ®æ”¶é›†å’Œæ³¨é‡Šç¨‹åºï¼Œæ¶µç›–äº†å¯¹å„ç§æ³¨é‡Šç±»å‹çš„é€‚åº”æ€§ã€‚ |
| âœ… The data engine pipeline, shown in FigureÂ 3 , will be discussed in subsequent sections. | âœ… FigureÂ 3 ä¸­æ‰€ç¤ºçš„æ•°æ®å¼•æ“ç®¡é“å°†åœ¨åç»­ç« èŠ‚ä¸­è®¨è®ºã€‚ |

### 4.1 Image Collection

| ã€ç¬¬4.1èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬4.1èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We construct our data by gathering a diverse collection of images from various sources. | âœ… æˆ‘ä»¬é€šè¿‡æ”¶é›†æ¥è‡ªå„ç§æ¥æºçš„å¤šæ ·åŒ–å›¾åƒæ¥æ„å»ºæ•°æ®ã€‚ |
| âœ… We begin with the identification of three key tasks that act as primary sources for our image corpus: image classification, object detection, and image captioning. | âœ… æˆ‘ä»¬é¦–å…ˆç¡®å®šä½œä¸ºå›¾åƒè¯­æ–™åº“ä¸»è¦æ¥æºçš„ä¸‰ä¸ªå…³é”®ä»»åŠ¡ï¼šå›¾åƒåˆ†ç±»ã€å¯¹è±¡æ£€æµ‹å’Œå›¾åƒå­—å¹•ã€‚ |
| âœ… Consequently, we curate and combine five distinct datasets originating from the aforementioned tasks: ImageNet-22k ( **Imagenet: A large-scale hierarchical image database.** ) , Object 365 ( **Objects365: A large-scale, high-quality dataset for object detection.** ) , Open Images ( **The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.** ) , Conceptual Captions ( **Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.** ) , and LAION ( **Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.** ) filtered by ( **Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.** ). | âœ… å› æ­¤ï¼Œæˆ‘ä»¬ä»ä¸Šè¿°ä»»åŠ¡ä¸­æ•´ç†å¹¶ç»„åˆäº†äº”ä¸ªä¸åŒçš„æ•°æ®é›†ï¼šImageNet-22k ( **Imagenet: A large-scale hierarchical image database.** )ã€Object 365 ( **Objects365: A large-scale, high-quality dataset for object detection.** )ã€Open Images ( **The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.** )ã€Conceptual Captions ( **Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.** ) å’Œé€šè¿‡ ( **Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.** ) è¿‡æ»¤çš„ LAION ( **Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.** )ã€‚ |
| âœ… This combination results in a dataset of 126 million images in total. | âœ… è¿™ç§ç»„åˆäº§ç”Ÿäº†æ€»è®¡ 1.26 äº¿å¼ å›¾åƒçš„æ•°æ®é›†ã€‚ |

### 4.2 Data Annotation

| ã€ç¬¬4.2èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬4.2èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Our primary objective is to generate comprehensive annotations that can support multitask learning effectively. | âœ… æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯ç”Ÿæˆèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒå¤šä»»åŠ¡å­¦ä¹ çš„ç»¼åˆæ³¨é‡Šã€‚ |
| âœ… Accordingly, our annotation endeavors span a comprehensive range of tasks, encapsulated within three discrete annotation categories: text , region-text pairs, and text-phrase-region triplets, which is illustrated in FigureÂ 4. | âœ… å› æ­¤ï¼Œæˆ‘ä»¬çš„æ³¨é‡Šå·¥ä½œæ¶µç›–äº†å¹¿æ³›çš„ä»»åŠ¡ï¼Œå°è£…åœ¨ä¸‰ä¸ªç¦»æ•£çš„æ³¨é‡Šç±»åˆ«ä¸­ï¼štextã€region-text å¯¹å’Œ text-phrase-region ä¸‰å…ƒç»„ï¼Œå¦‚ FigureÂ 4 ä¸­æ‰€ç¤ºã€‚ |
| âœ… The data annotation workflow consists of three essential phases, each of which ensures the accuracy and quality of the annotations: (1) initial annotation employing specialist models, (2) data filtering to correct errors and remove irrelevant annotations, and (3) an iterative process for data refinement. | âœ… æ•°æ®æ³¨é‡Šå·¥ä½œæµç¨‹åŒ…æ‹¬ä¸‰ä¸ªåŸºæœ¬é˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½ç¡®ä¿æ³¨é‡Šçš„å‡†ç¡®æ€§å’Œè´¨é‡ï¼šï¼ˆ1ï¼‰é‡‡ç”¨ä¸“å®¶æ¨¡å‹è¿›è¡Œåˆå§‹æ³¨é‡Šï¼Œï¼ˆ2ï¼‰æ•°æ®è¿‡æ»¤ä»¥çº æ­£é”™è¯¯å¹¶åˆ é™¤ä¸ç›¸å…³çš„æ³¨é‡Šï¼Œä»¥åŠï¼ˆ3ï¼‰æ•°æ®ç»†åŒ–çš„è¿­ä»£è¿‡ç¨‹ã€‚ |

#### 4.2.1 Initial annotation with specialist models.

| ã€ç¬¬4.2.1èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬4.2.1èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… To initiate the annotation process for each annotation type, we employ synthetic labels obtained from specialist models. | âœ… ä¸ºäº†å¯åŠ¨æ¯ç§æ³¨é‡Šç±»å‹çš„æ³¨é‡Šè¿‡ç¨‹ï¼Œæˆ‘ä»¬é‡‡ç”¨ä»ä¸“å®¶æ¨¡å‹è·å¾—çš„åˆæˆæ ‡ç­¾ã€‚ |
| âœ… These specialist models are a combination of offline models trained on a diverse range of publicly available datasets and online services hosted on cloud platforms. | âœ… è¿™äº›ä¸“ä¸šæ¨¡å‹æ˜¯åœ¨å„ç§å…¬å¼€æ•°æ®é›†ä¸Šè®­ç»ƒçš„ç¦»çº¿æ¨¡å‹å’Œæ‰˜ç®¡åœ¨äº‘å¹³å°ä¸Šçš„åœ¨çº¿æœåŠ¡çš„ç»„åˆã€‚ |
| âœ… They are specifically tailored to excel in annotating their respective annotation types. | âœ… å®ƒä»¬ç»è¿‡ä¸“é—¨å®šåˆ¶ï¼Œèƒ½å¤Ÿå‡ºè‰²åœ°æ³¨é‡Šå„è‡ªçš„æ³¨é‡Šç±»å‹ã€‚ |

| ã€ç¬¬4.2.1èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬4.2.1èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… It is worth noting that certain image datasets may already contain partial annotations for some annotation types. | âœ… å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒæŸäº›å›¾åƒæ•°æ®é›†å¯èƒ½å·²ç»åŒ…å«æŸäº›æ³¨é‡Šç±»å‹çš„éƒ¨åˆ†æ³¨é‡Šã€‚ |
| âœ… For instance, the Object 365 ( **Objects365: A large-scale, high-quality dataset for object detection.** ) dataset already includes human-annotated bounding boxes and corresponding categories as region-text annotations. | âœ… ä¾‹å¦‚ï¼ŒObject 365 ( **Objects365: A large-scale, high-quality dataset for object detection.** ) æ•°æ®é›†å·²ç»åŒ…å«äººå·¥æ³¨é‡Šçš„è¾¹ç•Œæ¡†å’Œç›¸åº”çš„ç±»åˆ«ä½œä¸ºåŒºåŸŸæ–‡æœ¬æ³¨é‡Šã€‚ |
| âœ… In such cases, we merge the pre-existing annotations with the synthetic labels generated by the specialist models. | âœ… åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†é¢„å…ˆå­˜åœ¨çš„æ³¨é‡Šä¸ä¸“å®¶æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ ‡ç­¾åˆå¹¶ã€‚ |
| âœ… This approach enhances the coverage and diversity of the annotations. | âœ… è¿™ç§æ–¹æ³•å¢å¼ºäº†æ³¨é‡Šçš„è¦†ç›–ç‡å’Œå¤šæ ·æ€§ã€‚ |

| ã€ç¬¬4.2.1èŠ‚ï¼Œç¬¬3æ®µã€‘åŸæ–‡ | ã€ç¬¬4.2.1èŠ‚ï¼Œç¬¬3æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Moreover, specific annotations, such as detailed descriptions in the text annotation type, are represented by datasets of a considerably small size. | âœ… æ­¤å¤–ï¼Œç‰¹å®šæ³¨é‡Šï¼ˆä¾‹å¦‚æ–‡æœ¬æ³¨é‡Šç±»å‹ä¸­çš„è¯¦ç»†æè¿°ï¼‰ç”±ç›¸å½“å°çš„æ•°æ®é›†è¡¨ç¤ºã€‚ |
| âœ… This inherently poses challenges in obtaining high-performance specialist models. | âœ… è¿™æœ¬è´¨ä¸Šå¯¹è·å¾—é«˜æ€§èƒ½ä¸“å®¶æ¨¡å‹å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ |
| âœ… Consequently, we opt to omit these tasks during the initial annotation phase. | âœ… å› æ­¤ï¼Œæˆ‘ä»¬é€‰æ‹©åœ¨åˆå§‹æ³¨é‡Šé˜¶æ®µçœç•¥è¿™äº›ä»»åŠ¡ã€‚ |
| âœ… Annotations for these tasks are generated later during the iterative data refinement process. | âœ… è¿™äº›ä»»åŠ¡çš„æ³¨é‡Šç¨ååœ¨è¿­ä»£æ•°æ®ç»†åŒ–è¿‡ç¨‹ä¸­ç”Ÿæˆã€‚ |

| ã€ç¬¬4.2.1èŠ‚ï¼Œç¬¬4æ®µã€‘åŸæ–‡ | ã€ç¬¬4.2.1èŠ‚ï¼Œç¬¬4æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… In summation, through the rigorous initial annotation procedures, we ensure that the aggregated dataset of 126 million images is comprehensively labeled across the majority of annotation types. | âœ… æ€»è€Œè¨€ä¹‹ï¼Œé€šè¿‡ä¸¥æ ¼çš„åˆå§‹æ³¨é‡Šç¨‹åºï¼Œæˆ‘ä»¬ç¡®ä¿ 1.26 äº¿å¼ å›¾åƒçš„èšåˆæ•°æ®é›†åœ¨å¤§å¤šæ•°æ³¨é‡Šç±»å‹ä¸­å¾—åˆ°å…¨é¢æ ‡è®°ã€‚ |

#### 4.2.2 Data filtering and enhancement.

| ã€ç¬¬4.2.2èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬4.2.2èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… The initial annotations obtained from the specialist models, while comprehensive, are susceptible to noise and imprecision. | âœ… ä»ä¸“å®¶æ¨¡å‹è·å¾—çš„åˆå§‹æ³¨é‡Šè™½ç„¶å…¨é¢ï¼Œä½†å®¹æ˜“å—åˆ°å™ªéŸ³å’Œä¸ç²¾ç¡®çš„å½±å“ã€‚ |
| âœ… In response to this challenge, we have implemented a multifaceted filtering process to refine and eliminate undesired annotations. | âœ… ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å®æ–½äº†å¤šæ–¹é¢çš„è¿‡æ»¤è¿‡ç¨‹æ¥æ”¹è¿›å’Œæ¶ˆé™¤ä¸éœ€è¦çš„æ³¨é‡Šã€‚ |
| âœ… Our general filtering protocol mainly focuses on two data types in the annotations: text and region data. | âœ… æˆ‘ä»¬çš„é€šç”¨è¿‡æ»¤åè®®ä¸»è¦å…³æ³¨æ³¨é‡Šä¸­çš„ä¸¤ç§æ•°æ®ç±»å‹ï¼šæ–‡æœ¬å’ŒåŒºåŸŸæ•°æ®ã€‚ |

| ã€ç¬¬4.2.2èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬4.2.2èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… First, pertaining to textual annotations, we are inspired by DiHT ( **Filtering, distillation, and hard negatives for vision-language pre-training.** ) and develop a parsing tool based on SpaCy ( **spacy: Industrial-strength natural language processing in python.** ) to extract objects, attributes, and actions. | âœ… é¦–å…ˆï¼Œå…³äºæ–‡æœ¬æ³¨é‡Šï¼Œæˆ‘ä»¬å—åˆ° DiHT ( **Filtering, distillation, and hard negatives for vision-language pre-training.** ) çš„å¯å‘ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªåŸºäº SpaCy ( **spacy: Industrial-strength natural language processing in python.** ) çš„è§£æå·¥å…·æ¥æå–å¯¹è±¡ã€å±æ€§å’ŒåŠ¨ä½œã€‚ |
| âœ… We filter out texts containing excessive objects, as they tend to introduce noise and may not accurately reflect the actual content in the corresponding images. | âœ… æˆ‘ä»¬è¿‡æ»¤æ‰åŒ…å«è¿‡å¤šå¯¹è±¡çš„æ–‡æœ¬ï¼Œå› ä¸ºå®ƒä»¬å¾€å¾€ä¼šå¼•å…¥å™ªéŸ³ï¼Œå¹¶ä¸”å¯èƒ½æ— æ³•å‡†ç¡®åæ˜ ç›¸åº”å›¾åƒä¸­çš„å®é™…å†…å®¹ã€‚ |
| âœ… Additionally, we assess the complexity of the actions and objects by measuring their degree of node in the dependency parsing tree. | âœ… æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æµ‹é‡ä¾èµ–è§£ææ ‘ä¸­çš„èŠ‚ç‚¹åº¦æ¥è¯„ä¼°åŠ¨ä½œå’Œå¯¹è±¡çš„å¤æ‚æ€§ã€‚ |
| âœ… We retain texts with a certain minimum action and object complexity to ensure the richness of visual concepts in the images. | âœ… æˆ‘ä»¬ä¿ç•™å…·æœ‰ä¸€å®šæœ€å°åŠ¨ä½œå’Œå¯¹è±¡å¤æ‚åº¦çš„æ–‡æœ¬ï¼Œä»¥ç¡®ä¿å›¾åƒä¸­è§†è§‰æ¦‚å¿µçš„ä¸°å¯Œæ€§ã€‚ |

| ã€ç¬¬4.2.2èŠ‚ï¼Œç¬¬3æ®µã€‘åŸæ–‡ | ã€ç¬¬4.2.2èŠ‚ï¼Œç¬¬3æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Second, in relation to the region annotations, specifically bounding boxes, we remove the noisy boxes under a confidence score threshold. | âœ… å…¶æ¬¡ï¼Œé’ˆå¯¹åŒºåŸŸæ³¨é‡Šï¼Œç‰¹åˆ«æ˜¯è¾¹ç•Œæ¡†ï¼Œæˆ‘ä»¬åˆ é™¤äº†ç½®ä¿¡åº¦åˆ†æ•°é˜ˆå€¼ä»¥ä¸‹çš„å™ªå£°æ¡†ã€‚ |
| âœ… Complementing this, we also employ non-maximum suppression to reduce redundant or overlapping bounding boxes. | âœ… é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨éæœ€å¤§æŠ‘åˆ¶æ¥å‡å°‘å†—ä½™æˆ–é‡å çš„è¾¹ç•Œæ¡†ã€‚ |

#### 4.2.3 Iterative data refinement.

| ã€ç¬¬4.2.3èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬4.2.3èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Using our filtered initial annotations, we trained a multitask model that processes sequences of data. | âœ… ä½¿ç”¨æˆ‘ä»¬è¿‡æ»¤çš„åˆå§‹æ³¨é‡Šï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå¤„ç†æ•°æ®åºåˆ—çš„å¤šä»»åŠ¡æ¨¡å‹ã€‚ |
| âœ… Upon evaluating this model against our training images, we discerned a marked enhancement in its predictions, particularly in instances where original labels were marred by inaccuracies or extraneous noise, such as in alt-texts. | âœ… åœ¨æ ¹æ®æˆ‘ä»¬çš„è®­ç»ƒå›¾åƒè¯„ä¼°è¯¥æ¨¡å‹åï¼Œæˆ‘ä»¬å‘ç°å…¶é¢„æµ‹æ•ˆæœæœ‰äº†æ˜æ˜¾å¢å¼ºï¼Œç‰¹åˆ«æ˜¯åœ¨åŸå§‹æ ‡ç­¾å› ä¸å‡†ç¡®æˆ–å¤–éƒ¨å™ªéŸ³ï¼ˆä¾‹å¦‚æ›¿ä»£æ–‡æœ¬ï¼‰è€Œå—æŸçš„æƒ…å†µä¸‹ã€‚ |
| âœ… Motivated by these findings, we integrated these updated annotations with our original ones and subjected the model to another training iteration. | âœ… åœ¨è¿™äº›å‘ç°çš„å¯å‘ä¸‹ï¼Œæˆ‘ä»¬å°†è¿™äº›æ›´æ–°çš„æ³¨é‡Šä¸æˆ‘ä»¬åŸæ¥çš„æ³¨é‡Šç›¸ç»“åˆï¼Œå¹¶å¯¹æ¨¡å‹è¿›è¡Œäº†å¦ä¸€æ¬¡è®­ç»ƒè¿­ä»£ã€‚ |
| âœ… This cyclical refinement process incrementally improves the quality of our training dataset. | âœ… è¿™ä¸ªå¾ªç¯çš„æ”¹è¿›è¿‡ç¨‹é€æ­¥æé«˜äº†æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®é›†çš„è´¨é‡ã€‚ |

| ã€ç¬¬4.2.3èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬4.2.3èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… In the case of tasks we initially bypassed due to insufficient data for the training of a robust specialist model, we leveraged the iteratively trained model for pre-training purposes. | âœ… å¯¹äºæˆ‘ä»¬æœ€åˆç”±äºæ•°æ®ä¸è¶³ä»¥è®­ç»ƒå¼ºå¤§çš„ä¸“å®¶æ¨¡å‹è€Œç»•è¿‡çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬åˆ©ç”¨è¿­ä»£è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€‚ |
| âœ… Subsequent fine-tuning of this pre-trained model with the sparse dataset showcased superior performance compared to a model trained from scratch on the same data. | âœ… ä½¿ç”¨ç¨€ç–æ•°æ®é›†å¯¹è¯¥é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œåç»­å¾®è°ƒï¼Œä¸ä½¿ç”¨ç›¸åŒæ•°æ®ä»å¤´å¼€å§‹è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œå…¶æ€§èƒ½æ›´ä¸ºå‡ºè‰²ã€‚ |
| âœ… Thus, we harness the fine-tuned model as a specialist for annotating our expansive dataset comprising 126 million images, ensuring comprehensive annotation coverage. | âœ… å› æ­¤ï¼Œæˆ‘ä»¬åˆ©ç”¨å¾®è°ƒæ¨¡å‹ä½œä¸ºä¸“å®¶æ¥æ³¨é‡Šæˆ‘ä»¬åŒ…å« 1.26 äº¿å¼ å›¾åƒçš„å¹¿æ³›æ•°æ®é›†ï¼Œç¡®ä¿å…¨é¢çš„æ³¨é‡Šè¦†ç›–ã€‚ |

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/x4.png)

| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 4:  An illustrative example of an image and its corresponding annotations in FLD-5B dataset. Each image in FLD-5B is annotated with text, region-text pairs, and text-phrase-region triplets by Florence data engine, which covers multiple spatial hierarchies, brief-to-detailed progressive granularity, and a wide semantics spectrum, enabling more comprehensive visual understanding from diverse perspectives. | âœ… Figure 4:  An illustrative example of an image and its corresponding annotations in FLD-5B dataset. Each image in FLD-5B is annotated with text, region-text pairs, and text-phrase-region triplets by Florence data engine, which covers multiple spatial hierarchies, brief-to-detailed progressive granularity, and a wide semantics spectrum, enabling more comprehensive visual understanding from diverse perspectives. |

### 4.3 Annotation-specific Variations

| ã€ç¬¬4.3èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬4.3èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… In SectionÂ 4.2 , we introduce our general annotation workflow. | âœ… åœ¨SectionÂ 4.2ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€èˆ¬çš„æ³¨é‡Šå·¥ä½œæµç¨‹ã€‚ |
| âœ… This section delves into each annotation type and the corresponding variations of the annotation procedure. | âœ… æœ¬èŠ‚æ·±å…¥æ¢è®¨æ¯ç§æ³¨é‡Šç±»å‹ä»¥åŠæ³¨é‡Šè¿‡ç¨‹çš„ç›¸åº”å˜åŒ–ã€‚ |

#### 4.3.1 Text.

| ã€ç¬¬4.3.1èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬4.3.1èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Text annotations categorize images using three types of granularities: brief, detailed, and more detailed. | âœ… æ–‡æœ¬æ³¨é‡Šä½¿ç”¨ä¸‰ç§ç²’åº¦å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ï¼šç®€è¦ã€è¯¦ç»†å’Œæ›´è¯¦ç»†ã€‚ |
| âœ… The brief text includes only one sentence that demonstrates the most salient objects and activities, which is similar to COCO caption ( **Microsoft coco captions: Data collection and evaluation server.** ). | âœ… ç®€çŸ­çš„æ–‡æœ¬ä»…åŒ…å«ä¸€å¥è¯ï¼Œå±•ç¤ºäº†æœ€çªå‡ºçš„ç‰©ä½“å’Œæ´»åŠ¨ï¼Œä¸ COCO æ ‡é¢˜ ( **Microsoft coco captions: Data collection and evaluation server.** ) ç±»ä¼¼ã€‚ |
| âœ… In contrast, the detailed text and more detailed text contain multiple sentences that describe the image with richer objects, attributes, and actions. | âœ… ç›¸æ¯”ä¹‹ä¸‹ï¼Œè¯¦ç»†æ–‡æœ¬å’Œæ›´è¯¦ç»†çš„æ–‡æœ¬åŒ…å«å¤šä¸ªå¥å­ï¼Œç”¨æ›´ä¸°å¯Œçš„å¯¹è±¡ã€å±æ€§å’ŒåŠ¨ä½œæ¥æè¿°å›¾åƒã€‚ |

| ã€ç¬¬4.3.1èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬4.3.1èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… For the brief text, a Florence-2 model is trained as the specialist on publicly available image caption and image-text datasets, creating an image-to-text model for initial annotations. | âœ… å¯¹äºç®€çŸ­çš„æ–‡æœ¬ï¼ŒFlorence-2 æ¨¡å‹ä½œä¸ºå…¬å¼€å¯ç”¨çš„å›¾åƒæ ‡é¢˜å’Œå›¾åƒæ–‡æœ¬æ•°æ®é›†çš„ä¸“å®¶è¿›è¡Œè®­ç»ƒï¼Œä»è€Œåˆ›å»ºç”¨äºåˆå§‹æ³¨é‡Šçš„å›¾åƒåˆ°æ–‡æœ¬æ¨¡å‹ã€‚ |
| âœ… Iterative refinement is used to minimize noise in these texts. | âœ… ä½¿ç”¨è¿­ä»£ç»†åŒ–æ¥æœ€å°åŒ–è¿™äº›æ–‡æœ¬ä¸­çš„å™ªéŸ³ã€‚ |
| âœ… For the detailed text, prompts including existing image annotations like the brief text and region-text annotations, are fed to large language models (LLMs) or large multimodal models (LMMs) to generate comprehensive descriptions. | âœ… å¯¹äºè¯¦ç»†æ–‡æœ¬ï¼ŒåŒ…æ‹¬ç°æœ‰å›¾åƒæ³¨é‡Šï¼ˆå¦‚ç®€çŸ­æ–‡æœ¬å’ŒåŒºåŸŸæ–‡æœ¬æ³¨é‡Šï¼‰çš„æç¤ºè¢«è¾“å…¥åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆ–å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ä»¥ç”Ÿæˆå…¨é¢çš„æè¿°ã€‚ |
| âœ… Due to the high cost of the large models, only a small set of detailed text and more detailed text are generated. | âœ… ç”±äºå¤§å‹æ¨¡å‹æˆæœ¬é«˜ï¼Œå› æ­¤åªèƒ½ç”Ÿæˆä¸€å°éƒ¨åˆ†è¯¦ç»†æ–‡æœ¬å’Œæ›´è¯¦ç»†çš„æ–‡æœ¬ã€‚ |
| âœ… These are used to fine-tune the caption specialist, developing a detailed description specialist for further annotations. | âœ… è¿™äº›ç”¨äºå¾®è°ƒå­—å¹•ä¸“å®¶ï¼Œå¼€å‘è¯¦ç»†çš„æè¿°ä¸“å®¶ä»¥ä¾›è¿›ä¸€æ­¥æ³¨é‡Šã€‚ |

#### 4.3.2 Region-text pairs.

| ã€ç¬¬4.3.2èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬4.3.2èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… The region-text pairs provide descriptive textual annotation for semantic regions in the image. | âœ… åŒºåŸŸ-æ–‡æœ¬å¯¹ä¸ºå›¾åƒä¸­çš„è¯­ä¹‰åŒºåŸŸæä¾›æè¿°æ€§æ–‡æœ¬æ³¨é‡Šã€‚ |
| âœ… Semantic regions include regions of visual objects as well as text regions. | âœ… è¯­ä¹‰åŒºåŸŸåŒ…æ‹¬è§†è§‰å¯¹è±¡åŒºåŸŸä»¥åŠæ–‡æœ¬åŒºåŸŸã€‚ |
| âœ… The region is represented by a tight bounding box surrounds the region. | âœ… è¯¥åŒºåŸŸç”±å›´ç»•è¯¥åŒºåŸŸçš„ç´§å¯†è¾¹ç•Œæ¡†è¡¨ç¤ºã€‚ |
| âœ… Moreover, each region can be annotated with varying degrees of granularity, including phrases and sentences, that contribute to a richer understanding of the region. | âœ… æ­¤å¤–ï¼Œæ¯ä¸ªåŒºåŸŸéƒ½å¯ä»¥ç”¨ä¸åŒç¨‹åº¦çš„ç²’åº¦è¿›è¡Œæ³¨é‡Šï¼ŒåŒ…æ‹¬çŸ­è¯­å’Œå¥å­ï¼Œä»è€Œæœ‰åŠ©äºæ›´æ·±å…¥åœ°äº†è§£è¯¥åŒºåŸŸã€‚ |

| ã€ç¬¬4.3.2èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬4.3.2èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Region-text pairs are annotated differently for text regions and visual object regions. | âœ… åŒºåŸŸ-æ–‡æœ¬å¯¹å¯¹äºæ–‡æœ¬åŒºåŸŸå’Œè§†è§‰å¯¹è±¡åŒºåŸŸçš„æ³¨é‡Šä¸åŒã€‚ |
| âœ… Text regions are labeled using Azure AI Servicesâ€™ OCR API ( **https://azure.microsoft.com/en-us/products/ai-services?activetab=pivot:azureopenaiservicetab.** ) , while visual objects are initially annotated with a DINO object detector ( **Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** ) trained on public datasets. | âœ… æ–‡æœ¬åŒºåŸŸä½¿ç”¨ Azure AI æœåŠ¡çš„ OCR API ( **https://azure.microsoft.com/en-us/products/ai-services?activetab=pivot:azureopenaiservicetab.** ) è¿›è¡Œæ ‡è®°ï¼Œè€Œè§†è§‰å¯¹è±¡æœ€åˆä½¿ç”¨åœ¨å…¬å…±æ•°æ®é›†ä¸Šè®­ç»ƒçš„ DINO å¯¹è±¡æ£€æµ‹å™¨ ( **Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** ) è¿›è¡Œæ³¨é‡Šã€‚ |
| âœ… Data filtering, including confidence thresholding and non-maximum suppression, removes noisy boxes. | âœ… æ•°æ®è¿‡æ»¤ï¼ˆåŒ…æ‹¬ç½®ä¿¡åº¦é˜ˆå€¼å’Œéæœ€å¤§æŠ‘åˆ¶ï¼‰å¯ä»¥æ¶ˆé™¤å™ªå£°æ¡†ã€‚ |
| âœ… Textual annotations for the visual object regions are further enriched by brief text generated from an image-to-text model with cropped image regions. | âœ… é€šè¿‡ä»å…·æœ‰è£å‰ªå›¾åƒåŒºåŸŸçš„å›¾åƒåˆ°æ–‡æœ¬æ¨¡å‹ç”Ÿæˆçš„ç®€çŸ­æ–‡æœ¬ï¼Œè¿›ä¸€æ­¥ä¸°å¯Œäº†è§†è§‰å¯¹è±¡åŒºåŸŸçš„æ–‡æœ¬æ³¨é‡Šã€‚ |
| âœ… Each region then receives three textual annotations: phrase from object category, brief text, and noun phrase chunks from the brief text. | âœ… ç„¶åï¼Œæ¯ä¸ªåŒºåŸŸä¼šæ”¶åˆ°ä¸‰ä¸ªæ–‡æœ¬æ³¨é‡Šï¼šæ¥è‡ªå¯¹è±¡ç±»åˆ«çš„çŸ­è¯­ã€ç®€çŸ­æ–‡æœ¬å’Œæ¥è‡ªç®€çŸ­æ–‡æœ¬çš„åè¯çŸ­è¯­å—ã€‚ |
| âœ… The Florence-1 ( **Florence: A new foundation model for computer vision.** ) model determines the most similar textual annotation to each image region. | âœ… Florence-1 ( **Florence: A new foundation model for computer vision.** ) æ¨¡å‹ç¡®å®šæ¯ä¸ªå›¾åƒåŒºåŸŸæœ€ç›¸ä¼¼çš„æ–‡æœ¬æ³¨é‡Šã€‚ |

#### 4.3.3 Text-phrase-region triplets.

| ã€ç¬¬4.3.3èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬4.3.3èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Text-phrase-region triplets consist of a descriptive text of the image, noun phrases in this text related to image objects, and region annotations for these objects. | âœ… æ–‡æœ¬-â€‹â€‹çŸ­è¯­-åŒºåŸŸä¸‰å…ƒç»„ç”±å›¾åƒçš„æè¿°æ€§æ–‡æœ¬ã€ä¸å›¾åƒå¯¹è±¡ç›¸å…³çš„æ–‡æœ¬ä¸­çš„åè¯çŸ­è¯­ä»¥åŠè¿™äº›å¯¹è±¡çš„åŒºåŸŸæ³¨é‡Šç»„æˆã€‚ |
| âœ… The text includes brief, detailed, and more detailed text generated earlier. | âœ… æ–‡æœ¬åŒ…æ‹¬å…ˆå‰ç”Ÿæˆçš„ç®€çŸ­ã€è¯¦ç»†å’Œæ›´è¯¦ç»†çš„æ–‡æœ¬ã€‚ |
| âœ… For each text, the Grounding DINO model ( **Grounding dino: Marrying dino with grounded pre-training for open-set object detection.** ) identifies noun phrases and creates bounding boxes for them. | âœ… å¯¹äºæ¯ç¯‡æ–‡æœ¬ï¼ŒGrounding DINO æ¨¡å‹ ( **Grounding dino: Marrying dino with grounded pre-training for open-set object detection.** ) ä¼šè¯†åˆ«åè¯çŸ­è¯­å¹¶ä¸ºå®ƒä»¬åˆ›å»ºè¾¹ç•Œæ¡†ã€‚ |
| âœ… Additionally, the SAM model ( **Segment anything.** ) generates segmentation masks for each box, offering more precise object localization. | âœ… æ­¤å¤–ï¼ŒSAM æ¨¡å‹ ( **Segment anything.** ) ä¸ºæ¯ä¸ªæ¡†ç”Ÿæˆåˆ†å‰²æ©ç ï¼Œæä¾›æ›´ç²¾ç¡®çš„å¯¹è±¡å®šä½ã€‚ |
| âœ… During data filtering, a confidence score threshold is applied to both noun phrases and bounding boxes to ensure relevance. | âœ… åœ¨æ•°æ®è¿‡æ»¤æœŸé—´ï¼Œå¯¹åè¯çŸ­è¯­å’Œè¾¹ç•Œæ¡†åº”ç”¨ç½®ä¿¡åº¦åˆ†æ•°é˜ˆå€¼ä»¥ç¡®ä¿ç›¸å…³æ€§ã€‚ |
| âœ… A blacklist is also used to exclude irrelevant noun phrases like pronouns and abstract concepts. | âœ… é»‘åå•è¿˜ç”¨äºæ’é™¤ä¸ç›¸å…³çš„åè¯çŸ­è¯­ï¼Œå¦‚ä»£è¯å’ŒæŠ½è±¡æ¦‚å¿µã€‚ |

<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.2"><tbody class="ltx_tbody"><tr class="ltx_tr" id="S4.T1.2.1.1"><td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T1.2.1.1.1" style="padding-left:7.5pt;padding-right:7.5pt;">Dataset</td><td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T1.2.1.1.2" style="padding-left:7.5pt;padding-right:7.5pt;">Rep. Model</td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S4.T1.2.1.1.3" style="padding-left:7.5pt;padding-right:7.5pt;">#Images</td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S4.T1.2.1.1.4" style="padding-left:7.5pt;padding-right:7.5pt;">#Annotations</td><td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T1.2.1.1.5" style="padding-left:7.5pt;padding-right:7.5pt;">Spatial hierarchy</td><td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.2.1.1.6" style="padding-left:7.5pt;padding-right:7.5pt;">Semantics granularity</td></tr><tr class="ltx_tr" id="S4.T1.2.2.2"><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.2.2.2.1" style="padding-left:7.5pt;padding-right:7.5pt;">JFT300MÂ <html><body><p>( <strong>An image is worth 16x16 words: Transformers for image recognition atscale, 2021.</strong> )</p></body></html></td><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.2.2.2.2" style="padding-left:7.5pt;padding-right:7.5pt;">ViT</td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T1.2.2.2.3" style="padding-left:7.5pt;padding-right:7.5pt;">300M</td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T1.2.2.2.4" style="padding-left:7.5pt;padding-right:7.5pt;">300M</td><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.2.2.2.5" style="padding-left:7.5pt;padding-right:7.5pt;">Image-level</td><td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.2.2.6" style="padding-left:7.5pt;padding-right:7.5pt;">Coarse</td></tr><tr class="ltx_tr" id="S4.T1.2.3.3"><td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.3.3.1" style="padding-left:7.5pt;padding-right:7.5pt;">WITÂ <html><body><p>( <strong>Learning transferable visual models from natural languagesupervision.</strong> )</p></body></html></td><td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.3.3.2" style="padding-left:7.5pt;padding-right:7.5pt;">CLIP</td><td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.2.3.3.3" style="padding-left:7.5pt;padding-right:7.5pt;">400M</td><td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.2.3.3.4" style="padding-left:7.5pt;padding-right:7.5pt;">400M</td><td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.3.3.5" style="padding-left:7.5pt;padding-right:7.5pt;">Image-level</td><td class="ltx_td ltx_align_left" id="S4.T1.2.3.3.6" style="padding-left:7.5pt;padding-right:7.5pt;">Coarse</td></tr><tr class="ltx_tr" id="S4.T1.2.4.4"><td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.4.4.1" style="padding-left:7.5pt;padding-right:7.5pt;">SA-1BÂ <html><body><p>( <strong>Segment anything.</strong> )</p></body></html></td><td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.4.4.2" style="padding-left:7.5pt;padding-right:7.5pt;">SAM</td><td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.2.4.4.3" style="padding-left:7.5pt;padding-right:7.5pt;">11M</td><td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.2.4.4.4" style="padding-left:7.5pt;padding-right:7.5pt;">1B</td><td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.4.4.5" style="padding-left:7.5pt;padding-right:7.5pt;">Region-level</td><td class="ltx_td ltx_align_left" id="S4.T1.2.4.4.6" style="padding-left:7.5pt;padding-right:7.5pt;">Non-semantic</td></tr><tr class="ltx_tr" id="S4.T1.2.5.5"><td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.5.5.1" style="padding-left:7.5pt;padding-right:7.5pt;">GrITÂ <html><body><p>( <strong>Kosmos-2: Grounding multimodal large language models to the world.</strong> )</p></body></html></td><td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.5.5.2" style="padding-left:7.5pt;padding-right:7.5pt;">Kosmos-2</td><td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.2.5.5.3" style="padding-left:7.5pt;padding-right:7.5pt;">91M</td><td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.2.5.5.4" style="padding-left:7.5pt;padding-right:7.5pt;">137M</td><td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.5.5.5" style="padding-left:7.5pt;padding-right:7.5pt;">Image &amp; Region-level</td><td class="ltx_td ltx_align_left" id="S4.T1.2.5.5.6" style="padding-left:7.5pt;padding-right:7.5pt;">Fine-grained</td></tr><tr class="ltx_tr" id="S4.T1.2.6.6"><td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.6.6.1" style="padding-left:7.5pt;padding-right:7.5pt;">M3WÂ <html><body><p>( <strong>Flamingo: a visual language model for few-shot learning.</strong> )</p></body></html></td><td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.6.6.2" style="padding-left:7.5pt;padding-right:7.5pt;">Flamingo</td><td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.2.6.6.3" style="padding-left:7.5pt;padding-right:7.5pt;">185M</td><td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.2.6.6.4" style="padding-left:7.5pt;padding-right:7.5pt;">43.3M*</td><td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.6.6.5" style="padding-left:7.5pt;padding-right:7.5pt;">Multi-image-level</td><td class="ltx_td ltx_align_left" id="S4.T1.2.6.6.6" style="padding-left:7.5pt;padding-right:7.5pt;">Fine-grained</td></tr><tr class="ltx_tr" id="S4.T1.2.7.7" style="background-color:#E6E6E6;"><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T1.2.7.7.1" style="padding-left:7.5pt;padding-right:7.5pt;"><em class="ltx_emph ltx_font_italic" id="S4.T1.2.7.7.1.1" style="background-color:#E6E6E6;">FLD-5B</em> (ours)</td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T1.2.7.7.2" style="padding-left:7.5pt;padding-right:7.5pt;"><em class="ltx_emph ltx_font_italic" id="S4.T1.2.7.7.2.1" style="background-color:#E6E6E6;">Florence-2</em> (ours)</td><td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S4.T1.2.7.7.3" style="padding-left:7.5pt;padding-right:7.5pt;">126M</td><td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S4.T1.2.7.7.4" style="padding-left:7.5pt;padding-right:7.5pt;">5B</td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T1.2.7.7.5" style="padding-left:7.5pt;padding-right:7.5pt;">Image &amp; Region-level</td><td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.2.7.7.6" style="padding-left:7.5pt;padding-right:7.5pt;">Coarse to fine-grained</td></tr></tbody></table>

| ã€è¡¨æ ‡é¢˜ã€‘åŸæ–‡ | ã€è¡¨æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Table 1:  Comparison with datasets in vision foundation model training. *Flamingoâ€™s annotations are counted in the number of documents, where each document may have multiple images. | âœ… Table 1:  Comparison with datasets in vision foundation model training. *Flamingoâ€™s annotations are counted in the number of documents, where each document may have multiple images. |

## 5 Dataset

| ã€ç¬¬5èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬5èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… This section introduces the statistics and analysis of FLD-5B that we built using the data engine in SectionÂ 4. | âœ… æœ¬èŠ‚ä»‹ç»æˆ‘ä»¬åˆ©ç”¨SectionÂ 4ä¸­çš„æ•°æ®å¼•æ“æ„å»ºçš„FLD-5Bçš„ç»Ÿè®¡å’Œåˆ†æã€‚ |
| âœ… We begin with an overview of the dataset and compare it with the recent works. | âœ… æˆ‘ä»¬é¦–å…ˆæ¦‚è¿°æ•°æ®é›†ï¼Œå¹¶å°†å…¶ä¸æœ€è¿‘çš„ç ”ç©¶è¿›è¡Œæ¯”è¾ƒã€‚ |
| âœ… We then show further analyses of detailed annotation statistics, semantic coverage and spatial coverage in the established dataset. | âœ… ç„¶åï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æå·²å»ºç«‹çš„æ•°æ®é›†ä¸­çš„è¯¦ç»†æ³¨é‡Šç»Ÿè®¡ã€è¯­ä¹‰è¦†ç›–ç‡å’Œç©ºé—´è¦†ç›–ç‡ã€‚ |

### 5.1 Overview

| ã€ç¬¬5.1èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬5.1èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Following the data engine, we build a large-scale training set ( FLD-5B ) of 126M images, more than 500M text annotations, 1.3B region-text annotations, and 3.6B text-phrase-region annotations. | âœ… æŒ‰ç…§æ•°æ®å¼•æ“ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«126Må¼ å›¾åƒçš„å¤§è§„æ¨¡è®­ç»ƒé›†ï¼ˆFLD-5Bï¼‰ã€è¶…è¿‡500Mçš„æ–‡æœ¬æ ‡æ³¨ã€1.3Bçš„åŒºåŸŸæ–‡æœ¬æ ‡æ³¨ã€ä»¥åŠ3.6Bçš„æ–‡æœ¬çŸ­è¯­åŒºåŸŸæ ‡æ³¨ã€‚ |
| âœ… Each image is annotated with text, region-text pairs, and text-phrase-region triplets and each annotation type has multiple instances varying in diverse granularity. | âœ… æ¯ä¸ªå›¾åƒéƒ½å¸¦æœ‰æ–‡æœ¬ã€åŒºåŸŸ-æ–‡æœ¬å¯¹å’Œæ–‡æœ¬-çŸ­è¯­-åŒºåŸŸä¸‰å…ƒç»„æ³¨é‡Šï¼Œå¹¶ä¸”æ¯ç§æ³¨é‡Šç±»å‹éƒ½æœ‰å¤šä¸ªä¸åŒç²’åº¦çš„å®ä¾‹ã€‚ |
| âœ… An illustrative example of an image and its corresponding annotations can be found in FigureÂ 4 . | âœ… åœ¨ FigureÂ 4 ä¸­å¯ä»¥æ‰¾åˆ°å›¾åƒåŠå…¶ç›¸åº”æ³¨é‡Šçš„è¯´æ˜æ€§ç¤ºä¾‹ã€‚ |

| ã€ç¬¬5.1èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬5.1èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We provide a comparison between our data set and the existing data sets that are commonly used for training foundation models in TableÂ 1. | âœ… æˆ‘ä»¬å¯¹æˆ‘ä»¬çš„æ•°æ®é›†å’Œå¸¸ç”¨äºè®­ç»ƒ TableÂ 1 åŸºç¡€æ¨¡å‹çš„ç°æœ‰æ•°æ®é›†è¿›è¡Œäº†æ¯”è¾ƒã€‚ |
| âœ… Our data set has several advantages over the previous ones, such as having more annotations in total and per image. | âœ… æˆ‘ä»¬çš„æ•°æ®é›†æ¯”ä»¥å‰çš„æ•°æ®é›†æœ‰å‡ ä¸ªä¼˜åŠ¿ï¼Œä¾‹å¦‚æ€»ä½“å’Œæ¯ä¸ªå›¾åƒæœ‰æ›´å¤šçš„æ³¨é‡Šã€‚ |
| âœ… Moreover, the annotations in our data set span multiple levels of spatial and semantic granularity, which allows for more diverse and comprehensive visual understanding tasks. | âœ… æ­¤å¤–ï¼Œæˆ‘ä»¬æ•°æ®é›†ä¸­çš„æ³¨é‡Šæ¶µç›–äº†å¤šä¸ªç©ºé—´å’Œè¯­ä¹‰ç²’åº¦çº§åˆ«ï¼Œä»è€Œå¯ä»¥å®ç°æ›´åŠ å¤šæ ·åŒ–å’Œå…¨é¢çš„è§†è§‰ç†è§£ä»»åŠ¡ã€‚ |

### 5.2 Data Analysis

#### 5.2.1 Annotation statistics.

| ã€ç¬¬5.2.1èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬5.2.1èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… The statistics for each annotation type within our dataset are presented in TableÂ 2 . | âœ… æˆ‘ä»¬çš„æ•°æ®é›†ä¸­æ¯ç§æ³¨é‡Šç±»å‹çš„ç»Ÿè®¡æ•°æ®éƒ½æ˜¾ç¤ºåœ¨ TableÂ 2 ä¸­ã€‚ |

| ã€ç¬¬5.2.1èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬5.2.1èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Firstly, we have around 500M text annotations, including brief, detailed, and more detailed texts with different lengths. | âœ… é¦–å…ˆï¼Œæˆ‘ä»¬æœ‰å¤§çº¦500Mä¸ªæ–‡æœ¬æ³¨é‡Šï¼ŒåŒ…æ‹¬ä¸åŒé•¿åº¦çš„ç®€çŸ­ã€è¯¦ç»†å’Œæ›´è¯¦ç»†çš„æ–‡æœ¬ã€‚ |
| âœ… It is noteworthy that our detailed and more detailed text has 4x and 9x number of tokens compared with the brief text that is similar to COCO captions ( **Microsoft coco captions: Data collection and evaluation server.** ). | âœ… å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸ç±»ä¼¼äº COCO å­—å¹• ( **Microsoft coco captions: Data collection and evaluation server.** ) çš„ç®€çŸ­æ–‡æœ¬ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„è¯¦ç»†å’Œæ›´è¯¦ç»†çš„æ–‡æœ¬å…·æœ‰ 4 å€å’Œ 9 å€çš„æ ‡è®°æ•°é‡ã€‚ |
| âœ… These lengthy annotations provide much richer information for comphrensive visual understanding. | âœ… è¿™äº›å†—é•¿çš„æ³¨é‡Šä¸ºå…¨é¢çš„è§†è§‰ç†è§£æä¾›äº†æ›´ä¸°å¯Œçš„ä¿¡æ¯ã€‚ |

| ã€ç¬¬5.2.1èŠ‚ï¼Œç¬¬3æ®µã€‘åŸæ–‡ | ã€ç¬¬5.2.1èŠ‚ï¼Œç¬¬3æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… In addition, our dataset has around 1.3B region-text annotations, which is more than 30x larger than the academic object detection datasets such as OpenImages ( **The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.** ) and Object 365 ( **Objects365: A large-scale, high-quality dataset for object detection.** ). | âœ… æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†æœ‰å¤§çº¦ 1.3B åŒºåŸŸæ–‡æœ¬æ³¨é‡Šï¼Œæ¯” OpenImages ( **The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.** ) å’Œ Object 365 ( **Objects365: A large-scale, high-quality dataset for object detection.** ) ç­‰å­¦æœ¯å¯¹è±¡æ£€æµ‹æ•°æ®é›†å¤§ 30 å€ä»¥ä¸Šã€‚ |
| âœ… On average, each image has around 5 regions, and each region is annotated with either a phrase or a relatively longer brief text. | âœ… å¹³å‡è€Œè¨€ï¼Œæ¯å¼ å›¾åƒæœ‰å¤§çº¦ 5 ä¸ªåŒºåŸŸï¼Œæ¯ä¸ªåŒºåŸŸéƒ½ç”¨çŸ­è¯­æˆ–ç›¸å¯¹è¾ƒé•¿çš„ç®€çŸ­æ–‡æœ¬è¿›è¡Œæ³¨é‡Šã€‚ |
| âœ… Note that the regional brief text (2.55 avg tokens) is shorter than typical brief text annotation (7.95 avg tokens), as the regional brief text annotation actually includes a mixture of phrase, noun chunks, and brief text based on the Florence-1 score. | âœ… è¯·æ³¨æ„ï¼ŒåŒºåŸŸç®€çŸ­æ–‡æœ¬ï¼ˆ2.55 ä¸ªå¹³å‡æ ‡è®°ï¼‰æ¯”å…¸å‹çš„ç®€çŸ­æ–‡æœ¬æ³¨é‡Šï¼ˆ7.95 ä¸ªå¹³å‡æ ‡è®°ï¼‰çŸ­ï¼Œå› ä¸ºåŒºåŸŸç®€çŸ­æ–‡æœ¬æ³¨é‡Šå®é™…ä¸ŠåŒ…æ‹¬åŸºäº Florence-1 åˆ†æ•°çš„çŸ­è¯­ã€åè¯å—å’Œç®€çŸ­æ–‡æœ¬çš„æ··åˆã€‚ |
| âœ… More details can be found from SectionÂ 4.3 - region-text pairs. | âœ… æ›´å¤šè¯¦ç»†ä¿¡æ¯å¯å‚è§ SectionÂ 4.3 - åŒºåŸŸ-æ–‡æœ¬å¯¹ã€‚ |

| ã€ç¬¬5.2.1èŠ‚ï¼Œç¬¬4æ®µã€‘åŸæ–‡ | ã€ç¬¬5.2.1èŠ‚ï¼Œç¬¬4æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Moreover, we collect text-phrase-region annotations that include more than 3.6B phrase-region pairs for the 500M text annotations. | âœ… æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸º 500M æ–‡æœ¬æ³¨é‡Šæ”¶é›†äº†åŒ…å«è¶…è¿‡ 3.6B çŸ­è¯­åŒºåŸŸå¯¹çš„æ–‡æœ¬çŸ­è¯­åŒºåŸŸæ³¨é‡Šã€‚ |
| âœ… Specifically, the brief text annotation has 4.27 average phrase-region pairs, while detailed and more detailed text annotation has more than 10 pairs, indicating that the richer text annotation covers more objects and their corresponding phrases in the text. | âœ… å…·ä½“æ¥è¯´ï¼Œç®€çŸ­çš„æ–‡æœ¬æ³¨é‡Šå¹³å‡æœ‰ 4.27 ä¸ªçŸ­è¯­-åŒºåŸŸå¯¹ï¼Œè€Œè¯¦ç»†å’Œæ›´è¯¦ç»†çš„æ–‡æœ¬æ³¨é‡Šå¹³å‡æœ‰ 10 å¯¹ä»¥ä¸Šï¼Œè¿™è¡¨æ˜æ›´ä¸°å¯Œçš„æ–‡æœ¬æ³¨é‡Šæ¶µç›–äº†æ–‡æœ¬ä¸­æ›´å¤šçš„å¯¹è±¡åŠå…¶å¯¹åº”çš„çŸ­è¯­ã€‚ |

<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.2"><thead class="ltx_thead"><tr class="ltx_tr" id="S5.T2.2.1.1"><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T2.2.1.1.1" style="padding:1.6pt 5.5pt;">Annotation Type</th><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T2.2.1.1.2" style="padding:1.6pt 5.5pt;">Text Type</th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T2.2.1.1.3" style="padding:1.6pt 5.5pt;">#Image Annotations</th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T2.2.1.1.4" style="padding:1.6pt 5.5pt;">#Avg Tokens</th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T2.2.1.1.5" style="padding:1.6pt 5.5pt;">#Regions</th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T2.2.1.1.6" style="padding:1.6pt 5.5pt;">#Avg Regions</th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S5.T2.2.1.1.7" style="padding:1.6pt 5.5pt;">#Avg Regional Tokens</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr" id="S5.T2.2.2.1"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.2.2.1.1" style="padding:1.6pt 5.5pt;">Text</th><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.2.2.1.2" style="padding:1.6pt 5.5pt;">Brief</th><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.2.2.1.3" style="padding:1.6pt 5.5pt;">235M</td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.2.2.1.4" style="padding:1.6pt 5.5pt;">7.95</td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.2.2.1.5" style="padding:1.6pt 5.5pt;">-</td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.2.2.1.6" style="padding:1.6pt 5.5pt;">-</td><td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.2.2.1.7" style="padding:1.6pt 5.5pt;">-</td></tr><tr class="ltx_tr" id="S5.T2.2.3.2"><th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T2.2.3.2.1" style="padding:1.6pt 5.5pt;"></th><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.2.3.2.2" style="padding:1.6pt 5.5pt;">Detailed</th><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.3.2.3" style="padding:1.6pt 5.5pt;">126M</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.3.2.4" style="padding:1.6pt 5.5pt;">31.65</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.3.2.5" style="padding:1.6pt 5.5pt;">-</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.3.2.6" style="padding:1.6pt 5.5pt;">-</td><td class="ltx_td ltx_align_right" id="S5.T2.2.3.2.7" style="padding:1.6pt 5.5pt;">-</td></tr><tr class="ltx_tr" id="S5.T2.2.4.3"><th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T2.2.4.3.1" style="padding:1.6pt 5.5pt;"></th><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.2.4.3.2" style="padding:1.6pt 5.5pt;">More detailed</th><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.4.3.3" style="padding:1.6pt 5.5pt;">126M</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.4.3.4" style="padding:1.6pt 5.5pt;">70.53</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.4.3.5" style="padding:1.6pt 5.5pt;">-</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.4.3.6" style="padding:1.6pt 5.5pt;">-</td><td class="ltx_td ltx_align_right" id="S5.T2.2.4.3.7" style="padding:1.6pt 5.5pt;">-</td></tr><tr class="ltx_tr" id="S5.T2.2.5.4"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.2.5.4.1" style="padding:1.6pt 5.5pt;">Region-Text</th><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.2.5.4.2" style="padding:1.6pt 5.5pt;">Phrase</th><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.2.5.4.3" style="padding:1.6pt 5.5pt;">126M</td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.2.5.4.4" style="padding:1.6pt 5.5pt;">-</td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.2.5.4.5" style="padding:1.6pt 5.5pt;">681M</td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.2.5.4.6" style="padding:1.6pt 5.5pt;">5.42</td><td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.2.5.4.7" style="padding:1.6pt 5.5pt;">1.19</td></tr><tr class="ltx_tr" id="S5.T2.2.6.5"><th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T2.2.6.5.1" style="padding:1.6pt 5.5pt;"></th><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.2.6.5.2" style="padding:1.6pt 5.5pt;">Brief</th><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.6.5.3" style="padding:1.6pt 5.5pt;">126M</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.6.5.4" style="padding:1.6pt 5.5pt;">-</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.6.5.5" style="padding:1.6pt 5.5pt;">681M</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.6.5.6" style="padding:1.6pt 5.5pt;">5.42</td><td class="ltx_td ltx_align_right" id="S5.T2.2.6.5.7" style="padding:1.6pt 5.5pt;">2.55</td></tr><tr class="ltx_tr" id="S5.T2.2.7.6"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.2.7.6.1" style="padding:1.6pt 5.5pt;">Text-Phrase-Region</th><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.2.7.6.2" style="padding:1.6pt 5.5pt;">Brief</th><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.2.7.6.3" style="padding:1.6pt 5.5pt;">235M</td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.2.7.6.4" style="padding:1.6pt 5.5pt;">7.95</td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.2.7.6.5" style="padding:1.6pt 5.5pt;">1007M</td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T2.2.7.6.6" style="padding:1.6pt 5.5pt;">4.27</td><td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.2.7.6.7" style="padding:1.6pt 5.5pt;">1.93</td></tr><tr class="ltx_tr" id="S5.T2.2.8.7"><th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T2.2.8.7.1" style="padding:1.6pt 5.5pt;"></th><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.2.8.7.2" style="padding:1.6pt 5.5pt;">Detailed</th><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.8.7.3" style="padding:1.6pt 5.5pt;">126M</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.8.7.4" style="padding:1.6pt 5.5pt;">31.65</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.8.7.5" style="padding:1.6pt 5.5pt;">1289M</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T2.2.8.7.6" style="padding:1.6pt 5.5pt;">10.25</td><td class="ltx_td ltx_align_right" id="S5.T2.2.8.7.7" style="padding:1.6pt 5.5pt;">1.49</td></tr><tr class="ltx_tr" id="S5.T2.2.9.8"><th class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T2.2.9.8.1" style="padding:1.6pt 5.5pt;"></th><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T2.2.9.8.2" style="padding:1.6pt 5.5pt;">More detailed</th><td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S5.T2.2.9.8.3" style="padding:1.6pt 5.5pt;">126M</td><td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S5.T2.2.9.8.4" style="padding:1.6pt 5.5pt;">70.53</td><td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S5.T2.2.9.8.5" style="padding:1.6pt 5.5pt;">1278M</td><td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S5.T2.2.9.8.6" style="padding:1.6pt 5.5pt;">10.17</td><td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T2.2.9.8.7" style="padding:1.6pt 5.5pt;">1.35</td></tr></tbody></table>

| ã€è¡¨æ ‡é¢˜ã€‘åŸæ–‡ | ã€è¡¨æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Table 2:  Annotation statistics of FLD-5B dataset. | âœ… Table 2:  FLD-5Bæ•°æ®é›†çš„æ³¨é‡Šç»Ÿè®¡ã€‚ |

#### 5.2.2 Semantic coverage.

| ã€ç¬¬5.2.2èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬5.2.2èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Our text annotations comprise various text types, addressing different levels of detail. | âœ… æˆ‘ä»¬çš„æ–‡æœ¬æ³¨é‡ŠåŒ…å«å„ç§æ–‡æœ¬ç±»å‹ï¼Œæ¶‰åŠä¸åŒçº§åˆ«çš„ç»†èŠ‚ã€‚ |
| âœ… To assess semantic coverage, we employ SpaCy ( **spacy: Industrial-strength natural language processing in python.** ) for tokenization and parsing, inspired by DiHT ( **Filtering, distillation, and hard negatives for vision-language pre-training.** ). | âœ… ä¸ºäº†è¯„ä¼°è¯­ä¹‰è¦†ç›–èŒƒå›´ï¼Œæˆ‘ä»¬é‡‡ç”¨ SpaCy ( **spacy: Industrial-strength natural language processing in python.** ) è¿›è¡Œæ ‡è®°å’Œè§£æï¼Œå—åˆ° DiHT ( **Filtering, distillation, and hard negatives for vision-language pre-training.** ) çš„å¯å‘ã€‚ |
| âœ… This process yields part-of-speech (POS) tags and the dependency parsing tree among tokens. | âœ… è¯¥è¿‡ç¨‹äº§ç”Ÿäº†è¯æ€§ (POS) æ ‡è®°å’Œæ ‡è®°ä¹‹é—´çš„ä¾èµ–å…³ç³»è§£ææ ‘ã€‚ |
| âœ… We establish heuristic rules based on POS tags, categorizing tokens into semantic element types, e.g. | âœ… æˆ‘ä»¬æ ¹æ® POS æ ‡ç­¾å»ºç«‹å¯å‘å¼è§„åˆ™ï¼Œå°†æ ‡è®°åˆ†ç±»ä¸ºè¯­ä¹‰å…ƒç´ ç±»å‹ e.gã€‚ |
| âœ…  , objects, attributes, actions, and proper nouns. | âœ… ã€å¯¹è±¡ã€å±æ€§ã€åŠ¨ä½œå’Œä¸“æœ‰åè¯ã€‚ |
| âœ… Additionally, we introduce the concept of token complexity , measured by the total degrees of the token in the dependency parsing tree when treated as an undirected graph. | âœ… æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº† token complexity çš„æ¦‚å¿µï¼Œå½“å°†ä¾èµ–å…³ç³»è§£ææ ‘è§†ä¸ºæ— å‘å›¾æ—¶ï¼Œä»¥æ ‡è®°åœ¨ä¾èµ–å…³ç³»è§£ææ ‘ä¸­çš„æ€»åº¦æ•°æ¥è¡¡é‡ã€‚ |
| âœ… This complexity reflects the richness of semantic connections. | âœ… è¿™ç§å¤æ‚æ€§åæ˜ äº†è¯­ä¹‰è”ç³»çš„ä¸°å¯Œæ€§ã€‚ |
| âœ… In our study, we focus on measuring the complexity of objects and actions. | âœ… åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹æµ‹é‡ç‰©ä½“å’ŒåŠ¨ä½œçš„å¤æ‚æ€§ã€‚ |

| ã€ç¬¬5.2.2èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬5.2.2èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… TableÂ 3 presents the statistics on the average number of semantic elements and their corresponding complexity. | âœ… TableÂ 3 æ˜¾ç¤ºäº†è¯­ä¹‰å…ƒç´ çš„å¹³å‡æ•°é‡åŠå…¶å¯¹åº”çš„å¤æ‚åº¦çš„ç»Ÿè®¡æ•°æ®ã€‚ |
| âœ… The results indicate that all measurements increase with the inclusion of more details in text annotations. | âœ… ç»“æœè¡¨æ˜ï¼Œéšç€æ–‡æœ¬æ³¨é‡Šä¸­åŒ…å«æ›´å¤šç»†èŠ‚ï¼Œæ‰€æœ‰æµ‹é‡å€¼éƒ½ä¼šå¢åŠ ã€‚ |
| âœ… Notably, average actions experience the most significant boost, with detailed and more detailed text exhibiting 7  $\times$  and 15  $\times$  increases, respectively, compared to brief text. | âœ… å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¹³å‡æ“ä½œç»å†äº†æœ€æ˜¾è‘—çš„æå‡ï¼Œä¸ç®€çŸ­æ–‡æœ¬ç›¸æ¯”ï¼Œè¯¦ç»†æ–‡æœ¬å’Œæ›´è¯¦ç»†çš„æ–‡æœ¬åˆ†åˆ«æ˜¾ç¤ºäº† 7  $\times$  å’Œ 15  $\times$  çš„å¢åŠ ã€‚ |
| âœ… This highlights the limitations of traditional brief text annotations in describing image actions. | âœ… è¿™å‡¸æ˜¾äº†ä¼ ç»Ÿç®€çŸ­æ–‡æœ¬æ³¨é‡Šåœ¨æè¿°å›¾åƒåŠ¨ä½œæ–¹é¢çš„å±€é™æ€§ã€‚ |
| âœ… Conversely, the increment in proper nouns is relatively low, potentially because specialists often describe objects more generally than using specific proper nouns. | âœ… ç›¸åï¼Œä¸“æœ‰åè¯çš„å¢é‡ç›¸å¯¹è¾ƒä½ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºä¸“å®¶é€šå¸¸æ›´ç¬¼ç»Ÿåœ°æè¿°å¯¹è±¡è€Œä¸æ˜¯ä½¿ç”¨ç‰¹å®šçš„ä¸“æœ‰åè¯ã€‚ |
| âœ… In terms of complexity measurements, both objects and actions show more semantic connections in detailed text annotations. | âœ… åœ¨å¤æ‚æ€§æµ‹é‡æ–¹é¢ï¼Œå¯¹è±¡å’ŒåŠ¨ä½œåœ¨è¯¦ç»†çš„æ–‡æœ¬æ³¨é‡Šä¸­éƒ½è¡¨ç°å‡ºæ›´å¤šçš„è¯­ä¹‰è”ç³»ã€‚ |
| âœ… The complexity of actions exhibits a higher improvement, aligning with our observation of the increasing number of actions. | âœ… åŠ¨ä½œçš„å¤æ‚æ€§è¡¨ç°å‡ºæ›´é«˜çš„æ”¹è¿›ï¼Œè¿™ä¸æˆ‘ä»¬å¯¹åŠ¨ä½œæ•°é‡ä¸æ–­å¢åŠ çš„è§‚å¯Ÿä¸€è‡´ã€‚ |

<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.2"><thead class="ltx_thead"><tr class="ltx_tr" id="S5.T3.2.1.1"><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.2.1.1.1" style="padding-left:5.1pt;padding-right:5.1pt;">Text Type</th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T3.2.1.1.2" style="padding-left:5.1pt;padding-right:5.1pt;">Brief</th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T3.2.1.1.3" style="padding-left:5.1pt;padding-right:5.1pt;">Detailed</th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S5.T3.2.1.1.4" style="padding-left:5.1pt;padding-right:5.1pt;">More detailed</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr" id="S5.T3.2.2.1"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.2.1.1" style="padding-left:5.1pt;padding-right:5.1pt;">#Image Annotations</th><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T3.2.2.1.2" style="padding-left:5.1pt;padding-right:5.1pt;">235M</td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T3.2.2.1.3" style="padding-left:5.1pt;padding-right:5.1pt;">126M</td><td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.2.2.1.4" style="padding-left:5.1pt;padding-right:5.1pt;">126M</td></tr><tr class="ltx_tr" id="S5.T3.2.3.2"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.2.3.2.1" style="padding-left:5.1pt;padding-right:5.1pt;">#Avg Tokens</th><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.2.3.2.2" style="padding-left:5.1pt;padding-right:5.1pt;">7.95</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.2.3.2.3" style="padding-left:5.1pt;padding-right:5.1pt;">31.65</td><td class="ltx_td ltx_align_right" id="S5.T3.2.3.2.4" style="padding-left:5.1pt;padding-right:5.1pt;">70.53</td></tr><tr class="ltx_tr" id="S5.T3.2.4.3"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.2.4.3.1" style="padding-left:5.1pt;padding-right:5.1pt;">#Avg Objects</th><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.2.4.3.2" style="padding-left:5.1pt;padding-right:5.1pt;">3.23</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.2.4.3.3" style="padding-left:5.1pt;padding-right:5.1pt;">13.31</td><td class="ltx_td ltx_align_right" id="S5.T3.2.4.3.4" style="padding-left:5.1pt;padding-right:5.1pt;">28.06</td></tr><tr class="ltx_tr" id="S5.T3.2.5.4"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.2.5.4.1" style="padding-left:5.1pt;padding-right:5.1pt;">#Avg Attributes</th><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.2.5.4.2" style="padding-left:5.1pt;padding-right:5.1pt;">2.80</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.2.5.4.3" style="padding-left:5.1pt;padding-right:5.1pt;">7.27</td><td class="ltx_td ltx_align_right" id="S5.T3.2.5.4.4" style="padding-left:5.1pt;padding-right:5.1pt;">16.25</td></tr><tr class="ltx_tr" id="S5.T3.2.6.5"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.2.6.5.1" style="padding-left:5.1pt;padding-right:5.1pt;">#Avg Actions</th><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.2.6.5.2" style="padding-left:5.1pt;padding-right:5.1pt;">0.58</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.2.6.5.3" style="padding-left:5.1pt;padding-right:5.1pt;">4.21</td><td class="ltx_td ltx_align_right" id="S5.T3.2.6.5.4" style="padding-left:5.1pt;padding-right:5.1pt;">8.76</td></tr><tr class="ltx_tr" id="S5.T3.2.7.6"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.2.7.6.1" style="padding-left:5.1pt;padding-right:5.1pt;">#Proper Nouns</th><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.2.7.6.2" style="padding-left:5.1pt;padding-right:5.1pt;">1.10</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.2.7.6.3" style="padding-left:5.1pt;padding-right:5.1pt;">2.40</td><td class="ltx_td ltx_align_right" id="S5.T3.2.7.6.4" style="padding-left:5.1pt;padding-right:5.1pt;">2.41</td></tr><tr class="ltx_tr" id="S5.T3.2.8.7"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.2.8.7.1" style="padding-left:5.1pt;padding-right:5.1pt;">Avg Object Complexity</th><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.2.8.7.2" style="padding-left:5.1pt;padding-right:5.1pt;">2.80</td><td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.2.8.7.3" style="padding-left:5.1pt;padding-right:5.1pt;">4.00</td><td class="ltx_td ltx_align_right" id="S5.T3.2.8.7.4" style="padding-left:5.1pt;padding-right:5.1pt;">4.02</td></tr><tr class="ltx_tr" id="S5.T3.2.9.8"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T3.2.9.8.1" style="padding-left:5.1pt;padding-right:5.1pt;">Avg Action Complexity</th><td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S5.T3.2.9.8.2" style="padding-left:5.1pt;padding-right:5.1pt;">1.14</td><td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S5.T3.2.9.8.3" style="padding-left:5.1pt;padding-right:5.1pt;">3.63</td><td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T3.2.9.8.4" style="padding-left:5.1pt;padding-right:5.1pt;">4.38</td></tr></tbody></table>

| ã€è¡¨æ ‡é¢˜ã€‘åŸæ–‡ | ã€è¡¨æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Table 3:  Statistics of the average number of semantic elements and corresponding complexity in FLD-5B dataset. | âœ… Table 3:  FLD-5Bæ•°æ®é›†ä¸­è¯­ä¹‰å…ƒç´ çš„å¹³å‡æ•°é‡åŠç›¸åº”çš„å¤æ‚åº¦ç»Ÿè®¡ã€‚ |

#### 5.2.3 Spatial coverage.

| ã€ç¬¬5.2.3èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬5.2.3èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Our region-text and text-phrase-region annotations, represented by bounding boxes and masks, capture the location of visual concepts within images. | âœ… æˆ‘ä»¬çš„åŒºåŸŸæ–‡æœ¬å’Œæ–‡æœ¬çŸ­è¯­åŒºåŸŸæ³¨é‡Šä»¥è¾¹ç•Œæ¡†å’Œè’™ç‰ˆè¡¨ç¤ºï¼Œæ•æ‰å›¾åƒå†…è§†è§‰æ¦‚å¿µçš„ä½ç½®ã€‚ |
| âœ… The distribution of box areas, as shown in FigureÂ 5(a) , reveals more small boxes in region-text pairs and a uniform box size distribution in text-phrase-region triplets. | âœ… æ¡†åŒºåŸŸåˆ†å¸ƒå¦‚ FigureÂ 5(a) æ‰€ç¤ºï¼Œè¡¨æ˜åŒºåŸŸ-æ–‡æœ¬å¯¹ä¸­å­˜åœ¨æ›´å¤šå°æ¡†ï¼Œè€Œæ–‡æœ¬-çŸ­è¯­-åŒºåŸŸä¸‰å…ƒç»„ä¸­çš„æ¡†å¤§å°åˆ†å¸ƒå‡åŒ€ã€‚ |
| âœ… This difference stems from the the divergent origins of these boxes: object detectors for region-text pairs and a grounding model for text-phrase-region triplets, which aligns boxes to textual phrases representing both localized and overarching image concepts. | âœ… è¿™ç§å·®å¼‚æºäºè¿™äº›æ¡†çš„ä¸åŒæ¥æºï¼šç”¨äºåŒºåŸŸ-æ–‡æœ¬å¯¹çš„å¯¹è±¡æ£€æµ‹å™¨å’Œç”¨äºæ–‡æœ¬-çŸ­è¯­-åŒºåŸŸä¸‰å…ƒç»„çš„æ¥åœ°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†æ¡†ä¸ä»£è¡¨å±€éƒ¨å’Œæ€»ä½“å›¾åƒæ¦‚å¿µçš„æ–‡æœ¬çŸ­è¯­å¯¹é½ã€‚ |
| âœ… In FigureÂ 5(b) , the log-format distribution of aspect ratios is illustrated. | âœ… åœ¨ FigureÂ 5(b) ä¸­ï¼Œæ˜¾ç¤ºäº†é•¿å®½æ¯”çš„å¯¹æ•°æ ¼å¼åˆ†å¸ƒã€‚ |
| âœ… Region-text pairs and text-phrase-region triplets exhibit similar symmetric distributions, covering a wide range of aspect ratios. | âœ… åŒºåŸŸ-æ–‡æœ¬å¯¹å’Œæ–‡æœ¬-çŸ­è¯­-åŒºåŸŸä¸‰å…ƒç»„è¡¨ç°å‡ºç›¸ä¼¼çš„å¯¹ç§°åˆ†å¸ƒï¼Œæ¶µç›–äº†å¹¿æ³›çš„çºµæ¨ªæ¯”ã€‚ |
| âœ… Heatmaps of the box center for each annotation type, shown in Figures.Â 5(c) and 5(d) , indicate a center bias, with region-text pairs displaying a more uniform distribution than text-phrase-region triplets. | âœ… Figures.Â 5(c) å’Œ 5(d) ä¸­æ˜¾ç¤ºçš„æ¯ç§æ³¨é‡Šç±»å‹çš„æ¡†ä¸­å¿ƒçƒ­å›¾è¡¨æ˜å­˜åœ¨ä¸­å¿ƒåå·®ï¼Œå…¶ä¸­åŒºåŸŸ-æ–‡æœ¬å¯¹æ¯”æ–‡æœ¬-çŸ­è¯­-åŒºåŸŸä¸‰å…ƒç»„æ˜¾ç¤ºå‡ºæ›´å‡åŒ€çš„åˆ†å¸ƒã€‚ |

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/x5.png)

| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… (a)  | âœ… (a)  |

## 6 Experiments

| ã€ç¬¬6èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬6èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Our Florence-2 models are trained on FLD-5B to learn a universal image representation. | âœ… æˆ‘ä»¬çš„ Florence-2 æ¨¡å‹åœ¨ FLD-5B ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥å­¦ä¹ é€šç”¨å›¾åƒè¡¨ç¤ºã€‚ |
| âœ… We conduct our experiments in three main parts: (1) We evaluate the zero-shot performance of our method on various tasks to show its inherent ability to handle multiple tasks without any extra fine-tuning on task-specific data using one single generalist model. | âœ… æˆ‘ä»¬çš„å®éªŒä¸»è¦åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§ä»»åŠ¡ä¸Šçš„ zero-shot æ€§èƒ½ï¼Œä»¥æ˜¾ç¤ºå…¶å¤„ç†å¤šé¡¹ä»»åŠ¡çš„å›ºæœ‰èƒ½åŠ›ï¼Œè€Œæ— éœ€ä½¿ç”¨ one single generalist æ¨¡å‹å¯¹ç‰¹å®šäºä»»åŠ¡çš„æ•°æ®è¿›è¡Œä»»ä½•é¢å¤–çš„å¾®è°ƒã€‚ |
| âœ… (2) We show the adaptability of our method by further training one single generalist model with additional supervised data on a wide range of tasks, achieving competitive state-of-the-art performance. | âœ… ï¼ˆ2ï¼‰æˆ‘ä»¬é€šè¿‡åœ¨å¹¿æ³›ä»»åŠ¡ä¸Šä½¿ç”¨é¢å¤–çš„ç›‘ç£æ•°æ®è¿›ä¸€æ­¥è®­ç»ƒ one single generalist æ¨¡å‹æ¥å±•ç¤ºæˆ‘ä»¬æ–¹æ³•çš„é€‚åº”æ€§ï¼Œå¹¶å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ |
| âœ… (3) We examine the performance of the learned visual representation on the downstream tasks as the backbone to show the superiority of our pre-training method over previous approaches. | âœ… ï¼ˆ3ï¼‰æˆ‘ä»¬ä»¥å­¦ä¹ åˆ°çš„è§†è§‰è¡¨å¾åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ºéª¨å¹²ï¼Œå±•ç¤ºæˆ‘ä»¬çš„é¢„è®­ç»ƒæ–¹æ³•ç›¸å¯¹äºä»¥å‰çš„æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚ |

### 6.1 Setup

| ã€ç¬¬6.1èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬6.1èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We investigate two model variants with different sizes: Florence-2-B model with 232 million parameters and Florence-2-L model with 771 million parameters. | âœ… æˆ‘ä»¬ç ”ç©¶äº†ä¸¤ç§ä¸åŒå¤§å°çš„æ¨¡å‹å˜ä½“ï¼šå…·æœ‰ 2.32 äº¿ä¸ªå‚æ•°çš„ Florence-2-B æ¨¡å‹å’Œå…·æœ‰ 7.71 äº¿ä¸ªå‚æ•°çš„ Florence-2-L æ¨¡å‹ã€‚ |
| âœ… The detailed architectures of each model are given in TableÂ 15. | âœ… æ¯ä¸ªæ¨¡å‹çš„è¯¦ç»†æ¶æ„åœ¨TableÂ 15ä¸­ç»™å‡ºã€‚ |
| âœ… We initialize the weights of the image encoder and multi-modality encoder-decoder from UniCL ( **Unified contrastive learning in image-text-label space, 2022.** ) and BART ( **Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019.** ) , respectively. | âœ… æˆ‘ä»¬åˆ†åˆ«ä» UniCL ( **Unified contrastive learning in image-text-label space, 2022.** ) å’Œ BART ( **Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019.** ) åˆå§‹åŒ–å›¾åƒç¼–ç å™¨å’Œå¤šæ¨¡æ€ç¼–ç å™¨-è§£ç å™¨çš„æƒé‡ã€‚ |

| ã€ç¬¬6.1èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬6.1èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We adopt AdamW ( **Decoupled weight decay regularization, 2019.** ) with cosine learning rate decay ( **Sgdr: Stochastic gradient descent with warm restarts, 2017.** ) for training our models. | âœ… æˆ‘ä»¬é‡‡ç”¨ AdamW ( **Decoupled weight decay regularization, 2019.** ) å’Œä½™å¼¦å­¦ä¹ ç‡è¡°å‡ ( **Sgdr: Stochastic gradient descent with warm restarts, 2017.** ) æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚ |
| âœ… We leverage Deepspeed ( **Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.** ) and mixed precision to improve the training efficiency. | âœ… æˆ‘ä»¬åˆ©ç”¨ Deepspeed ( **Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.** ) å’Œæ··åˆç²¾åº¦æ¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚ |
| âœ… The maximum learning rate is set at  $1e-4$  for the base model and  $1e-5$  for the large model. | âœ… åŸºç¡€æ¨¡å‹çš„æœ€å¤§å­¦ä¹ ç‡è®¾ç½®ä¸º  $1e-4$ ï¼Œå¤§å‹æ¨¡å‹çš„æœ€å¤§å­¦ä¹ ç‡è®¾ç½®ä¸º  $1e-5$ ã€‚ |
| âœ… A linear warm-up to the maximum learning rate is applied during the first 5,000 optimization steps. | âœ… åœ¨å‰ 5,000 ä¸ªä¼˜åŒ–æ­¥éª¤ä¸­ï¼Œåº”ç”¨çº¿æ€§é¢„çƒ­è‡³æœ€å¤§å­¦ä¹ ç‡ã€‚ |

| ã€ç¬¬6.1èŠ‚ï¼Œç¬¬3æ®µã€‘åŸæ–‡ | ã€ç¬¬6.1èŠ‚ï¼Œç¬¬3æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We train our models with a mini-batch size of 2048/3072 (base/large) and an image size of 384  $\times$  384 until reaching 3 billion effective training samples. | âœ… æˆ‘ä»¬ä½¿ç”¨ 2048/3072ï¼ˆåŸºç¡€/å¤§ï¼‰çš„å°æ‰¹é‡å’Œ 384  $\times$  384 çš„å›¾åƒå¤§å°æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œç›´åˆ°è¾¾åˆ° 30 äº¿æœ‰æ•ˆè®­ç»ƒæ ·æœ¬ã€‚ |
| âœ… Similar to ( **1. Pali: A jointly-scaled multilingual language-image model, 2022.** ï½œ **2. Learning transferable visual models from natural language supervision.** ï½œ **3. Scaling up visual and vision-language representation learning with noisy text supervision, 2021.** ï½œ **4. Florence: A new foundation model for computer vision.** ï½œ **5. Coca: Contrastive captioners are image-text foundation models, 2022.** ) , we further conduct high-resolution tuning with an image size of 768  $\times$  768 for 0.5 billion samples for the base model and 0.1 billion samples for the large model. | âœ… ä¸ ( **1. Pali: A jointly-scaled multilingual language-image model, 2022.** ï½œ **2. Learning transferable visual models from natural language supervision.** ï½œ **3. Scaling up visual and vision-language representation learning with noisy text supervision, 2021.** ï½œ **4. Florence: A new foundation model for computer vision.** ï½œ **5. Coca: Contrastive captioners are image-text foundation models, 2022.** ) ç±»ä¼¼ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¯¹å›¾åƒå¤§å°ä¸º 768  $\times$  768 çš„åŸºç¡€æ¨¡å‹è¿›è¡Œ 5 äº¿ä¸ªæ ·æœ¬çš„é«˜åˆ†è¾¨ç‡è°ƒä¼˜ï¼Œå¯¹å¤§å‹æ¨¡å‹è¿›è¡Œ 1 äº¿ä¸ªæ ·æœ¬çš„é«˜åˆ†è¾¨ç‡è°ƒä¼˜ã€‚ |

### 6.2 Zero-shot Evaluation Across Tasks

<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T4.2"><tbody class="ltx_tbody"><tr class="ltx_tr" id="S6.T4.2.1.1"><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T4.2.1.1.1" rowspan="3" style="padding-left:2.4pt;padding-right:2.4pt;">Method</th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T4.2.1.1.2" rowspan="3" style="padding-left:2.4pt;padding-right:2.4pt;">#params</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.3" style="padding-left:2.4pt;padding-right:2.4pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.4" style="padding-left:2.4pt;padding-right:2.4pt;">COCO Cap.</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.5" style="padding-left:2.4pt;padding-right:2.4pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.6" style="padding-left:2.4pt;padding-right:2.4pt;">NoCaps</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.7" style="padding-left:2.4pt;padding-right:2.4pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.8" style="padding-left:2.4pt;padding-right:2.4pt;">TextCaps</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.9" style="padding-left:2.4pt;padding-right:2.4pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.10" style="padding-left:2.4pt;padding-right:2.4pt;">COCO Det.</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.11" style="padding-left:2.4pt;padding-right:2.4pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.12" style="padding-left:2.4pt;padding-right:2.4pt;">Flickr30k</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.13" style="padding-left:2.4pt;padding-right:2.4pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S6.T4.2.1.1.14" style="padding-left:2.4pt;padding-right:2.4pt;">Refcoco</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.15" style="padding-left:2.4pt;padding-right:2.4pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S6.T4.2.1.1.16" style="padding-left:2.4pt;padding-right:2.4pt;">Refcoco+</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.17" style="padding-left:2.4pt;padding-right:2.4pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S6.T4.2.1.1.18" style="padding-left:2.4pt;padding-right:2.4pt;">Refcocog</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.19" style="padding-left:2.4pt;padding-right:2.4pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.1.1.20" style="padding-left:2.4pt;padding-right:2.4pt;">Refcoco RES</th></tr><tr class="ltx_tr" id="S6.T4.2.2.2"><td class="ltx_td" id="S6.T4.2.2.2.1" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.2.2" style="padding-left:2.4pt;padding-right:2.4pt;">test</td><td class="ltx_td" id="S6.T4.2.2.2.3" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.2.4" style="padding-left:2.4pt;padding-right:2.4pt;">val</td><td class="ltx_td" id="S6.T4.2.2.2.5" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.2.6" style="padding-left:2.4pt;padding-right:2.4pt;">val</td><td class="ltx_td" id="S6.T4.2.2.2.7" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.2.8" style="padding-left:2.4pt;padding-right:2.4pt;">val2017</td><td class="ltx_td" id="S6.T4.2.2.2.9" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.2.10" style="padding-left:2.4pt;padding-right:2.4pt;">test</td><td class="ltx_td" id="S6.T4.2.2.2.11" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.2.12" style="padding-left:2.4pt;padding-right:2.4pt;">val</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.2.13" style="padding-left:2.4pt;padding-right:2.4pt;">test-A</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.2.14" style="padding-left:2.4pt;padding-right:2.4pt;">test-B</td><td class="ltx_td" id="S6.T4.2.2.2.15" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.2.16" style="padding-left:2.4pt;padding-right:2.4pt;">val</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.2.17" style="padding-left:2.4pt;padding-right:2.4pt;">test-A</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.2.18" style="padding-left:2.4pt;padding-right:2.4pt;">test-B</td><td class="ltx_td" id="S6.T4.2.2.2.19" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.2.20" style="padding-left:2.4pt;padding-right:2.4pt;">val</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.2.21" style="padding-left:2.4pt;padding-right:2.4pt;">test</td><td class="ltx_td" id="S6.T4.2.2.2.22" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.2.23" style="padding-left:2.4pt;padding-right:2.4pt;">val</td></tr><tr class="ltx_tr" id="S6.T4.2.3.3"><td class="ltx_td" id="S6.T4.2.3.3.1" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" id="S6.T4.2.3.3.2" style="padding-left:2.4pt;padding-right:2.4pt;">CIDEr</td><td class="ltx_td" id="S6.T4.2.3.3.3" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" id="S6.T4.2.3.3.4" style="padding-left:2.4pt;padding-right:2.4pt;">CIDEr</td><td class="ltx_td" id="S6.T4.2.3.3.5" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" id="S6.T4.2.3.3.6" style="padding-left:2.4pt;padding-right:2.4pt;">CIDEr</td><td class="ltx_td" id="S6.T4.2.3.3.7" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" id="S6.T4.2.3.3.8" style="padding-left:2.4pt;padding-right:2.4pt;">mAP</td><td class="ltx_td" id="S6.T4.2.3.3.9" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" id="S6.T4.2.3.3.10" style="padding-left:2.4pt;padding-right:2.4pt;">R@1</td><td class="ltx_td" id="S6.T4.2.3.3.11" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" colspan="3" id="S6.T4.2.3.3.12" style="padding-left:2.4pt;padding-right:2.4pt;">Accuracy</td><td class="ltx_td" id="S6.T4.2.3.3.13" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" colspan="3" id="S6.T4.2.3.3.14" style="padding-left:2.4pt;padding-right:2.4pt;">Accuracy</td><td class="ltx_td" id="S6.T4.2.3.3.15" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" colspan="2" id="S6.T4.2.3.3.16" style="padding-left:2.4pt;padding-right:2.4pt;">Accuracy</td><td class="ltx_td" id="S6.T4.2.3.3.17" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" id="S6.T4.2.3.3.18" style="padding-left:2.4pt;padding-right:2.4pt;">mIoU</td></tr><tr class="ltx_tr" id="S6.T4.2.4.4"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T4.2.4.4.1" style="padding-left:2.4pt;padding-right:2.4pt;">FlamingoÂ <html><body><p>( <strong>Flamingo: a visual language model for few-shot learning.</strong> )</p></body></html></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T4.2.4.4.2" style="padding-left:2.4pt;padding-right:2.4pt;">80B</th><td class="ltx_td ltx_border_t" id="S6.T4.2.4.4.3" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.4.4.4" style="padding-left:2.4pt;padding-right:2.4pt;">84.3</td><td class="ltx_td ltx_border_t" id="S6.T4.2.4.4.5" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.4.4.6" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td ltx_border_t" id="S6.T4.2.4.4.7" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.4.4.8" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td ltx_border_t" id="S6.T4.2.4.4.9" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.4.4.10" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td ltx_border_t" id="S6.T4.2.4.4.11" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.4.4.12" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td ltx_border_t" id="S6.T4.2.4.4.13" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.4.4.14" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.4.4.15" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.4.4.16" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td ltx_border_t" id="S6.T4.2.4.4.17" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.4.4.18" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.4.4.19" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.4.4.20" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td ltx_border_t" id="S6.T4.2.4.4.21" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.4.4.22" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.4.4.23" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td ltx_border_t" id="S6.T4.2.4.4.24" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.4.4.25" style="padding-left:2.4pt;padding-right:2.4pt;">-</td></tr><tr class="ltx_tr" id="S6.T4.2.5.5"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T4.2.5.5.1" style="padding-left:2.4pt;padding-right:2.4pt;">Kosmos-2Â <html><body><p>( <strong>Kosmos-2: Grounding multimodal large language models to the world.</strong> )</p></body></html></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T4.2.5.5.2" style="padding-left:2.4pt;padding-right:2.4pt;">1.6B</th><td class="ltx_td" id="S6.T4.2.5.5.3" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" id="S6.T4.2.5.5.4" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td" id="S6.T4.2.5.5.5" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" id="S6.T4.2.5.5.6" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td" id="S6.T4.2.5.5.7" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" id="S6.T4.2.5.5.8" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td" id="S6.T4.2.5.5.9" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" id="S6.T4.2.5.5.10" style="padding-left:2.4pt;padding-right:2.4pt;">-</td><td class="ltx_td" id="S6.T4.2.5.5.11" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" id="S6.T4.2.5.5.12" style="padding-left:2.4pt;padding-right:2.4pt;">78.7</td><td class="ltx_td" id="S6.T4.2.5.5.13" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" id="S6.T4.2.5.5.14" style="padding-left:2.4pt;padding-right:2.4pt;">52.3</td><td class="ltx_td ltx_align_center" id="S6.T4.2.5.5.15" style="padding-left:2.4pt;padding-right:2.4pt;">57.4</td><td class="ltx_td ltx_align_center" id="S6.T4.2.5.5.16" style="padding-left:2.4pt;padding-right:2.4pt;">47.3</td><td class="ltx_td" id="S6.T4.2.5.5.17" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" id="S6.T4.2.5.5.18" style="padding-left:2.4pt;padding-right:2.4pt;">45.5</td><td class="ltx_td ltx_align_center" id="S6.T4.2.5.5.19" style="padding-left:2.4pt;padding-right:2.4pt;">50.7</td><td class="ltx_td ltx_align_center" id="S6.T4.2.5.5.20" style="padding-left:2.4pt;padding-right:2.4pt;">42.2</td><td class="ltx_td" id="S6.T4.2.5.5.21" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" id="S6.T4.2.5.5.22" style="padding-left:2.4pt;padding-right:2.4pt;">60.6</td><td class="ltx_td ltx_align_center" id="S6.T4.2.5.5.23" style="padding-left:2.4pt;padding-right:2.4pt;">61.7</td><td class="ltx_td" id="S6.T4.2.5.5.24" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center" id="S6.T4.2.5.5.25" style="padding-left:2.4pt;padding-right:2.4pt;">-</td></tr><tr class="ltx_tr" id="S6.T4.2.6.6"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T4.2.6.6.1" style="padding-left:2.4pt;padding-right:2.4pt;"><em class="ltx_emph ltx_font_italic" id="S6.T4.2.6.6.1.1" style="font-size:90%;">Florence-2-B</em></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T4.2.6.6.2" style="padding-left:2.4pt;padding-right:2.4pt;">0.23B</th><td class="ltx_td ltx_border_t" id="S6.T4.2.6.6.3" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.6.6.4" style="padding-left:2.4pt;padding-right:2.4pt;">133.0</td><td class="ltx_td ltx_border_t" id="S6.T4.2.6.6.5" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.6.6.6" style="padding-left:2.4pt;padding-right:2.4pt;">118.7</td><td class="ltx_td ltx_border_t" id="S6.T4.2.6.6.7" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.6.6.8" style="padding-left:2.4pt;padding-right:2.4pt;">70.1</td><td class="ltx_td ltx_border_t" id="S6.T4.2.6.6.9" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.6.6.10" style="padding-left:2.4pt;padding-right:2.4pt;">34.7</td><td class="ltx_td ltx_border_t" id="S6.T4.2.6.6.11" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.6.6.12" style="padding-left:2.4pt;padding-right:2.4pt;">83.6</td><td class="ltx_td ltx_border_t" id="S6.T4.2.6.6.13" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.6.6.14" style="padding-left:2.4pt;padding-right:2.4pt;">53.9</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.6.6.15" style="padding-left:2.4pt;padding-right:2.4pt;">58.4</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.6.6.16" style="padding-left:2.4pt;padding-right:2.4pt;">49.7</td><td class="ltx_td ltx_border_t" id="S6.T4.2.6.6.17" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.6.6.18" style="padding-left:2.4pt;padding-right:2.4pt;">51.5</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.6.6.19" style="padding-left:2.4pt;padding-right:2.4pt;">56.4</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.6.6.20" style="padding-left:2.4pt;padding-right:2.4pt;">47.9</td><td class="ltx_td ltx_border_t" id="S6.T4.2.6.6.21" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.6.6.22" style="padding-left:2.4pt;padding-right:2.4pt;">66.3</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.6.6.23" style="padding-left:2.4pt;padding-right:2.4pt;">65.1</td><td class="ltx_td ltx_border_t" id="S6.T4.2.6.6.24" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.6.6.25" style="padding-left:2.4pt;padding-right:2.4pt;">34.6</td></tr><tr class="ltx_tr" id="S6.T4.2.7.7"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T4.2.7.7.1" style="padding-left:2.4pt;padding-right:2.4pt;"><em class="ltx_emph ltx_font_italic" id="S6.T4.2.7.7.1.1" style="font-size:90%;">Florence-2-L</em></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T4.2.7.7.2" style="padding-left:2.4pt;padding-right:2.4pt;">0.77B</th><td class="ltx_td ltx_border_bb" id="S6.T4.2.7.7.3" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.7.7.4" style="padding-left:2.4pt;padding-right:2.4pt;">135.6</td><td class="ltx_td ltx_border_bb" id="S6.T4.2.7.7.5" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.7.7.6" style="padding-left:2.4pt;padding-right:2.4pt;">120.8</td><td class="ltx_td ltx_border_bb" id="S6.T4.2.7.7.7" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.7.7.8" style="padding-left:2.4pt;padding-right:2.4pt;">72.8</td><td class="ltx_td ltx_border_bb" id="S6.T4.2.7.7.9" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.7.7.10" style="padding-left:2.4pt;padding-right:2.4pt;">37.5</td><td class="ltx_td ltx_border_bb" id="S6.T4.2.7.7.11" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.7.7.12" style="padding-left:2.4pt;padding-right:2.4pt;">84.4</td><td class="ltx_td ltx_border_bb" id="S6.T4.2.7.7.13" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.7.7.14" style="padding-left:2.4pt;padding-right:2.4pt;">56.3</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.7.7.15" style="padding-left:2.4pt;padding-right:2.4pt;">61.6</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.7.7.16" style="padding-left:2.4pt;padding-right:2.4pt;">51.4</td><td class="ltx_td ltx_border_bb" id="S6.T4.2.7.7.17" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.7.7.18" style="padding-left:2.4pt;padding-right:2.4pt;">53.6</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.7.7.19" style="padding-left:2.4pt;padding-right:2.4pt;">57.9</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.7.7.20" style="padding-left:2.4pt;padding-right:2.4pt;">49.9</td><td class="ltx_td ltx_border_bb" id="S6.T4.2.7.7.21" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.7.7.22" style="padding-left:2.4pt;padding-right:2.4pt;">68.0</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.7.7.23" style="padding-left:2.4pt;padding-right:2.4pt;">67.0</td><td class="ltx_td ltx_border_bb" id="S6.T4.2.7.7.24" style="padding-left:2.4pt;padding-right:2.4pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.7.7.25" style="padding-left:2.4pt;padding-right:2.4pt;">35.8</td></tr></tbody></table>

| ã€è¡¨æ ‡é¢˜ã€‘åŸæ–‡ | ã€è¡¨æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Table 4:  Zero-shot performance of generalist vision foundation models. | âœ… Table 4:  Zero-shot é€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚ |
| âœ… The models do not see the training data of the evaluation tasks during training. | âœ… æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´çœ‹ä¸åˆ°è¯„ä¼°ä»»åŠ¡çš„è®­ç»ƒæ•°æ®ã€‚ |
| âœ… Florence-2 models are pre-trained on FLD-5B dataset. | âœ… Florence-2 æ¨¡å‹åœ¨ FLD-5B æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚ |
| âœ… Karpathy test split is used for COCO caption evaluation. | âœ… Karpathy æµ‹è¯•åˆ†å‰²ç”¨äº COCO å­—å¹•è¯„ä¼°ã€‚ |

| ã€ç¬¬6.2èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬6.2èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We present a powerful vision foundation model that does not require task-specific supervised annotations for fine-tuning. | âœ… æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¼ºå¤§çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸éœ€è¦é’ˆå¯¹ç‰¹å®šâ€‹â€‹ä»»åŠ¡çš„ç›‘ç£æ³¨é‡Šè¿›è¡Œå¾®è°ƒã€‚ |
| âœ… The zero-shot performance of our model is shown in TableÂ 4. | âœ… æˆ‘ä»¬çš„æ¨¡å‹çš„zero-shotæ€§èƒ½æ˜¾ç¤ºåœ¨TableÂ 4ä¸­ã€‚ |
| âœ… For image-level tasks, Florence-2-L achieves a 135.6 CIDEr score on the COCO caption benchmark ( **Microsoft coco: Common objects in context.** ) , utilizing less than 1% of the parameters compared to the 80B Flamingo ( **Flamingo: a visual language model for few-shot learning.** ) model (which has an 84.3 CIDEr score). | âœ… å¯¹äºå›¾åƒçº§ä»»åŠ¡ï¼ŒFlorence-2-L åœ¨ COCO æ ‡é¢˜åŸºå‡† ( **Microsoft coco: Common objects in context.** ) ä¸Šè·å¾—äº† 135.6 CIDEr åˆ†æ•°ï¼Œä¸ 80B Flamingo ( **Flamingo: a visual language model for few-shot learning.** ) æ¨¡å‹ï¼ˆå…¶å…·æœ‰ 84.3 CIDEr åˆ†æ•°ï¼‰ç›¸æ¯”ï¼Œä½¿ç”¨çš„å‚æ•°ä¸åˆ° 1ï¼…ã€‚ |
| âœ… For region-level grounding and referring expression comprehension tasks, Florence-2-L establishes a new record in zero-shot performance achieving a 5.7 improvement in Flickr30k ( **Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.** ) Recall@1, and approximately 4%, 8%, and 8% absolute improvements on Refcoco, Refcoco+, and Refcocog ( **Modeling context in referring expressions.** ) , respectively, compared to the Kosmos-2 ( **Kosmos-2: Grounding multimodal large language models to the world.** ) model, which has 1.6B parameters. | âœ… å¯¹äºåŒºåŸŸçº§åŸºç¡€å®šä½å’ŒæŒ‡ç§°è¡¨è¾¾ç†è§£ä»»åŠ¡ï¼Œä¸æ‹¥æœ‰ 16 äº¿å‚æ•°çš„ Kosmos-2 ( **Kosmos-2: Grounding multimodal large language models to the world.** ) æ¨¡å‹ç›¸æ¯”ï¼ŒFlorence-2-L åœ¨é›¶æ ·æœ¬æ€§èƒ½æ–¹é¢åˆ›ä¸‹äº†æ–°çºªå½•ï¼Œåœ¨ Flickr30k ( **Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.** ) Recall@1 ä¸Šå®ç°äº† 5.7 çš„æå‡ï¼Œåœ¨ Refcocoã€Refcoco+ å’Œ Refcocog ( **Modeling context in referring expressions.** ) ä¸Šåˆ†åˆ«å®ç°äº†çº¦ 4%ã€8% å’Œ 8% çš„ç»å¯¹æå‡ã€‚ |
| âœ… Additionally, our pre-trained model attains a 35.8% mIOU in the Refcoco referring expression segmentation (RES) ( **Modeling context in referring expressions.** ) task, a capability not supported by prior foundation models. | âœ… æ­¤å¤–ï¼Œæˆ‘ä»¬é¢„è®­ç»ƒçš„æ¨¡å‹åœ¨ Refcoco æŒ‡ç§°è¡¨è¾¾åˆ†å‰² (RES) ( **Modeling context in referring expressions.** ) ä»»åŠ¡ä¸­è¾¾åˆ°äº† 35.8% mIOUï¼Œè¿™æ˜¯ä¹‹å‰çš„åŸºç¡€æ¨¡å‹æ‰€ä¸æ”¯æŒçš„åŠŸèƒ½ã€‚ |

### 6.3 Generalist Model with Public Supervised Data

<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T5.6"><tbody class="ltx_tbody"><tr class="ltx_tr" id="S6.T5.6.7.1"><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T5.6.7.1.1" rowspan="3" style="padding-left:9.4pt;padding-right:9.4pt;">Method</th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T5.6.7.1.2" rowspan="3" style="padding-left:9.4pt;padding-right:9.4pt;">#params</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.6.7.1.3" style="padding-left:9.4pt;padding-right:9.4pt;">COCO Caption</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.6.7.1.4" style="padding-left:9.4pt;padding-right:9.4pt;">NoCaps</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.6.7.1.5" style="padding-left:9.4pt;padding-right:9.4pt;">TextCaps</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.6.7.1.6" style="padding-left:9.4pt;padding-right:9.4pt;">VQAv2</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.6.7.1.7" style="padding-left:9.4pt;padding-right:9.4pt;">TextVQA</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.6.7.1.8" style="padding-left:9.4pt;padding-right:9.4pt;">VizWiz VQA</th></tr><tr class="ltx_tr" id="S6.T5.6.8.2"><td class="ltx_td ltx_align_center" id="S6.T5.6.8.2.1" style="padding-left:9.4pt;padding-right:9.4pt;">Karpathy test</td><td class="ltx_td ltx_align_center" id="S6.T5.6.8.2.2" style="padding-left:9.4pt;padding-right:9.4pt;">val</td><td class="ltx_td ltx_align_center" id="S6.T5.6.8.2.3" style="padding-left:9.4pt;padding-right:9.4pt;">val</td><td class="ltx_td ltx_align_center" id="S6.T5.6.8.2.4" style="padding-left:9.4pt;padding-right:9.4pt;">test-dev</td><td class="ltx_td ltx_align_center" id="S6.T5.6.8.2.5" style="padding-left:9.4pt;padding-right:9.4pt;">test-dev</td><td class="ltx_td ltx_align_center" id="S6.T5.6.8.2.6" style="padding-left:9.4pt;padding-right:9.4pt;">test-dev</td></tr><tr class="ltx_tr" id="S6.T5.6.9.3"><td class="ltx_td ltx_align_center" id="S6.T5.6.9.3.1" style="padding-left:9.4pt;padding-right:9.4pt;">CIDEr</td><td class="ltx_td ltx_align_center" id="S6.T5.6.9.3.2" style="padding-left:9.4pt;padding-right:9.4pt;">CIDEr</td><td class="ltx_td ltx_align_center" id="S6.T5.6.9.3.3" style="padding-left:9.4pt;padding-right:9.4pt;">CIDEr</td><td class="ltx_td ltx_align_center" id="S6.T5.6.9.3.4" style="padding-left:9.4pt;padding-right:9.4pt;">Acc</td><td class="ltx_td ltx_align_center" id="S6.T5.6.9.3.5" style="padding-left:9.4pt;padding-right:9.4pt;">Acc</td><td class="ltx_td ltx_align_center" id="S6.T5.6.9.3.6" style="padding-left:9.4pt;padding-right:9.4pt;">Acc</td></tr><tr class="ltx_tr" id="S6.T5.6.10.4"><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="8" id="S6.T5.6.10.4.1" style="padding-left:9.4pt;padding-right:9.4pt;">Specialist Models</th></tr><tr class="ltx_tr" id="S6.T5.6.11.5"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T5.6.11.5.1" style="padding-left:9.4pt;padding-right:9.4pt;">CoCaÂ <html><body><p>( <strong>Coca: Contrastive captioners are image-text foundation models, 2022.</strong> )</p></body></html></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T5.6.11.5.2" style="padding-left:9.4pt;padding-right:9.4pt;">2.1B</th><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.6.11.5.3" style="padding-left:9.4pt;padding-right:9.4pt;">143.6</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.6.11.5.4" style="padding-left:9.4pt;padding-right:9.4pt;">122.4</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.6.11.5.5" style="padding-left:9.4pt;padding-right:9.4pt;">-</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.6.11.5.6" style="padding-left:9.4pt;padding-right:9.4pt;">82.3</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.6.11.5.7" style="padding-left:9.4pt;padding-right:9.4pt;">-</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.6.11.5.8" style="padding-left:9.4pt;padding-right:9.4pt;">-</td></tr><tr class="ltx_tr" id="S6.T5.6.12.6"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T5.6.12.6.1" style="padding-left:9.4pt;padding-right:9.4pt;">BLIP-2Â <html><body><p>( <strong>Blip-2: Bootstrapping language-image pre-training with frozen imageencoders and large language models.</strong> )</p></body></html></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T5.6.12.6.2" style="padding-left:9.4pt;padding-right:9.4pt;">7.8B</th><td class="ltx_td ltx_align_center" id="S6.T5.6.12.6.3" style="padding-left:9.4pt;padding-right:9.4pt;">144.5</td><td class="ltx_td ltx_align_center" id="S6.T5.6.12.6.4" style="padding-left:9.4pt;padding-right:9.4pt;">121.6</td><td class="ltx_td ltx_align_center" id="S6.T5.6.12.6.5" style="padding-left:9.4pt;padding-right:9.4pt;">-</td><td class="ltx_td ltx_align_center" id="S6.T5.6.12.6.6" style="padding-left:9.4pt;padding-right:9.4pt;">82.2</td><td class="ltx_td ltx_align_center" id="S6.T5.6.12.6.7" style="padding-left:9.4pt;padding-right:9.4pt;">-</td><td class="ltx_td ltx_align_center" id="S6.T5.6.12.6.8" style="padding-left:9.4pt;padding-right:9.4pt;">-</td></tr><tr class="ltx_tr" id="S6.T5.6.13.7"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T5.6.13.7.1" style="padding-left:9.4pt;padding-right:9.4pt;">GIT2Â <html><body><p>( <strong>Git: A generative image-to-text transformer for vision and language,2022.</strong> )</p></body></html></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T5.6.13.7.2" style="padding-left:9.4pt;padding-right:9.4pt;">5.1B</th><td class="ltx_td ltx_align_center" id="S6.T5.6.13.7.3" style="padding-left:9.4pt;padding-right:9.4pt;">145</td><td class="ltx_td ltx_align_center" id="S6.T5.6.13.7.4" style="padding-left:9.4pt;padding-right:9.4pt;">126.9</td><td class="ltx_td ltx_align_center" id="S6.T5.6.13.7.5" style="padding-left:9.4pt;padding-right:9.4pt;">148.6</td><td class="ltx_td ltx_align_center" id="S6.T5.6.13.7.6" style="padding-left:9.4pt;padding-right:9.4pt;">81.7</td><td class="ltx_td ltx_align_center" id="S6.T5.6.13.7.7" style="padding-left:9.4pt;padding-right:9.4pt;">67.3</td><td class="ltx_td ltx_align_center" id="S6.T5.6.13.7.8" style="padding-left:9.4pt;padding-right:9.4pt;">71.0</td></tr><tr class="ltx_tr" id="S6.T5.6.14.8"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T5.6.14.8.1" style="padding-left:9.4pt;padding-right:9.4pt;">FlamingoÂ <html><body><p>( <strong>Flamingo: a visual language model for few-shot learning.</strong> )</p></body></html></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T5.6.14.8.2" style="padding-left:9.4pt;padding-right:9.4pt;">80B</th><td class="ltx_td ltx_align_center" id="S6.T5.6.14.8.3" style="padding-left:9.4pt;padding-right:9.4pt;">138.1</td><td class="ltx_td ltx_align_center" id="S6.T5.6.14.8.4" style="padding-left:9.4pt;padding-right:9.4pt;">-</td><td class="ltx_td ltx_align_center" id="S6.T5.6.14.8.5" style="padding-left:9.4pt;padding-right:9.4pt;">-</td><td class="ltx_td ltx_align_center" id="S6.T5.6.14.8.6" style="padding-left:9.4pt;padding-right:9.4pt;">82.0</td><td class="ltx_td ltx_align_center" id="S6.T5.6.14.8.7" style="padding-left:9.4pt;padding-right:9.4pt;">54.1</td><td class="ltx_td ltx_align_center" id="S6.T5.6.14.8.8" style="padding-left:9.4pt;padding-right:9.4pt;">65.7</td></tr><tr class="ltx_tr" id="S6.T5.3.3"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T5.3.3.4" style="padding-left:9.4pt;padding-right:9.4pt;">PaLIÂ <html><body><p>( <strong>Pali: A jointly-scaled multilingual language-image model, 2022.</strong> )</p></body></html></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T5.3.3.5" style="padding-left:9.4pt;padding-right:9.4pt;">17B</th><td class="ltx_td ltx_align_center" id="S6.T5.3.3.6" style="padding-left:9.4pt;padding-right:9.4pt;">149.1</td><td class="ltx_td ltx_align_center" id="S6.T5.3.3.7" style="padding-left:9.4pt;padding-right:9.4pt;">127.0</td><td class="ltx_td ltx_align_center" id="S6.T5.1.1.1" style="padding-left:9.4pt;padding-right:9.4pt;">160.0<sup class="ltx_sup" id="S6.T5.1.1.1.2">â–³</sup></td><td class="ltx_td ltx_align_center" id="S6.T5.3.3.8" style="padding-left:9.4pt;padding-right:9.4pt;">84.3</td><td class="ltx_td ltx_align_center" id="S6.T5.2.2.2" style="padding-left:9.4pt;padding-right:9.4pt;">58.8 / 73.1<sup class="ltx_sup" id="S6.T5.2.2.2.2">â–³</sup></td><td class="ltx_td ltx_align_center" id="S6.T5.3.3.3" style="padding-left:9.4pt;padding-right:9.4pt;">71.6 / 74.4<sup class="ltx_sup" id="S6.T5.3.3.3.2">â–³</sup></td></tr><tr class="ltx_tr" id="S6.T5.6.6"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T5.6.6.4" style="padding-left:9.4pt;padding-right:9.4pt;">PaLI-XÂ <html><body><p>( <strong>Pali-x: On scaling up a multilingual vision and language model.</strong> )</p></body></html></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T5.6.6.5" style="padding-left:9.4pt;padding-right:9.4pt;">55B</th><td class="ltx_td ltx_align_center" id="S6.T5.6.6.6" style="padding-left:9.4pt;padding-right:9.4pt;">149.2</td><td class="ltx_td ltx_align_center" id="S6.T5.6.6.7" style="padding-left:9.4pt;padding-right:9.4pt;">126.3</td><td class="ltx_td ltx_align_center" id="S6.T5.4.4.1" style="padding-left:9.4pt;padding-right:9.4pt;">147 / 163.7<sup class="ltx_sup" id="S6.T5.4.4.1.2">â–³</sup></td><td class="ltx_td ltx_align_center" id="S6.T5.6.6.8" style="padding-left:9.4pt;padding-right:9.4pt;">86.0</td><td class="ltx_td ltx_align_center" id="S6.T5.5.5.2" style="padding-left:9.4pt;padding-right:9.4pt;">71.4 / 80.8<sup class="ltx_sup" id="S6.T5.5.5.2.2">â–³</sup></td><td class="ltx_td ltx_align_center" id="S6.T5.6.6.3" style="padding-left:9.4pt;padding-right:9.4pt;">70.9 / 74.6<sup class="ltx_sup" id="S6.T5.6.6.3.2">â–³</sup></td></tr><tr class="ltx_tr" id="S6.T5.6.15.9"><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="8" id="S6.T5.6.15.9.1" style="padding-left:9.4pt;padding-right:9.4pt;">Generalist Models</th></tr><tr class="ltx_tr" id="S6.T5.6.16.10"><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S6.T5.6.16.10.1" style="padding-left:9.4pt;padding-right:9.4pt;">Unified-IOÂ <html><body><p>( <strong>Unified-io: A unified model for vision, language, and multi-modaltasks, 2022.</strong> )</p></body></html></th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S6.T5.6.16.10.2" style="padding-left:9.4pt;padding-right:9.4pt;">2.9B</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.6.16.10.3" style="padding-left:9.4pt;padding-right:9.4pt;">-</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.6.16.10.4" style="padding-left:9.4pt;padding-right:9.4pt;">100</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.6.16.10.5" style="padding-left:9.4pt;padding-right:9.4pt;">-</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.6.16.10.6" style="padding-left:9.4pt;padding-right:9.4pt;">77.9</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.6.16.10.7" style="padding-left:9.4pt;padding-right:9.4pt;">-</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.6.16.10.8" style="padding-left:9.4pt;padding-right:9.4pt;">57.4</th></tr><tr class="ltx_tr" id="S6.T5.6.17.11"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T5.6.17.11.1" style="padding-left:9.4pt;padding-right:9.4pt;"><em class="ltx_emph ltx_font_italic" id="S6.T5.6.17.11.1.1" style="font-size:90%;">Florence-2-B</em></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T5.6.17.11.2" style="padding-left:9.4pt;padding-right:9.4pt;">0.23B</th><td class="ltx_td ltx_align_center" id="S6.T5.6.17.11.3" style="padding-left:9.4pt;padding-right:9.4pt;">140.0</td><td class="ltx_td ltx_align_center" id="S6.T5.6.17.11.4" style="padding-left:9.4pt;padding-right:9.4pt;">116.7</td><td class="ltx_td ltx_align_center" id="S6.T5.6.17.11.5" style="padding-left:9.4pt;padding-right:9.4pt;">143.9</td><td class="ltx_td ltx_align_center" id="S6.T5.6.17.11.6" style="padding-left:9.4pt;padding-right:9.4pt;">79.7</td><td class="ltx_td ltx_align_center" id="S6.T5.6.17.11.7" style="padding-left:9.4pt;padding-right:9.4pt;">63.6</td><td class="ltx_td ltx_align_center" id="S6.T5.6.17.11.8" style="padding-left:9.4pt;padding-right:9.4pt;">63.6</td></tr><tr class="ltx_tr" id="S6.T5.6.18.12"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T5.6.18.12.1" style="padding-left:9.4pt;padding-right:9.4pt;"><em class="ltx_emph ltx_font_italic" id="S6.T5.6.18.12.1.1" style="font-size:90%;">Florence-2-L</em></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T5.6.18.12.2" style="padding-left:9.4pt;padding-right:9.4pt;">0.77B</th><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.6.18.12.3" style="padding-left:9.4pt;padding-right:9.4pt;">143.3</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.6.18.12.4" style="padding-left:9.4pt;padding-right:9.4pt;">124.9</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.6.18.12.5" style="padding-left:9.4pt;padding-right:9.4pt;">151.1</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.6.18.12.6" style="padding-left:9.4pt;padding-right:9.4pt;">81.7</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.6.18.12.7" style="padding-left:9.4pt;padding-right:9.4pt;">73.5</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.6.18.12.8" style="padding-left:9.4pt;padding-right:9.4pt;">72.6</td></tr></tbody></table>

| ã€è¡¨æ ‡é¢˜ã€‘åŸæ–‡ | ã€è¡¨æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Table 5:  Performance of specialist and generalist models on captioning and VQA tasks. | âœ… Table 5:  ä¸“å®¶æ¨¡å‹å’Œé€šæ‰æ¨¡å‹åœ¨å­—å¹•å’Œ VQA ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ |
| âœ… Specialist Models refer to those that are fine-tuned specifically for each task, while Generalist Models denote a single model fine-tuned in a task-agnostic manner, applicable across all tasks. | âœ… Specialist Models æŒ‡é’ˆå¯¹æ¯ä¸ªä»»åŠ¡ä¸“é—¨è¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ï¼Œè€Œ Generalist Models è¡¨ç¤ºä»¥ä¸ä»»åŠ¡æ— å…³çš„æ–¹å¼è¿›è¡Œå¾®è°ƒçš„å•ä¸€æ¨¡å‹ï¼Œé€‚ç”¨äºæ‰€æœ‰ä»»åŠ¡ã€‚ |
| âœ… â–³ indicates usage of external OCR as input. | âœ… â–³ è¡¨ç¤ºä½¿ç”¨å¤–éƒ¨ OCR ä½œä¸ºè¾“å…¥ã€‚ |

<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T6.2"><tbody class="ltx_tbody"><tr class="ltx_tr" id="S6.T6.2.1.1"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S6.T6.2.1.1.1" rowspan="3" style="padding-left:3.3pt;padding-right:3.3pt;">Method</th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S6.T6.2.1.1.2" rowspan="3" style="padding-left:3.3pt;padding-right:3.3pt;">#params</th><td class="ltx_td ltx_border_tt" id="S6.T6.2.1.1.3" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T6.2.1.1.4" style="padding-left:3.3pt;padding-right:3.3pt;">COCO Det.</td><td class="ltx_td ltx_border_tt" id="S6.T6.2.1.1.5" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T6.2.1.1.6" style="padding-left:3.3pt;padding-right:3.3pt;">Flickr30k</td><td class="ltx_td ltx_border_tt" id="S6.T6.2.1.1.7" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S6.T6.2.1.1.8" style="padding-left:3.3pt;padding-right:3.3pt;">Refcoco</td><td class="ltx_td ltx_border_tt" id="S6.T6.2.1.1.9" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S6.T6.2.1.1.10" style="padding-left:3.3pt;padding-right:3.3pt;">Refcoco+</td><td class="ltx_td ltx_border_tt" id="S6.T6.2.1.1.11" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S6.T6.2.1.1.12" style="padding-left:3.3pt;padding-right:3.3pt;">Refcocog</td><td class="ltx_td ltx_border_tt" id="S6.T6.2.1.1.13" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T6.2.1.1.14" style="padding-left:3.3pt;padding-right:3.3pt;">Refcoco RES</td></tr><tr class="ltx_tr" id="S6.T6.2.2.2"><td class="ltx_td" id="S6.T6.2.2.2.1" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.2.2.2" style="padding-left:3.3pt;padding-right:3.3pt;">val2017</td><td class="ltx_td" id="S6.T6.2.2.2.3" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.2.2.4" style="padding-left:3.3pt;padding-right:3.3pt;">test</td><td class="ltx_td" id="S6.T6.2.2.2.5" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.2.2.6" style="padding-left:3.3pt;padding-right:3.3pt;">val</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.2.2.7" style="padding-left:3.3pt;padding-right:3.3pt;">test-A</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.2.2.8" style="padding-left:3.3pt;padding-right:3.3pt;">test-B</td><td class="ltx_td" id="S6.T6.2.2.2.9" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.2.2.10" style="padding-left:3.3pt;padding-right:3.3pt;">val</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.2.2.11" style="padding-left:3.3pt;padding-right:3.3pt;">test-A</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.2.2.12" style="padding-left:3.3pt;padding-right:3.3pt;">test-B</td><td class="ltx_td" id="S6.T6.2.2.2.13" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.2.2.14" style="padding-left:3.3pt;padding-right:3.3pt;">val</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.2.2.15" style="padding-left:3.3pt;padding-right:3.3pt;">test</td><td class="ltx_td" id="S6.T6.2.2.2.16" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.2.2.17" style="padding-left:3.3pt;padding-right:3.3pt;">val</td></tr><tr class="ltx_tr" id="S6.T6.2.3.3"><td class="ltx_td" id="S6.T6.2.3.3.1" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.3.3.2" style="padding-left:3.3pt;padding-right:3.3pt;">mAP</td><td class="ltx_td" id="S6.T6.2.3.3.3" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.3.3.4" style="padding-left:3.3pt;padding-right:3.3pt;">R@1</td><td class="ltx_td" id="S6.T6.2.3.3.5" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" colspan="3" id="S6.T6.2.3.3.6" style="padding-left:3.3pt;padding-right:3.3pt;">Accuracy</td><td class="ltx_td" id="S6.T6.2.3.3.7" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" colspan="3" id="S6.T6.2.3.3.8" style="padding-left:3.3pt;padding-right:3.3pt;">Accuracy</td><td class="ltx_td" id="S6.T6.2.3.3.9" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" colspan="2" id="S6.T6.2.3.3.10" style="padding-left:3.3pt;padding-right:3.3pt;">Accuracy</td><td class="ltx_td" id="S6.T6.2.3.3.11" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.3.3.12" style="padding-left:3.3pt;padding-right:3.3pt;">mIoU</td></tr><tr class="ltx_tr" id="S6.T6.2.4.4"><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="19" id="S6.T6.2.4.4.1" style="padding-left:3.3pt;padding-right:3.3pt;">Specialist Models</th></tr><tr class="ltx_tr" id="S6.T6.2.5.5"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T6.2.5.5.1" style="padding-left:3.3pt;padding-right:3.3pt;">SeqTRÂ <html><body><p>( <strong>Seqtr: A simple yet universal network for visual grounding.</strong> )</p></body></html></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T6.2.5.5.2" style="padding-left:3.3pt;padding-right:3.3pt;">-</th><td class="ltx_td ltx_border_t" id="S6.T6.2.5.5.3" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.5.5.4" style="padding-left:3.3pt;padding-right:3.3pt;">-</td><td class="ltx_td ltx_border_t" id="S6.T6.2.5.5.5" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.5.5.6" style="padding-left:3.3pt;padding-right:3.3pt;">-</td><td class="ltx_td ltx_border_t" id="S6.T6.2.5.5.7" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.5.5.8" style="padding-left:3.3pt;padding-right:3.3pt;">83.7</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.5.5.9" style="padding-left:3.3pt;padding-right:3.3pt;">86.5</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.5.5.10" style="padding-left:3.3pt;padding-right:3.3pt;">81.2</td><td class="ltx_td ltx_border_t" id="S6.T6.2.5.5.11" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.5.5.12" style="padding-left:3.3pt;padding-right:3.3pt;">71.5</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.5.5.13" style="padding-left:3.3pt;padding-right:3.3pt;">76.3</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.5.5.14" style="padding-left:3.3pt;padding-right:3.3pt;">64.9</td><td class="ltx_td ltx_border_t" id="S6.T6.2.5.5.15" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.5.5.16" style="padding-left:3.3pt;padding-right:3.3pt;">74.9</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.5.5.17" style="padding-left:3.3pt;padding-right:3.3pt;">74.2</td><td class="ltx_td ltx_border_t" id="S6.T6.2.5.5.18" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.5.5.19" style="padding-left:3.3pt;padding-right:3.3pt;">-</td></tr><tr class="ltx_tr" id="S6.T6.2.6.6"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T6.2.6.6.1" style="padding-left:3.3pt;padding-right:3.3pt;">PolyFormerÂ <html><body><p>( <strong>Polyformer: Referring image segmentation as sequential polygongeneration.</strong> )</p></body></html></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T6.2.6.6.2" style="padding-left:3.3pt;padding-right:3.3pt;">-</th><td class="ltx_td" id="S6.T6.2.6.6.3" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.6.6.4" style="padding-left:3.3pt;padding-right:3.3pt;">-</td><td class="ltx_td" id="S6.T6.2.6.6.5" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.6.6.6" style="padding-left:3.3pt;padding-right:3.3pt;">-</td><td class="ltx_td" id="S6.T6.2.6.6.7" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.6.6.8" style="padding-left:3.3pt;padding-right:3.3pt;">90.4</td><td class="ltx_td ltx_align_center" id="S6.T6.2.6.6.9" style="padding-left:3.3pt;padding-right:3.3pt;">92.9</td><td class="ltx_td ltx_align_center" id="S6.T6.2.6.6.10" style="padding-left:3.3pt;padding-right:3.3pt;">87.2</td><td class="ltx_td" id="S6.T6.2.6.6.11" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.6.6.12" style="padding-left:3.3pt;padding-right:3.3pt;">85.0</td><td class="ltx_td ltx_align_center" id="S6.T6.2.6.6.13" style="padding-left:3.3pt;padding-right:3.3pt;">89.8</td><td class="ltx_td ltx_align_center" id="S6.T6.2.6.6.14" style="padding-left:3.3pt;padding-right:3.3pt;">78.0</td><td class="ltx_td" id="S6.T6.2.6.6.15" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.6.6.16" style="padding-left:3.3pt;padding-right:3.3pt;">85.8</td><td class="ltx_td ltx_align_center" id="S6.T6.2.6.6.17" style="padding-left:3.3pt;padding-right:3.3pt;">85.9</td><td class="ltx_td" id="S6.T6.2.6.6.18" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.6.6.19" style="padding-left:3.3pt;padding-right:3.3pt;">76.9</td></tr><tr class="ltx_tr" id="S6.T6.2.7.7"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T6.2.7.7.1" style="padding-left:3.3pt;padding-right:3.3pt;">UNINEXTÂ <html><body><p>( <strong>Universal instance perception as object discovery and retrieval.</strong> )</p></body></html></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T6.2.7.7.2" style="padding-left:3.3pt;padding-right:3.3pt;">0.74B</th><td class="ltx_td" id="S6.T6.2.7.7.3" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.7.7.4" style="padding-left:3.3pt;padding-right:3.3pt;">60.6</td><td class="ltx_td" id="S6.T6.2.7.7.5" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.7.7.6" style="padding-left:3.3pt;padding-right:3.3pt;">-</td><td class="ltx_td" id="S6.T6.2.7.7.7" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.7.7.8" style="padding-left:3.3pt;padding-right:3.3pt;">92.6</td><td class="ltx_td ltx_align_center" id="S6.T6.2.7.7.9" style="padding-left:3.3pt;padding-right:3.3pt;">94.3</td><td class="ltx_td ltx_align_center" id="S6.T6.2.7.7.10" style="padding-left:3.3pt;padding-right:3.3pt;">91.5</td><td class="ltx_td" id="S6.T6.2.7.7.11" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.7.7.12" style="padding-left:3.3pt;padding-right:3.3pt;">85.2</td><td class="ltx_td ltx_align_center" id="S6.T6.2.7.7.13" style="padding-left:3.3pt;padding-right:3.3pt;">89.6</td><td class="ltx_td ltx_align_center" id="S6.T6.2.7.7.14" style="padding-left:3.3pt;padding-right:3.3pt;">79.8</td><td class="ltx_td" id="S6.T6.2.7.7.15" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.7.7.16" style="padding-left:3.3pt;padding-right:3.3pt;">88.7</td><td class="ltx_td ltx_align_center" id="S6.T6.2.7.7.17" style="padding-left:3.3pt;padding-right:3.3pt;">89.4</td><td class="ltx_td" id="S6.T6.2.7.7.18" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.7.7.19" style="padding-left:3.3pt;padding-right:3.3pt;">-</td></tr><tr class="ltx_tr" id="S6.T6.2.8.8"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T6.2.8.8.1" style="padding-left:3.3pt;padding-right:3.3pt;">FerretÂ <html><body><p>( <strong>Ferret: Refer and ground anything anywhere at any granularity, 2023.</strong> )</p></body></html></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T6.2.8.8.2" style="padding-left:3.3pt;padding-right:3.3pt;">13B</th><td class="ltx_td" id="S6.T6.2.8.8.3" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.8.8.4" style="padding-left:3.3pt;padding-right:3.3pt;">-</td><td class="ltx_td" id="S6.T6.2.8.8.5" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.8.8.6" style="padding-left:3.3pt;padding-right:3.3pt;">-</td><td class="ltx_td" id="S6.T6.2.8.8.7" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.8.8.8" style="padding-left:3.3pt;padding-right:3.3pt;">89.5</td><td class="ltx_td ltx_align_center" id="S6.T6.2.8.8.9" style="padding-left:3.3pt;padding-right:3.3pt;">92.4</td><td class="ltx_td ltx_align_center" id="S6.T6.2.8.8.10" style="padding-left:3.3pt;padding-right:3.3pt;">84.4</td><td class="ltx_td" id="S6.T6.2.8.8.11" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.8.8.12" style="padding-left:3.3pt;padding-right:3.3pt;">82.8</td><td class="ltx_td ltx_align_center" id="S6.T6.2.8.8.13" style="padding-left:3.3pt;padding-right:3.3pt;">88.1</td><td class="ltx_td ltx_align_center" id="S6.T6.2.8.8.14" style="padding-left:3.3pt;padding-right:3.3pt;">75.2</td><td class="ltx_td" id="S6.T6.2.8.8.15" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.8.8.16" style="padding-left:3.3pt;padding-right:3.3pt;">85.8</td><td class="ltx_td ltx_align_center" id="S6.T6.2.8.8.17" style="padding-left:3.3pt;padding-right:3.3pt;">86.3</td><td class="ltx_td" id="S6.T6.2.8.8.18" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.8.8.19" style="padding-left:3.3pt;padding-right:3.3pt;">-</td></tr><tr class="ltx_tr" id="S6.T6.2.9.9"><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="19" id="S6.T6.2.9.9.1" style="padding-left:3.3pt;padding-right:3.3pt;">Generalist Models</th></tr><tr class="ltx_tr" id="S6.T6.2.10.10"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T6.2.10.10.1" style="padding-left:3.3pt;padding-right:3.3pt;">UniTABÂ <html><body><p>( <strong>Unitab: Unifying text and box outputs for grounded vision-languagemodeling.</strong> )</p></body></html></th><th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T6.2.10.10.2" style="padding-left:3.3pt;padding-right:3.3pt;"></th><td class="ltx_td ltx_border_t" id="S6.T6.2.10.10.3" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.10.10.4" style="padding-left:3.3pt;padding-right:3.3pt;">-</td><td class="ltx_td ltx_border_t" id="S6.T6.2.10.10.5" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.10.10.6" style="padding-left:3.3pt;padding-right:3.3pt;">-</td><td class="ltx_td ltx_border_t" id="S6.T6.2.10.10.7" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.10.10.8" style="padding-left:3.3pt;padding-right:3.3pt;">88.6</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.10.10.9" style="padding-left:3.3pt;padding-right:3.3pt;">91.1</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.10.10.10" style="padding-left:3.3pt;padding-right:3.3pt;">83.8</td><td class="ltx_td ltx_border_t" id="S6.T6.2.10.10.11" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.10.10.12" style="padding-left:3.3pt;padding-right:3.3pt;">81.0</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.10.10.13" style="padding-left:3.3pt;padding-right:3.3pt;">85.4</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.10.10.14" style="padding-left:3.3pt;padding-right:3.3pt;">71.6</td><td class="ltx_td ltx_border_t" id="S6.T6.2.10.10.15" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.10.10.16" style="padding-left:3.3pt;padding-right:3.3pt;">84.6</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.10.10.17" style="padding-left:3.3pt;padding-right:3.3pt;">84.7</td><td class="ltx_td ltx_border_t" id="S6.T6.2.10.10.18" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.2.10.10.19" style="padding-left:3.3pt;padding-right:3.3pt;">-</td></tr><tr class="ltx_tr" id="S6.T6.2.11.11"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T6.2.11.11.1" style="padding-left:3.3pt;padding-right:3.3pt;"><em class="ltx_emph ltx_font_italic" id="S6.T6.2.11.11.1.1" style="font-size:90%;">Florence-2-B</em></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T6.2.11.11.2" style="padding-left:3.3pt;padding-right:3.3pt;">0.23B</th><td class="ltx_td" id="S6.T6.2.11.11.3" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.11.11.4" style="padding-left:3.3pt;padding-right:3.3pt;">41.4</td><td class="ltx_td" id="S6.T6.2.11.11.5" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.11.11.6" style="padding-left:3.3pt;padding-right:3.3pt;">84.0</td><td class="ltx_td" id="S6.T6.2.11.11.7" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.11.11.8" style="padding-left:3.3pt;padding-right:3.3pt;">92.6</td><td class="ltx_td ltx_align_center" id="S6.T6.2.11.11.9" style="padding-left:3.3pt;padding-right:3.3pt;">94.8</td><td class="ltx_td ltx_align_center" id="S6.T6.2.11.11.10" style="padding-left:3.3pt;padding-right:3.3pt;">91.5</td><td class="ltx_td" id="S6.T6.2.11.11.11" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.11.11.12" style="padding-left:3.3pt;padding-right:3.3pt;">86.8</td><td class="ltx_td ltx_align_center" id="S6.T6.2.11.11.13" style="padding-left:3.3pt;padding-right:3.3pt;">91.7</td><td class="ltx_td ltx_align_center" id="S6.T6.2.11.11.14" style="padding-left:3.3pt;padding-right:3.3pt;">82.2</td><td class="ltx_td" id="S6.T6.2.11.11.15" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.11.11.16" style="padding-left:3.3pt;padding-right:3.3pt;">89.8</td><td class="ltx_td ltx_align_center" id="S6.T6.2.11.11.17" style="padding-left:3.3pt;padding-right:3.3pt;">82.2</td><td class="ltx_td" id="S6.T6.2.11.11.18" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center" id="S6.T6.2.11.11.19" style="padding-left:3.3pt;padding-right:3.3pt;">78.0</td></tr><tr class="ltx_tr" id="S6.T6.2.12.12"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T6.2.12.12.1" style="padding-left:3.3pt;padding-right:3.3pt;"><em class="ltx_emph ltx_font_italic" id="S6.T6.2.12.12.1.1" style="font-size:90%;">Florence-2-L</em></th><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T6.2.12.12.2" style="padding-left:3.3pt;padding-right:3.3pt;">0.77B</th><td class="ltx_td ltx_border_bb" id="S6.T6.2.12.12.3" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.2.12.12.4" style="padding-left:3.3pt;padding-right:3.3pt;">43.4</td><td class="ltx_td ltx_border_bb" id="S6.T6.2.12.12.5" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.2.12.12.6" style="padding-left:3.3pt;padding-right:3.3pt;">85.2</td><td class="ltx_td ltx_border_bb" id="S6.T6.2.12.12.7" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.2.12.12.8" style="padding-left:3.3pt;padding-right:3.3pt;">93.4</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.2.12.12.9" style="padding-left:3.3pt;padding-right:3.3pt;">95.3</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.2.12.12.10" style="padding-left:3.3pt;padding-right:3.3pt;">92.0</td><td class="ltx_td ltx_border_bb" id="S6.T6.2.12.12.11" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.2.12.12.12" style="padding-left:3.3pt;padding-right:3.3pt;">88.3</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.2.12.12.13" style="padding-left:3.3pt;padding-right:3.3pt;">92.9</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.2.12.12.14" style="padding-left:3.3pt;padding-right:3.3pt;">83.6</td><td class="ltx_td ltx_border_bb" id="S6.T6.2.12.12.15" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.2.12.12.16" style="padding-left:3.3pt;padding-right:3.3pt;">91.2</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.2.12.12.17" style="padding-left:3.3pt;padding-right:3.3pt;">91.7</td><td class="ltx_td ltx_border_bb" id="S6.T6.2.12.12.18" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.2.12.12.19" style="padding-left:3.3pt;padding-right:3.3pt;">80.5</td></tr></tbody></table>

| ã€è¡¨æ ‡é¢˜ã€‘åŸæ–‡ | ã€è¡¨æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Table 6:  Performance of specialist and generalist models on region-level tasks. | âœ… Table 6:  ä¸“å®¶æ¨¡å‹å’Œé€šæ‰æ¨¡å‹åœ¨åŒºåŸŸçº§ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ |
| âœ… Specialist Models refer to those that are fine-tuned specifically for each task, while Generalist Models denote a single model fine-tuned in a task-agnostic manner, applicable across all tasks. | âœ… Specialist Models æŒ‡é’ˆå¯¹æ¯ä¸ªä»»åŠ¡ä¸“é—¨è¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ï¼Œè€Œ Generalist Models è¡¨ç¤ºä»¥ä¸ä»»åŠ¡æ— å…³çš„æ–¹å¼è¿›è¡Œå¾®è°ƒçš„å•ä¸€æ¨¡å‹ï¼Œé€‚ç”¨äºæ‰€æœ‰ä»»åŠ¡ã€‚ |

| ã€ç¬¬6.3èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬6.3èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We demonstrate the versatility and effectiveness of our model as a vision foundation that can be transferred to various downstream tasks. | âœ… æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹ä½œä¸ºå¯è½¬ç§»åˆ°å„ç§ä¸‹æ¸¸ä»»åŠ¡çš„è§†è§‰åŸºç¡€çš„å¤šåŠŸèƒ½æ€§å’Œæœ‰æ•ˆæ€§ã€‚ |
| âœ… We fine-tune Florence-2 models by adding a collection of public datasets that cover image-level, region-level, pixel-level tasks, yielding one generalist model for various vision tasks. | âœ… æˆ‘ä»¬é€šè¿‡æ·»åŠ æ¶µç›–å›¾åƒçº§ã€åŒºåŸŸçº§ã€åƒç´ çº§ä»»åŠ¡çš„å…¬å…±æ•°æ®é›†é›†åˆæ¥å¾®è°ƒ Florence-2 æ¨¡å‹ï¼Œä»è€Œäº§ç”Ÿé€‚ç”¨äºå„ç§è§†è§‰ä»»åŠ¡çš„ one é€šç”¨æ¨¡å‹ã€‚ |
| âœ… The details of the dataset collection are provided in TableÂ 14. | âœ… æ•°æ®é›†æ”¶é›†çš„è¯¦ç»†ä¿¡æ¯åœ¨ TableÂ 14 ä¸­æä¾›ã€‚ |
| âœ… TablesÂ 5 and 6 compare our model with other state-of-the-art models. | âœ… TablesÂ 5 å’Œ 6 å°†æˆ‘ä»¬çš„æ¨¡å‹ä¸å…¶ä»–æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚ |
| âœ… Our key findings are: | âœ… æˆ‘ä»¬çš„ä¸»è¦å‘ç°æ˜¯ï¼š |

#### 6.3.1 Simple design for strong performance.

| ã€ç¬¬6.3.1èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬6.3.1èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Florence-2 demonstrates strong performance with standard multi-modality Transformer encoder-decoder without special designs, particularly for region-level and pixel-level tasks. | âœ… Florence-2 ä½¿ç”¨ standard å¤šæ¨¡æ€ Transformer ç¼–ç å™¨-è§£ç å™¨å±•ç¤ºäº† strong çš„æ€§èƒ½ï¼Œæ— éœ€ç‰¹æ®Šè®¾è®¡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåŒºåŸŸçº§å’Œåƒç´ çº§ä»»åŠ¡ã€‚ |
| âœ… For example, Florence-2-L outperforms PolyFormer ( **Polyformer: Referring image segmentation as sequential polygon generation.** ) on both RefCOCO REC task and RES task by 3.0 Accuracy@0.5 and 3.54 mIOU respectively, where PolyFormer ( **Polyformer: Referring image segmentation as sequential polygon generation.** ) adapts specifically designed regression-based prediction head for coordinates. | âœ… ä¾‹å¦‚ï¼ŒFlorence-2-L åœ¨ RefCOCO REC ä»»åŠ¡å’Œ RES ä»»åŠ¡ä¸Šçš„è¡¨ç°åˆ†åˆ«æ¯” PolyFormer ( **Polyformer: Referring image segmentation as sequential polygon generation.** ) é«˜å‡º 3.0 Accuracy@0.5 å’Œ 3.54 mIOUï¼Œå…¶ä¸­ PolyFormer ( **Polyformer: Referring image segmentation as sequential polygon generation.** ) é‡‡ç”¨ä¸“é—¨è®¾è®¡çš„åŸºäºå›å½’çš„åæ ‡é¢„æµ‹å¤´ã€‚ |
| âœ… Florence-2-L also outperforms previous SOTA method UNINEXT ( **Universal instance perception as object discovery and retrieval.** ) on RefCOCO by 0.8 Accuracy@0.5, where UNINEXT ( **Universal instance perception as object discovery and retrieval.** ) is based on advanced object detector Deformable DETR ( **Deformable detr: Deformable transformers for end-to-end object detection.** ) and DINO ( **Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** ) . | âœ… Florence-2-L åœ¨ RefCOCO ä¸Šçš„è¡¨ç°ä¹Ÿæ¯”ä¹‹å‰çš„ SOTA æ–¹æ³• UNINEXT ( **Universal instance perception as object discovery and retrieval.** ) é«˜å‡º 0.8 ä¸ªå‡†ç¡®åº¦@0.5ï¼Œå…¶ä¸­ UNINEXT ( **Universal instance perception as object discovery and retrieval.** ) åŸºäºå…ˆè¿›çš„ç‰©ä½“æ£€æµ‹å™¨ Deformable DETR ( **Deformable detr: Deformable transformers for end-to-end object detection.** ) å’Œ DINO ( **Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** )ã€‚ |

#### 6.3.2 Competitive performance with fewer parameters.

| ã€ç¬¬6.3.2èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬6.3.2èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Florence-2-L achieves competitive performance without the need for LLMs, showcasing efficiency in handling diverse tasks while maintaining a compact size. | âœ… Florence-2-L æ— éœ€ LLM å³å¯å®ç°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå±•ç°å‡ºåœ¨ä¿æŒç´§å‡‘å°ºå¯¸çš„åŒæ—¶å¤„ç†å„ç§ä»»åŠ¡çš„æ•ˆç‡ã€‚ |
| âœ… For instance, Florence-2-L attains a CIDEr score of 140.0 on the COCO Caption karpathy test split ( **Deep visual-semantic alignments for generating image descriptions.** ) , outperforming models with significantly more parameters, such as Flamingo (80B parameters, 138.1 CIDEr score). | âœ… ä¾‹å¦‚ï¼ŒFlorence-2-L åœ¨ COCO Caption karpathy æµ‹è¯•åˆ†å‰² ( **Deep visual-semantic alignments for generating image descriptions.** ) ä¸Šè·å¾—äº† 140.0 çš„ CIDEr åˆ†æ•°ï¼Œå…¶è¡¨ç°ä¼˜äºå…·æœ‰æ›´å¤šå‚æ•°çš„æ¨¡å‹ï¼Œä¾‹å¦‚ Flamingoï¼ˆ80B å‚æ•°ï¼Œ138.1 CIDEr åˆ†æ•°ï¼‰ã€‚ |

#### 6.3.3 Adaptable generalization across task levels.

| ã€ç¬¬6.3.3èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬6.3.3èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Florence-2 demonstrates competitive performance across image-level, pixel-level, and region-level tasks, emphasizing its adaptability and effectiveness in addressing various challenges in computer vision and natural language processing. | âœ… Florence-2 åœ¨å›¾åƒçº§ã€åƒç´ çº§å’ŒåŒºåŸŸçº§ä»»åŠ¡ä¸­å±•ç°å‡ºæå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå‡¸æ˜¾äº†å…¶åœ¨è§£å†³è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å„ç§æŒ‘æˆ˜æ–¹é¢çš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚ |
| âœ… For example, in the TextVQA task, Florence-2-L sets a new state-of-the-art performance with an accuracy of 81.5 without any external OCR token input, surpassing previous SOTA methods ( **1. Pali: A jointly-scaled multilingual language-image model, 2022.** ï½œ **2. Pali-x: On scaling up a multilingual vision and language model.** ) . | âœ… ä¾‹å¦‚ï¼Œåœ¨ TextVQA ä»»åŠ¡ä¸­ï¼ŒFlorence-2-L åœ¨æ²¡æœ‰ä»»ä½•å¤–éƒ¨ OCR token è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œä»¥ 81.5 çš„å‡†ç¡®ç‡åˆ›ä¸‹äº†æ–°çš„ SOTA æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„ SOTA æ–¹æ³• ( **1. Pali: A jointly-scaled multilingual language-image model, 2022.** ï½œ **2. Pali-x: On scaling up a multilingual vision and language model.** )ã€‚ |

| ã€ç¬¬6.3.3èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬6.3.3èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… These achievements emphasize Florence-2 â€™s efficiency in handling diverse tasks while maintaining a compact size, making it a unique and valuable asset in the ever-evolving landscape of AI research and applications. | âœ… è¿™äº›æˆå°±å‡¸æ˜¾äº†Florence-2åœ¨ä¿æŒç´§å‡‘å°ºå¯¸çš„åŒæ—¶å¤„ç†å¤šæ ·åŒ–ä»»åŠ¡çš„æ•ˆç‡ï¼Œä½¿å…¶æˆä¸ºä¸æ–­å‘å±•çš„äººå·¥æ™ºèƒ½ç ”ç©¶å’Œåº”ç”¨é¢†åŸŸä¸­ç‹¬ç‰¹è€Œå®è´µçš„èµ„äº§ã€‚ |

### 6.4 Downstream Tasks Fine-tuning

| ã€ç¬¬6.4èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬6.4èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… In this section, we investigate the performance of our single model fine-tuning on downstream tasks. | âœ… åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶å•ä¸€æ¨¡å‹å¾®è°ƒåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ |
| âœ… This experiment highlights the superiority of Florence-2 pre-training over previous approaches, as it demonstrates the effectiveness of the learned universal image representation. | âœ… è¯¥å®éªŒå‡¸æ˜¾äº† Florence-2 é¢„è®­ç»ƒç›¸å¯¹äºä»¥å‰æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œå› ä¸ºå®ƒè¯æ˜äº†æ‰€å­¦ä¹ çš„é€šç”¨å›¾åƒè¡¨ç¤ºçš„æœ‰æ•ˆæ€§ã€‚ |
| âœ… We use the base size model with about 80M parameters in our experiments to ensure fair comparison with other methods. | âœ… æˆ‘ä»¬åœ¨å®éªŒä¸­ä½¿ç”¨å…·æœ‰çº¦ 80M ä¸ªå‚æ•°çš„åŸºæœ¬å°ºå¯¸æ¨¡å‹ï¼Œä»¥ç¡®ä¿ä¸å…¶ä»–æ–¹æ³•è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚ |

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/x9.png)

| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… (a)  Mask-RCNN on COCO detection. | âœ… (a)  Mask-RCNN on COCO detection. |

#### 6.4.1 Object detection and segmentation.

| ã€ç¬¬6.4.1èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬6.4.1èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We conduct COCO object detection and instance segmentation ( **Microsoft coco: Common objects in context.** ) experiments with Mask R-CNN ( **Mask r-cnn.** ) , and COCO object detection ( **Microsoft coco: Common objects in context.** ) experiments with DINO ( **Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** ) to further demonstrate the effectiveness of Florence-2 pre-training. | âœ… æˆ‘ä»¬åˆ©ç”¨ Mask R-CNN ( **Mask r-cnn.** ) è¿›è¡Œ COCO å¯¹è±¡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰² ( **Microsoft coco: Common objects in context.** ) å®éªŒï¼Œåˆ©ç”¨ DINO ( **Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** ) è¿›è¡Œ COCO å¯¹è±¡æ£€æµ‹ ( **Microsoft coco: Common objects in context.** ) å®éªŒï¼Œè¿›ä¸€æ­¥è¯æ˜ Florence-2 é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚ |
| âœ… We train on the train2017 split and evaluate on the val2017 split. | âœ… æˆ‘ä»¬åœ¨ train2017 åˆ†å‰²ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨ val2017 åˆ†å‰²ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ |

| ã€ç¬¬6.4.1èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬6.4.1èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… For Mask R-CNN ( **Mask r-cnn.** ) experiments, we follow the common setup used in ( **1. Swin transformer: Hierarchical vision transformer using shifted windows, 2021.** ï½œ **2. Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** ) , we use the standard 1  $\times$  (12 epochs) schedule with multi-scale training for all experiments. | âœ… å¯¹äº Mask R-CNN ( **Mask r-cnn.** ) å®éªŒï¼Œæˆ‘ä»¬éµå¾ª ( **1. Swin transformer: Hierarchical vision transformer using shifted windows, 2021.** ï½œ **2. Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** ) ä¸­ä½¿ç”¨çš„é€šç”¨è®¾ç½®ï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰å®éªŒä½¿ç”¨æ ‡å‡† 1  $\times$ ï¼ˆ12 ä¸ªæ—¶æœŸï¼‰è®¡åˆ’å’Œå¤šå°ºåº¦è®­ç»ƒã€‚ |
| âœ… The learning rate is stepped down by a factor of 0.1 at the 67% and 89% of training epochs. | âœ… åœ¨è®­ç»ƒé˜¶æ®µçš„ 67% å’Œ 89% æ—¶ï¼Œå­¦ä¹ ç‡é™ä½äº† 0.1 å€ã€‚ |
| âœ… We do not use any additional augmentation (such as random crop, mosaic, etc) or optimization techniques (such as EMA, weight normalization) during training to ensure a fair comparison. | âœ… ä¸ºäº†ç¡®ä¿å…¬å¹³æ¯”è¾ƒï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´ä¸ä½¿ç”¨ä»»ä½•é¢å¤–çš„å¢å¼ºï¼ˆå¦‚éšæœºè£å‰ªã€é©¬èµ›å…‹ç­‰ï¼‰æˆ–ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚ EMAã€æƒé‡æ ‡å‡†åŒ–ï¼‰ã€‚ |
| âœ… We do not use any test time augmentation (TTA) either. | âœ… æˆ‘ä»¬ä¹Ÿä¸ä½¿ç”¨ä»»ä½•æµ‹è¯•æ—¶é—´å¢å¼ºï¼ˆTTAï¼‰ã€‚ |
| âœ… Thanks to the strong universal representation learned by Florence-2 pre-training, we do not require longer training epochs, such as 36 epochs in ( **1. Convnext v2: Co-designing and scaling convnets with masked autoencoders.** ï½œ **2. Swin transformer: Hierarchical vision transformer using shifted windows, 2021.** ï½œ **3. Focal self-attention for local-global interactions in vision transformers.** ï½œ **4. Focal modulation networks.** ) , or 100 epochs in ( **Exploring plain vision transformer backbones for object detection.** ) , to achieve better results. | âœ… å¾—ç›Šäº Florence-2 é¢„è®­ç»ƒå­¦ä¹ åˆ°çš„å¼ºå¤§çš„é€šç”¨è¡¨å¾ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ›´é•¿çš„è®­ç»ƒå‘¨æœŸï¼ˆä¾‹å¦‚ ( **1. Convnext v2: Co-designing and scaling convnets with masked autoencoders.** ï½œ **2. Swin transformer: Hierarchical vision transformer using shifted windows, 2021.** ï½œ **3. Focal self-attention for local-global interactions in vision transformers.** ï½œ **4. Focal modulation networks.** ) ä¸­çš„ 36 ä¸ªå‘¨æœŸæˆ– ( **Exploring plain vision transformer backbones for object detection.** ) ä¸­çš„ 100 ä¸ªå‘¨æœŸï¼‰å³å¯è·å¾—æ›´å¥½çš„ç»“æœã€‚ |

| ã€ç¬¬6.4.1èŠ‚ï¼Œç¬¬3æ®µã€‘åŸæ–‡ | ã€ç¬¬6.4.1èŠ‚ï¼Œç¬¬3æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… For DINO ( **Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** ) experiments, we train DINO-4scale ( **Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** ) detector for 12 epochs (1  $\times$  ) using the same data augmentation strategy as employed by ( **End-to-end object detection with transformers.** ) . | âœ… å¯¹äº DINO ( **Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** ) å®éªŒï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸ ( **End-to-end object detection with transformers.** ) ç›¸åŒçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå¯¹ DINO-4scale ( **Dino: Detr with improved denoising anchor boxes for end-to-end object detection.** ) æ£€æµ‹å™¨è¿›è¡Œ 12 ä¸ªæ—¶æœŸï¼ˆ1  $\times$ ï¼‰çš„è®­ç»ƒã€‚ |

| ã€ç¬¬6.4.1èŠ‚ï¼Œç¬¬4æ®µã€‘åŸæ–‡ | ã€ç¬¬6.4.1èŠ‚ï¼Œç¬¬4æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… First, our base model achieves a strong performance improvement compared to other approaches. | âœ… é¦–å…ˆï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„åŸºç¡€æ¨¡å‹å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ |
| âœ… As shown in TableÂ 7 , our DaViT-B model pre-trained by Florence-2 surpasses previous best base model (ConvNext v2-B), which is pre-trained by FCMAE ( **Convnext v2: Co-designing and scaling convnets with masked autoencoders.** ) , by 0.7  $AP_{b}$  using Mask RCNN. | âœ… å¦‚ TableÂ 7 æ‰€ç¤ºï¼Œæˆ‘ä»¬é€šè¿‡ Florence-2 é¢„è®­ç»ƒçš„ DaViT-B æ¨¡å‹æ¯”ä¹‹å‰ä½¿ç”¨ FCMAE ( **Convnext v2: Co-designing and scaling convnets with masked autoencoders.** ) é¢„è®­ç»ƒçš„æœ€ä½³åŸºç¡€æ¨¡å‹ï¼ˆConvNext v2-Bï¼‰é«˜å‡º 0.7 ä¸ª  $AP_{b}$ ï¼ˆä½¿ç”¨ Mask RCNNï¼‰ã€‚ |
| âœ… Importantly, while ConvNeXt v2-B leverages a 3  $\times$  schedule (36 epochs), our model efficiently employs a 1  $\times$  schedule (12 epochs) thanks to our powerful pre-trained universal representation. | âœ… é‡è¦çš„æ˜¯ï¼Œè™½ç„¶ ConvNeXt v2-B åˆ©ç”¨äº† 3  $\times$  è®¡åˆ’ï¼ˆ36 ä¸ªæ—¶æœŸï¼‰ï¼Œä½†ç”±äºæˆ‘ä»¬å¼ºå¤§çš„é¢„è®­ç»ƒé€šç”¨è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„æ¨¡å‹æœ‰æ•ˆåœ°é‡‡ç”¨äº† 1  $\times$  è®¡åˆ’ï¼ˆ12 ä¸ªæ—¶æœŸï¼‰ã€‚ |
| âœ… For DINO framework, our model significantly outperforms the ViT-B, achieving a notable improvement of 4.2 AP. | âœ… å¯¹äºDINOæ¡†æ¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ˜æ˜¾ä¼˜äºViT-Bï¼Œå®ç°äº†4.2 APçš„æ˜¾è‘—æå‡ã€‚ |

| ã€ç¬¬6.4.1èŠ‚ï¼Œç¬¬5æ®µã€‘åŸæ–‡ | ã€ç¬¬6.4.1èŠ‚ï¼Œç¬¬5æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Second, our pre-training demonstrates higher training efficiency. | âœ… å…¶æ¬¡ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒè¡¨ç°å‡ºæ›´é«˜çš„è®­ç»ƒæ•ˆç‡ã€‚ |
| âœ… As shown in TableÂ 8 and FigureÂ 6 , compared to the model with supervised ImageNet-1k pre-training, our model with Florence-2 pre-training achieves 4x efficiency and a significant improvement of 6.9 AP and 5.5 AP with Mask-RCNN and DINO framework, respectively. | âœ… å¦‚TableÂ 8å’ŒFigureÂ 6æ‰€ç¤ºï¼Œä¸æœ‰ç›‘ç£çš„ImageNet-1ké¢„è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬é‡‡ç”¨Florence-2é¢„è®­ç»ƒçš„æ¨¡å‹åœ¨ä½¿ç”¨Mask-RCNNå’ŒDINOæ¡†æ¶æ—¶åˆ†åˆ«å®ç°äº†4å€çš„æ•ˆç‡å’Œ6.9 APå’Œ5.5 APçš„æ˜¾è‘—æå‡ã€‚ |

| ã€ç¬¬6.4.1èŠ‚ï¼Œç¬¬6æ®µã€‘åŸæ–‡ | ã€ç¬¬6.4.1èŠ‚ï¼Œç¬¬6æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Third, our pre-training provides a good generic representation without extensive fine-tuning. | âœ… ç¬¬ä¸‰ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæä¾›äº†è‰¯å¥½çš„é€šç”¨è¡¨ç¤ºï¼Œæ— éœ€è¿›è¡Œå¤§é‡çš„å¾®è°ƒã€‚ |
| âœ… TableÂ 8 indicates that the models with Florence-2 pre-training maintains competitive performances when the first two stages are frozen with only 0.3 and 0.2 drops for Mask-RCNN and DINO, respectively. | âœ… TableÂ 8 è¡¨ç¤ºå½“å‰ä¸¤ä¸ªé˜¶æ®µå†»ç»“æ—¶ï¼Œä½¿ç”¨ Florence-2 é¢„è®­ç»ƒçš„æ¨¡å‹ä¿æŒäº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¯¹äº Mask-RCNN å’Œ DINOï¼Œåˆ†åˆ«åªæœ‰ 0.3 å’Œ 0.2 çš„ä¸‹é™ã€‚ |
| âœ… Moreover, our approach with completely frozen backbone can outperform the model with supervised ImageNet-1k pre-training by 1.6 and 2.4 for Mask-RCNN and DINO. | âœ… æ­¤å¤–ï¼Œå¯¹äº Mask-RCNN å’Œ DINOï¼Œæˆ‘ä»¬é‡‡ç”¨å®Œå…¨å†»ç»“ä¸»å¹²çš„æ–¹æ³•å¯ä»¥æ¯”ä½¿ç”¨ç›‘ç£ ImageNet-1k é¢„è®­ç»ƒçš„æ¨¡å‹è¡¨ç°æ›´å¥½ 1.6 å’Œ 2.4ã€‚ |

<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T7.2"><thead class="ltx_thead"><tr class="ltx_tr" id="S6.T7.2.3.1"><th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T7.2.3.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"></th><th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T7.2.3.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"></th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T7.2.3.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S6.T7.2.3.1.4" style="padding-left:3.0pt;padding-right:3.0pt;">Mask R-CNN</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T7.2.3.1.5" style="padding-left:3.0pt;padding-right:3.0pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T7.2.3.1.6" style="padding-left:3.0pt;padding-right:3.0pt;">DINO</th></tr><tr class="ltx_tr" id="S6.T7.2.2"><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S6.T7.2.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">Backbone</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S6.T7.2.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">Pretrain</th><th class="ltx_td ltx_th ltx_th_column" id="S6.T7.2.2.5" style="padding-left:3.0pt;padding-right:3.0pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T7.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">AP<sub class="ltx_sub" id="S6.T7.1.1.1.2">b</sub></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T7.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">AP<sub class="ltx_sub" id="S6.T7.2.2.2.2">m</sub></th><th class="ltx_td ltx_th ltx_th_column" id="S6.T7.2.2.6" style="padding-left:3.0pt;padding-right:3.0pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T7.2.2.7" style="padding-left:3.0pt;padding-right:3.0pt;">AP</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr" id="S6.T7.2.4.1"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T7.2.4.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">ViT-BÂ <html><body><p>( <strong>Exploring plain vision transformer backbones for object detection.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T7.2.4.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">MAE, IN-1k</th><td class="ltx_td ltx_border_t" id="S6.T7.2.4.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.2.4.1.4" style="padding-left:3.0pt;padding-right:3.0pt;">51.6</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.2.4.1.5" style="padding-left:3.0pt;padding-right:3.0pt;">45.9</td><td class="ltx_td ltx_border_t" id="S6.T7.2.4.1.6" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.2.4.1.7" style="padding-left:3.0pt;padding-right:3.0pt;">55.0</td></tr><tr class="ltx_tr" id="S6.T7.2.5.2"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T7.2.5.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">Swin-B <html><body><p>( <strong>Swin transformer: Hierarchical vision transformer using shiftedwindows, 2021.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.2.5.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">Sup IN-1k</th><td class="ltx_td" id="S6.T7.2.5.2.3" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center" id="S6.T7.2.5.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">50.2</td><td class="ltx_td ltx_align_center" id="S6.T7.2.5.2.5" style="padding-left:3.0pt;padding-right:3.0pt;">-</td><td class="ltx_td" id="S6.T7.2.5.2.6" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center" id="S6.T7.2.5.2.7" style="padding-left:3.0pt;padding-right:3.0pt;">53.4</td></tr><tr class="ltx_tr" id="S6.T7.2.6.3"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T7.2.6.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">Swin-B <html><body><p>( <strong>Swin transformer: Hierarchical vision transformer using shiftedwindows, 2021.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.2.6.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">SimMIMÂ <html><body><p>( <strong>Simmim: A simple framework for masked image modeling.</strong> )</p></body></html></th><td class="ltx_td" id="S6.T7.2.6.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center" id="S6.T7.2.6.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">52.3</td><td class="ltx_td ltx_align_center" id="S6.T7.2.6.3.5" style="padding-left:3.0pt;padding-right:3.0pt;">-</td><td class="ltx_td" id="S6.T7.2.6.3.6" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center" id="S6.T7.2.6.3.7" style="padding-left:3.0pt;padding-right:3.0pt;">-</td></tr><tr class="ltx_tr" id="S6.T7.2.7.4"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T7.2.7.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">FocalAtt-BÂ <html><body><p>( <strong>Focal self-attention for local-global interactions in visiontransformers.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.2.7.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">Sup IN-1k</th><td class="ltx_td" id="S6.T7.2.7.4.3" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center" id="S6.T7.2.7.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">49.0</td><td class="ltx_td ltx_align_center" id="S6.T7.2.7.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">43.7</td><td class="ltx_td" id="S6.T7.2.7.4.6" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center" id="S6.T7.2.7.4.7" style="padding-left:3.0pt;padding-right:3.0pt;">-</td></tr><tr class="ltx_tr" id="S6.T7.2.8.5"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T7.2.8.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">FocalNet-BÂ <html><body><p>( <strong>Focal modulation networks.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.2.8.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">Sup IN-1k</th><td class="ltx_td" id="S6.T7.2.8.5.3" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center" id="S6.T7.2.8.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">49.8</td><td class="ltx_td ltx_align_center" id="S6.T7.2.8.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">44.1</td><td class="ltx_td" id="S6.T7.2.8.5.6" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center" id="S6.T7.2.8.5.7" style="padding-left:3.0pt;padding-right:3.0pt;">54.4</td></tr><tr class="ltx_tr" id="S6.T7.2.9.6"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T7.2.9.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">ConvNeXt v1-BÂ <html><body><p>( <strong>A convnet for the 2020s.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.2.9.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">Sup IN-1k</th><td class="ltx_td" id="S6.T7.2.9.6.3" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center" id="S6.T7.2.9.6.4" style="padding-left:3.0pt;padding-right:3.0pt;">50.3</td><td class="ltx_td ltx_align_center" id="S6.T7.2.9.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">44.9</td><td class="ltx_td" id="S6.T7.2.9.6.6" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center" id="S6.T7.2.9.6.7" style="padding-left:3.0pt;padding-right:3.0pt;">52.6</td></tr><tr class="ltx_tr" id="S6.T7.2.10.7"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T7.2.10.7.1" style="padding-left:3.0pt;padding-right:3.0pt;">ConvNeXt v2-BÂ <html><body><p>( <strong>Convnext v2: Co-designing and scaling convnets with maskedautoencoders.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.2.10.7.2" style="padding-left:3.0pt;padding-right:3.0pt;">Sup IN-1k</th><td class="ltx_td" id="S6.T7.2.10.7.3" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center" id="S6.T7.2.10.7.4" style="padding-left:3.0pt;padding-right:3.0pt;">51.0</td><td class="ltx_td ltx_align_center" id="S6.T7.2.10.7.5" style="padding-left:3.0pt;padding-right:3.0pt;">45.6</td><td class="ltx_td" id="S6.T7.2.10.7.6" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center" id="S6.T7.2.10.7.7" style="padding-left:3.0pt;padding-right:3.0pt;">-</td></tr><tr class="ltx_tr" id="S6.T7.2.11.8"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T7.2.11.8.1" style="padding-left:3.0pt;padding-right:3.0pt;">ConvNeXt v2-BÂ <html><body><p>( <strong>Convnext v2: Co-designing and scaling convnets with maskedautoencoders.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T7.2.11.8.2" style="padding-left:3.0pt;padding-right:3.0pt;">FCMAE</th><td class="ltx_td" id="S6.T7.2.11.8.3" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center" id="S6.T7.2.11.8.4" style="padding-left:3.0pt;padding-right:3.0pt;">52.9</td><td class="ltx_td ltx_align_center" id="S6.T7.2.11.8.5" style="padding-left:3.0pt;padding-right:3.0pt;">46.6</td><td class="ltx_td" id="S6.T7.2.11.8.6" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center" id="S6.T7.2.11.8.7" style="padding-left:3.0pt;padding-right:3.0pt;">-</td></tr><tr class="ltx_tr" id="S6.T7.2.12.9" style="background-color:#E6E6E6;"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T7.2.12.9.1" style="padding-left:3.0pt;padding-right:3.0pt;">DaViT-BÂ <html><body><p>( <strong>Davit: Dual attention vision transformers.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T7.2.12.9.2" style="padding-left:3.0pt;padding-right:3.0pt;"><em class="ltx_emph ltx_font_italic" id="S6.T7.2.12.9.2.1" style="font-size:90%;background-color:#E6E6E6;">Florence-2</em></th><td class="ltx_td ltx_border_bb" id="S6.T7.2.12.9.3" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.2.12.9.4" style="padding-left:3.0pt;padding-right:3.0pt;">53.6</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.2.12.9.5" style="padding-left:3.0pt;padding-right:3.0pt;">46.4</td><td class="ltx_td ltx_border_bb" id="S6.T7.2.12.9.6" style="padding-left:3.0pt;padding-right:3.0pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.2.12.9.7" style="padding-left:3.0pt;padding-right:3.0pt;">59.2</td></tr></tbody></table>

| ã€è¡¨æ ‡é¢˜ã€‘åŸæ–‡ | ã€è¡¨æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Table 7:  COCO object detection and instance segmentation results using Mask-RCNN framework, and COCO object detection results using DINO-4scale framework. | âœ… Table 7: ã€COCO object detection and instance segmentation resultsé‡‡ç”¨Mask-RCNNæ¡†æ¶ï¼ŒCOCO object detection resultsé‡‡ç”¨DINO-4scaleæ¡†æ¶ã€‚ |
| âœ… All the entries use a base size model to ensure a fair comparison. | âœ… æ‰€æœ‰å‚èµ›ä½œå“å‡é‡‡ç”¨åŸºæœ¬å°ºå¯¸æ¨¡å‹ï¼Œä»¥ç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚ |
| âœ… For Mask-RCNN experiments, our method utilizes 1  $\times$  schedule (12 epochs), ViT-B use 100 epochs, all others use 3  $\times$  (36 epochs). | âœ… å¯¹äº Mask-RCNN å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ 1  $\times$  è®¡åˆ’ï¼ˆ12 ä¸ª epochï¼‰ï¼ŒViT-B ä½¿ç”¨ 100 ä¸ª epochï¼Œå…¶ä»–æ‰€æœ‰æ–¹æ³•å‡ä½¿ç”¨ 3  $\times$ ï¼ˆ36 ä¸ª epochï¼‰ã€‚ |
| âœ… For DINO experiments, all the entries use 1  $\times$  schedule except for ViT-B which uses 50 epochs. | âœ… å¯¹äº DINO å®éªŒï¼Œé™¤ ViT-B ä½¿ç”¨ 50 ä¸ªæ—¶æœŸå¤–ï¼Œæ‰€æœ‰æ¡ç›®éƒ½ä½¿ç”¨ 1  $\times$  è®¡åˆ’ã€‚ |

<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T8.2"><tbody class="ltx_tbody"><tr class="ltx_tr" id="S6.T8.2.3.1"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S6.T8.2.3.1.1" rowspan="2" style="padding-left:2.5pt;padding-right:2.5pt;">Pretrain</th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S6.T8.2.3.1.2" rowspan="2" style="padding-left:2.5pt;padding-right:2.5pt;">Frozen stages</th><td class="ltx_td ltx_border_tt" id="S6.T8.2.3.1.3" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S6.T8.2.3.1.4" style="padding-left:2.5pt;padding-right:2.5pt;">Mask R-CNN</td><td class="ltx_td ltx_border_tt" id="S6.T8.2.3.1.5" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T8.2.3.1.6" style="padding-left:2.5pt;padding-right:2.5pt;">DINO</td><td class="ltx_td ltx_border_tt" id="S6.T8.2.3.1.7" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T8.2.3.1.8" style="padding-left:2.5pt;padding-right:2.5pt;">UperNet</td></tr><tr class="ltx_tr" id="S6.T8.2.2"><td class="ltx_td" id="S6.T8.2.2.3" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.1.1.1" style="padding-left:2.5pt;padding-right:2.5pt;">AP<sub class="ltx_sub" id="S6.T8.1.1.1.2">b</sub></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.2.2.2" style="padding-left:2.5pt;padding-right:2.5pt;">AP<sub class="ltx_sub" id="S6.T8.2.2.2.2">m</sub></td><td class="ltx_td" id="S6.T8.2.2.4" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.2.2.5" style="padding-left:2.5pt;padding-right:2.5pt;">AP</td><td class="ltx_td" id="S6.T8.2.2.6" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.2.2.7" style="padding-left:2.5pt;padding-right:2.5pt;">mIoU</td></tr><tr class="ltx_tr" id="S6.T8.2.4.2"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T8.2.4.2.1" style="padding-left:2.5pt;padding-right:2.5pt;">Sup IN1k</th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T8.2.4.2.2" style="padding-left:2.5pt;padding-right:2.5pt;">n/a</th><td class="ltx_td ltx_border_t" id="S6.T8.2.4.2.3" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.2.4.2.4" style="padding-left:2.5pt;padding-right:2.5pt;">46.7</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.2.4.2.5" style="padding-left:2.5pt;padding-right:2.5pt;">42.0</td><td class="ltx_td ltx_border_t" id="S6.T8.2.4.2.6" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.2.4.2.7" style="padding-left:2.5pt;padding-right:2.5pt;">53.7</td><td class="ltx_td ltx_border_t" id="S6.T8.2.4.2.8" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.2.4.2.9" style="padding-left:2.5pt;padding-right:2.5pt;">49</td></tr><tr class="ltx_tr" id="S6.T8.2.5.3"><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T8.2.5.3.1" style="padding-left:2.5pt;padding-right:2.5pt;">UniCLÂ <html><body><p>( <strong>Unified contrastive learning in image-text-label space, 2022.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T8.2.5.3.2" style="padding-left:2.5pt;padding-right:2.5pt;">n/a</th><td class="ltx_td" id="S6.T8.2.5.3.3" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center" id="S6.T8.2.5.3.4" style="padding-left:2.5pt;padding-right:2.5pt;">50.4</td><td class="ltx_td ltx_align_center" id="S6.T8.2.5.3.5" style="padding-left:2.5pt;padding-right:2.5pt;">45.0</td><td class="ltx_td" id="S6.T8.2.5.3.6" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center" id="S6.T8.2.5.3.7" style="padding-left:2.5pt;padding-right:2.5pt;">57.3</td><td class="ltx_td" id="S6.T8.2.5.3.8" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center" id="S6.T8.2.5.3.9" style="padding-left:2.5pt;padding-right:2.5pt;">53.6</td></tr><tr class="ltx_tr" id="S6.T8.2.6.4" style="background-color:#E6E6E6;"><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T8.2.6.4.1" style="padding-left:2.5pt;padding-right:2.5pt;"><em class="ltx_emph ltx_font_italic" id="S6.T8.2.6.4.1.1" style="font-size:90%;background-color:#E6E6E6;">Florence-2</em></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T8.2.6.4.2" style="padding-left:2.5pt;padding-right:2.5pt;">n/a</th><td class="ltx_td" id="S6.T8.2.6.4.3" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center" id="S6.T8.2.6.4.4" style="padding-left:2.5pt;padding-right:2.5pt;">53.6</td><td class="ltx_td ltx_align_center" id="S6.T8.2.6.4.5" style="padding-left:2.5pt;padding-right:2.5pt;">46.4</td><td class="ltx_td" id="S6.T8.2.6.4.6" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center" id="S6.T8.2.6.4.7" style="padding-left:2.5pt;padding-right:2.5pt;">59.2</td><td class="ltx_td" id="S6.T8.2.6.4.8" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center" id="S6.T8.2.6.4.9" style="padding-left:2.5pt;padding-right:2.5pt;">54.9</td></tr><tr class="ltx_tr" id="S6.T8.2.7.5"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T8.2.7.5.1" style="padding-left:2.5pt;padding-right:2.5pt;"><em class="ltx_emph ltx_font_italic" id="S6.T8.2.7.5.1.1" style="font-size:90%;">Florence-2</em></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T8.2.7.5.2" style="padding-left:2.5pt;padding-right:2.5pt;">[1]</th><td class="ltx_td ltx_border_t" id="S6.T8.2.7.5.3" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.2.7.5.4" style="padding-left:2.5pt;padding-right:2.5pt;">53.6</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.2.7.5.5" style="padding-left:2.5pt;padding-right:2.5pt;">46.3</td><td class="ltx_td ltx_border_t" id="S6.T8.2.7.5.6" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.2.7.5.7" style="padding-left:2.5pt;padding-right:2.5pt;">59.2</td><td class="ltx_td ltx_border_t" id="S6.T8.2.7.5.8" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.2.7.5.9" style="padding-left:2.5pt;padding-right:2.5pt;">54.1</td></tr><tr class="ltx_tr" id="S6.T8.2.8.6"><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T8.2.8.6.1" style="padding-left:2.5pt;padding-right:2.5pt;"><em class="ltx_emph ltx_font_italic" id="S6.T8.2.8.6.1.1" style="font-size:90%;">Florence-2</em></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T8.2.8.6.2" style="padding-left:2.5pt;padding-right:2.5pt;">[1, 2]</th><td class="ltx_td" id="S6.T8.2.8.6.3" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center" id="S6.T8.2.8.6.4" style="padding-left:2.5pt;padding-right:2.5pt;">53.3</td><td class="ltx_td ltx_align_center" id="S6.T8.2.8.6.5" style="padding-left:2.5pt;padding-right:2.5pt;">46.1</td><td class="ltx_td" id="S6.T8.2.8.6.6" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center" id="S6.T8.2.8.6.7" style="padding-left:2.5pt;padding-right:2.5pt;">59.0</td><td class="ltx_td" id="S6.T8.2.8.6.8" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center" id="S6.T8.2.8.6.9" style="padding-left:2.5pt;padding-right:2.5pt;">54.4</td></tr><tr class="ltx_tr" id="S6.T8.2.9.7"><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T8.2.9.7.1" style="padding-left:2.5pt;padding-right:2.5pt;"><em class="ltx_emph ltx_font_italic" id="S6.T8.2.9.7.1.1" style="font-size:90%;">Florence-2</em></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T8.2.9.7.2" style="padding-left:2.5pt;padding-right:2.5pt;">[1, 2, 3]</th><td class="ltx_td" id="S6.T8.2.9.7.3" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center" id="S6.T8.2.9.7.4" style="padding-left:2.5pt;padding-right:2.5pt;">49.5</td><td class="ltx_td ltx_align_center" id="S6.T8.2.9.7.5" style="padding-left:2.5pt;padding-right:2.5pt;">42.9</td><td class="ltx_td" id="S6.T8.2.9.7.6" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center" id="S6.T8.2.9.7.7" style="padding-left:2.5pt;padding-right:2.5pt;">56.7</td><td class="ltx_td" id="S6.T8.2.9.7.8" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center" id="S6.T8.2.9.7.9" style="padding-left:2.5pt;padding-right:2.5pt;">49.6</td></tr><tr class="ltx_tr" id="S6.T8.2.10.8"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T8.2.10.8.1" style="padding-left:2.5pt;padding-right:2.5pt;"><em class="ltx_emph ltx_font_italic" id="S6.T8.2.10.8.1.1" style="font-size:90%;">Florence-2</em></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T8.2.10.8.2" style="padding-left:2.5pt;padding-right:2.5pt;">[1, 2, 3, 4]</th><td class="ltx_td ltx_border_bb" id="S6.T8.2.10.8.3" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T8.2.10.8.4" style="padding-left:2.5pt;padding-right:2.5pt;">48.3</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T8.2.10.8.5" style="padding-left:2.5pt;padding-right:2.5pt;">44.5</td><td class="ltx_td ltx_border_bb" id="S6.T8.2.10.8.6" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T8.2.10.8.7" style="padding-left:2.5pt;padding-right:2.5pt;">56.1</td><td class="ltx_td ltx_border_bb" id="S6.T8.2.10.8.8" style="padding-left:2.5pt;padding-right:2.5pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T8.2.10.8.9" style="padding-left:2.5pt;padding-right:2.5pt;">45.9</td></tr></tbody></table>

| ã€è¡¨æ ‡é¢˜ã€‘åŸæ–‡ | ã€è¡¨æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Table 8:  Downstream task fine-tuning on COCO and ADE20K dataset. | âœ… Table 8:  åœ¨ COCO å’Œ ADE20K æ•°æ®é›†ä¸Šè¿›è¡Œä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒã€‚ |
| âœ… COCO object detection using Mask R-CNN and DINO. | âœ… COCO object detection ä½¿ç”¨ Mask R-CNN å’Œ DINOã€‚ |
| âœ… ADE20K semantic segmentation using UperNet. | âœ… ADE20K semantic segmentation ä½¿ç”¨ UpperNetã€‚ |
| âœ… All entries use DaViT-B with 80M parameters as the backbone and standard 1  $\times$  schedule. | âœ… æ‰€æœ‰å‚èµ›ä½œå“å‡é‡‡ç”¨å…·æœ‰ 80M å‚æ•°çš„ DaViT-B ä½œä¸ºä¸»å¹²å’Œæ ‡å‡† 1  $\times$  æ—¶é—´è¡¨ã€‚ |

#### 6.4.2 Semantic segmentation.

| ã€ç¬¬6.4.2èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬6.4.2èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We conduct semantic segmentation experiments with UperNet ( **Unified perceptual parsing for scene understanding.** ) framework on ADE20k ( **Scene parsing through ade20k dataset.** ) dataset. | âœ… æˆ‘ä»¬ä½¿ç”¨UperNet ( **Unified perceptual parsing for scene understanding.** )æ¡†æ¶åœ¨ADE20k ( **Scene parsing through ade20k dataset.** )æ•°æ®é›†ä¸Šè¿›è¡Œè¯­ä¹‰åˆ†å‰²å®éªŒã€‚ |
| âœ… We mostly follow the training and evaluation protocols from Swin ( **Swin transformer: Hierarchical vision transformer using shifted windows, 2021.** ). | âœ… æˆ‘ä»¬ä¸»è¦éµå¾ª Swin ( **Swin transformer: Hierarchical vision transformer using shifted windows, 2021.** ) çš„åŸ¹è®­å’Œè¯„ä¼°åè®®ã€‚ |
| âœ… Specifically, we use input size 512  $\times$  512 and train the model for 40k iterations with a batch size of 64. | âœ… å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨è¾“å…¥å¤§å° 512  $\times$  512 å¹¶ä»¥æ‰¹é‡å¤§å° 64 å¯¹æ¨¡å‹è¿›è¡Œ 40k æ¬¡è¿­ä»£è®­ç»ƒã€‚ |
| âœ… We adopt the AdamW ( **Decoupled weight decay regularization, 2019.** ) optimizer with the optimal learning rate searched from {8e-4,4e-4,2e-4,1e-4}. | âœ… æˆ‘ä»¬é‡‡ç”¨ AdamW ( **Decoupled weight decay regularization, 2019.** ) ä¼˜åŒ–å™¨ï¼Œæœ€ä½³å­¦ä¹ ç‡ä» {8e-4,4e-4,2e-4,1e-4} ä¸­æœç´¢ã€‚ |

| ã€ç¬¬6.4.2èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬6.4.2èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Our results show a similar trend to the object detection experiments. | âœ… æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºå‡ºä¸ç‰©ä½“æ£€æµ‹å®éªŒç›¸ä¼¼çš„è¶‹åŠ¿ã€‚ |
| âœ… As illustrated in TableÂ 9 , our base model outperforms the previous SoTA model, which is BEiT pre-trained ViT-B ( **BEiT: BERT pre-training of image transformers.** ) , by 1.3 and 1.4 points in single-scale and multi-scale testing protocol, respectively. | âœ… å¦‚ TableÂ 9 æ‰€ç¤ºï¼Œæˆ‘ä»¬çš„åŸºç¡€æ¨¡å‹åœ¨å•å°ºåº¦å’Œå¤šå°ºåº¦æµ‹è¯•åè®®ä¸­åˆ†åˆ«æ¯”ä¹‹å‰çš„ SoTA æ¨¡å‹ï¼ˆå³ BEiT é¢„è®­ç»ƒçš„ ViT-B ( **BEiT: BERT pre-training of image transformers.** )ï¼‰é«˜å‡º 1.3 å’Œ 1.4 ä¸ªç‚¹ã€‚ |
| âœ… With the same backbone architecture of DaViT-B ( **Davit: Dual attention vision transformers.** ) , Florence-2 pre-trained model achieves a remarkable improvement of 4.9 points and 4  $\times$  efficiency compared to the ImageNet-1k pre-trained counterpart as demonstrated in TablesÂ 8 and 6 . | âœ… ä½¿ç”¨ä¸ DaViT-B ( **Davit: Dual attention vision transformers.** ) ç›¸åŒçš„ä¸»å¹²æ¶æ„ï¼ŒFlorence-2 é¢„è®­ç»ƒæ¨¡å‹ä¸ ImageNet-1k é¢„è®­ç»ƒæ¨¡å‹ç›¸æ¯”ï¼Œå®ç°äº† 4.9 åˆ†å’Œ 4  $\times$  æ•ˆç‡çš„æ˜¾è‘—æå‡ï¼Œå¦‚ TablesÂ 8 å’Œ 6 æ‰€ç¤ºã€‚ |

<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T9.6"><thead class="ltx_thead"><tr class="ltx_tr" id="S6.T9.6.1.1"><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T9.6.1.1.1" style="padding-left:5.5pt;padding-right:5.5pt;">Backbone</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T9.6.1.1.2" style="padding-left:5.5pt;padding-right:5.5pt;">Pretrain</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T9.6.1.1.3" style="padding-left:5.5pt;padding-right:5.5pt;">mIoU</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T9.6.1.1.4" style="padding-left:5.5pt;padding-right:5.5pt;">ms-mIoU</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr" id="S6.T9.6.2.1"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.6.2.1.1" style="padding-left:5.5pt;padding-right:5.5pt;">ViT-BÂ <html><body><p>( <strong>Masked autoencoders are scalable vision learners.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.6.2.1.2" style="padding-left:5.5pt;padding-right:5.5pt;">Sup IN-1k</th><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T9.6.2.1.3" style="padding-left:5.5pt;padding-right:5.5pt;">47.4</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T9.6.2.1.4" style="padding-left:5.5pt;padding-right:5.5pt;">-</td></tr><tr class="ltx_tr" id="S6.T9.6.3.2"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.3.2.1" style="padding-left:5.5pt;padding-right:5.5pt;">ViT-BÂ <html><body><p>( <strong>Masked autoencoders are scalable vision learners.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.3.2.2" style="padding-left:5.5pt;padding-right:5.5pt;">MAE IN-1k</th><td class="ltx_td ltx_align_center" id="S6.T9.6.3.2.3" style="padding-left:5.5pt;padding-right:5.5pt;">48.1</td><td class="ltx_td ltx_align_center" id="S6.T9.6.3.2.4" style="padding-left:5.5pt;padding-right:5.5pt;">-</td></tr><tr class="ltx_tr" id="S6.T9.6.4.3"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.4.3.1" style="padding-left:5.5pt;padding-right:5.5pt;">ViT-BÂ <html><body><p>( <strong>BEiT: BERT pre-training of image transformers.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.4.3.2" style="padding-left:5.5pt;padding-right:5.5pt;">BEiT</th><td class="ltx_td ltx_align_center" id="S6.T9.6.4.3.3" style="padding-left:5.5pt;padding-right:5.5pt;">53.6</td><td class="ltx_td ltx_align_center" id="S6.T9.6.4.3.4" style="padding-left:5.5pt;padding-right:5.5pt;">54.1</td></tr><tr class="ltx_tr" id="S6.T9.6.5.4"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.5.4.1" style="padding-left:5.5pt;padding-right:5.5pt;">ViT-BÂ <html><body><p>( <strong>BEiT v2: Masked image modeling with vector-quantized visualtokenizers.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.5.4.2" style="padding-left:5.5pt;padding-right:5.5pt;">BEiTv2 IN-1k</th><td class="ltx_td ltx_align_center" id="S6.T9.6.5.4.3" style="padding-left:5.5pt;padding-right:5.5pt;">53.1</td><td class="ltx_td ltx_align_center" id="S6.T9.6.5.4.4" style="padding-left:5.5pt;padding-right:5.5pt;">-</td></tr><tr class="ltx_tr" id="S6.T9.6.6.5"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.6.5.1" style="padding-left:5.5pt;padding-right:5.5pt;">ViT-BÂ <html><body><p>( <strong>BEiT v2: Masked image modeling with vector-quantized visualtokenizers.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.6.5.2" style="padding-left:5.5pt;padding-right:5.5pt;">BEiTv2 IN-22k</th><td class="ltx_td ltx_align_center" id="S6.T9.6.6.5.3" style="padding-left:5.5pt;padding-right:5.5pt;">53.5</td><td class="ltx_td ltx_align_center" id="S6.T9.6.6.5.4" style="padding-left:5.5pt;padding-right:5.5pt;">-</td></tr><tr class="ltx_tr" id="S6.T9.6.7.6"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.7.6.1" style="padding-left:5.5pt;padding-right:5.5pt;">Swin-B <html><body><p>( <strong>Swin transformer: Hierarchical vision transformer using shiftedwindows, 2021.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.7.6.2" style="padding-left:5.5pt;padding-right:5.5pt;">Sup IN-1k</th><td class="ltx_td ltx_align_center" id="S6.T9.6.7.6.3" style="padding-left:5.5pt;padding-right:5.5pt;">48.1</td><td class="ltx_td ltx_align_center" id="S6.T9.6.7.6.4" style="padding-left:5.5pt;padding-right:5.5pt;">49.7</td></tr><tr class="ltx_tr" id="S6.T9.6.8.7"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.8.7.1" style="padding-left:5.5pt;padding-right:5.5pt;">Swin-B <html><body><p>( <strong>Swin transformer: Hierarchical vision transformer using shiftedwindows, 2021.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.8.7.2" style="padding-left:5.5pt;padding-right:5.5pt;">Sup IN-22k</th><td class="ltx_td ltx_align_center" id="S6.T9.6.8.7.3" style="padding-left:5.5pt;padding-right:5.5pt;">-</td><td class="ltx_td ltx_align_center" id="S6.T9.6.8.7.4" style="padding-left:5.5pt;padding-right:5.5pt;">51.8</td></tr><tr class="ltx_tr" id="S6.T9.6.9.8"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.9.8.1" style="padding-left:5.5pt;padding-right:5.5pt;">Swin-B <html><body><p>( <strong>Swin transformer: Hierarchical vision transformer using shiftedwindows, 2021.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.9.8.2" style="padding-left:5.5pt;padding-right:5.5pt;">SimMIMÂ <html><body><p>( <strong>Simmim: A simple framework for masked image modeling.</strong> )</p></body></html></th><td class="ltx_td ltx_align_center" id="S6.T9.6.9.8.3" style="padding-left:5.5pt;padding-right:5.5pt;">-</td><td class="ltx_td ltx_align_center" id="S6.T9.6.9.8.4" style="padding-left:5.5pt;padding-right:5.5pt;">52.8</td></tr><tr class="ltx_tr" id="S6.T9.6.10.9"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.10.9.1" style="padding-left:5.5pt;padding-right:5.5pt;">FocalAtt-BÂ <html><body><p>( <strong>Focal self-attention for local-global interactions in visiontransformers.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.10.9.2" style="padding-left:5.5pt;padding-right:5.5pt;">Sup IN-1k</th><td class="ltx_td ltx_align_center" id="S6.T9.6.10.9.3" style="padding-left:5.5pt;padding-right:5.5pt;">49.0</td><td class="ltx_td ltx_align_center" id="S6.T9.6.10.9.4" style="padding-left:5.5pt;padding-right:5.5pt;">50.5</td></tr><tr class="ltx_tr" id="S6.T9.6.11.10"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.11.10.1" style="padding-left:5.5pt;padding-right:5.5pt;">FocalNet-BÂ <html><body><p>( <strong>Focal modulation networks.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.11.10.2" style="padding-left:5.5pt;padding-right:5.5pt;">Sup IN-1k</th><td class="ltx_td ltx_align_center" id="S6.T9.6.11.10.3" style="padding-left:5.5pt;padding-right:5.5pt;">50.5</td><td class="ltx_td ltx_align_center" id="S6.T9.6.11.10.4" style="padding-left:5.5pt;padding-right:5.5pt;">51.4</td></tr><tr class="ltx_tr" id="S6.T9.6.12.11"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.12.11.1" style="padding-left:5.5pt;padding-right:5.5pt;">ConvNeXt v1-BÂ <html><body><p>( <strong>A convnet for the 2020s.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.12.11.2" style="padding-left:5.5pt;padding-right:5.5pt;">Sup IN-1k</th><td class="ltx_td ltx_align_center" id="S6.T9.6.12.11.3" style="padding-left:5.5pt;padding-right:5.5pt;">-</td><td class="ltx_td ltx_align_center" id="S6.T9.6.12.11.4" style="padding-left:5.5pt;padding-right:5.5pt;">49.9</td></tr><tr class="ltx_tr" id="S6.T9.6.13.12"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.13.12.1" style="padding-left:5.5pt;padding-right:5.5pt;">ConvNeXt v2-BÂ <html><body><p>( <strong>Convnext v2: Co-designing and scaling convnets with maskedautoencoders.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.13.12.2" style="padding-left:5.5pt;padding-right:5.5pt;">Sup IN-1k</th><td class="ltx_td ltx_align_center" id="S6.T9.6.13.12.3" style="padding-left:5.5pt;padding-right:5.5pt;">-</td><td class="ltx_td ltx_align_center" id="S6.T9.6.13.12.4" style="padding-left:5.5pt;padding-right:5.5pt;">50.5</td></tr><tr class="ltx_tr" id="S6.T9.6.14.13"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.14.13.1" style="padding-left:5.5pt;padding-right:5.5pt;">ConvNeXt v2-BÂ <html><body><p>( <strong>Convnext v2: Co-designing and scaling convnets with maskedautoencoders.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T9.6.14.13.2" style="padding-left:5.5pt;padding-right:5.5pt;">FCMAE</th><td class="ltx_td ltx_align_center" id="S6.T9.6.14.13.3" style="padding-left:5.5pt;padding-right:5.5pt;">-</td><td class="ltx_td ltx_align_center" id="S6.T9.6.14.13.4" style="padding-left:5.5pt;padding-right:5.5pt;">52.1</td></tr><tr class="ltx_tr" id="S6.T9.6.15.14" style="background-color:#E6E6E6;"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T9.6.15.14.1" style="padding-left:5.5pt;padding-right:5.5pt;">DaViT-BÂ <html><body><p>( <strong>Davit: Dual attention vision transformers.</strong> )</p></body></html></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T9.6.15.14.2" style="padding-left:5.5pt;padding-right:5.5pt;"><em class="ltx_emph ltx_font_italic" id="S6.T9.6.15.14.2.1" style="font-size:90%;background-color:#E6E6E6;">Florence-2</em></th><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T9.6.15.14.3" style="padding-left:5.5pt;padding-right:5.5pt;">54.9</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T9.6.15.14.4" style="padding-left:5.5pt;padding-right:5.5pt;">55.5</td></tr></tbody></table>

| ã€è¡¨æ ‡é¢˜ã€‘åŸæ–‡ | ã€è¡¨æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Table 9:  ADE20K semantic segmentation results using UperNet. | âœ… Table 9:  ADE20K semantic segmentation results ä½¿ç”¨ UperNetã€‚ |
| âœ… The input size is  $512\times 512$  for all the entries, except for models with BEiT pre-trained, which use the input size of  $640\times 640$  . | âœ… æ‰€æœ‰æ¡ç›®çš„è¾“å…¥å¤§å°å‡ä¸º  $512\times 512$ ï¼Œä½†ç»è¿‡ BEiT é¢„è®­ç»ƒçš„æ¨¡å‹é™¤å¤–ï¼Œè¿™äº›æ¨¡å‹ä½¿ç”¨  $640\times 640$  çš„è¾“å…¥å¤§å°ã€‚ |

### 6.5 Ablation Studies

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/x12.png)

| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 7:  Multitask transfer. We conduct experiments with three different versions of Florence-2 models, each trained on a different level of image annotation: image level, image and region level, and image, region, and pixel level. We then evaluate the transfer learning performance of these models on four downstream tasks: COCO caption, COCO object detection, Flickr30k grounding, and Refcoco referring segmentation. | âœ… Figure 7:  Multitask transfer. We conduct experiments with three different versions of Florence-2 models, each trained on a different level of image annotation: image level, image and region level, and image, region, and pixel level. We then evaluate the transfer learning performance of these models on four downstream tasks: COCO caption, COCO object detection, Flickr30k grounding, and Refcoco referring segmentation. |

#### 6.5.1 Multitask transfer.

| ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… In this study, we aimed to identify the most effective pre-trained model for transfer learning across various downstream tasks in computer vision. | âœ… åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°æœ€æœ‰æ•ˆçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨äºè®¡ç®—æœºè§†è§‰ä¸­å„ä¸ªä¸‹æ¸¸ä»»åŠ¡çš„è¿ç§»å­¦ä¹ ã€‚ |
| âœ… We compared three different models, each pre-trained on a different combination of tasks: | âœ… æˆ‘ä»¬æ¯”è¾ƒäº†ä¸‰ç§ä¸åŒçš„æ¨¡å‹ï¼Œæ¯ç§æ¨¡å‹éƒ½é’ˆå¯¹ä¸åŒçš„ä»»åŠ¡ç»„åˆè¿›è¡Œäº†é¢„è®­ç»ƒï¼š |

| ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Image-level Model: pre-trained on image-level tasks only | âœ… å›¾åƒçº§æ¨¡å‹ï¼šä»…åœ¨å›¾åƒçº§ä»»åŠ¡ä¸Šè¿›è¡Œé¢„è®­ç»ƒ |

| ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬3æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬3æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Image-Region Model: pre-trained on image-level and region-level tasks | âœ… å›¾åƒåŒºåŸŸæ¨¡å‹ï¼šé’ˆå¯¹å›¾åƒçº§å’ŒåŒºåŸŸçº§ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒ |

| ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬4æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬4æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Image-Region-Pixel Model: pre-trained on image-level, region-level, and pixel-level tasks | âœ… å›¾åƒ-åŒºåŸŸ-åƒç´ æ¨¡å‹ï¼šé’ˆå¯¹å›¾åƒçº§ã€åŒºåŸŸçº§å’Œåƒç´ çº§ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒ |

| ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬5æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬5æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… For pre-training, we optimize all models for the same number of effective samples (72M) on a subset of our FLD-5B dataset. | âœ… å¯¹äºé¢„è®­ç»ƒï¼Œæˆ‘ä»¬åœ¨ FLD-5B æ•°æ®é›†çš„å­é›†ä¸Šé’ˆå¯¹ç›¸åŒæ•°é‡çš„æœ‰æ•ˆæ ·æœ¬ï¼ˆ72Mï¼‰ä¼˜åŒ–æ‰€æœ‰æ¨¡å‹ã€‚ |

| ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬6æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬6æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… These models are then transferred to a combined dataset with four downstream tasks, each representing a different level of task granularity: COCO caption (image-level task), COCO object detection (region-level task), Flickr30k grounding (region-level task), RefCOCO referring segmentation (pixel-level task). | âœ… ç„¶åå°†è¿™äº›æ¨¡å‹è½¬ç§»åˆ°å…·æœ‰å››ä¸ªä¸‹æ¸¸ä»»åŠ¡çš„ç»„åˆæ•°æ®é›†ï¼Œæ¯ä¸ªä»»åŠ¡ä»£è¡¨ä¸åŒçº§åˆ«çš„ä»»åŠ¡ç²’åº¦ï¼šCOCO æ ‡é¢˜ï¼ˆå›¾åƒçº§ä»»åŠ¡ï¼‰ã€COCO å¯¹è±¡æ£€æµ‹ï¼ˆåŒºåŸŸçº§ä»»åŠ¡ï¼‰ã€Flickr30k æ¥åœ°ï¼ˆåŒºåŸŸçº§ä»»åŠ¡ï¼‰ã€RefCOCO å‚è€ƒåˆ†å‰²ï¼ˆåƒç´ çº§ä»»åŠ¡ï¼‰ã€‚ |

| ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬7æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬7æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… The results are shown in FigureÂ 7. | âœ… ç»“æœæ˜¾ç¤ºåœ¨FigureÂ 7ä¸­ã€‚ |
| âœ… The results demonstrate that Image-Region-Pixel Model, pre-trained on all three levels of tasks, consistently demonstrated competitive performance across the four downstream tasks. | âœ… ç»“æœè¡¨æ˜ï¼Œåœ¨æ‰€æœ‰ä¸‰ä¸ªçº§åˆ«çš„ä»»åŠ¡ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„å›¾åƒåŒºåŸŸåƒç´ æ¨¡å‹åœ¨å››ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å§‹ç»ˆè¡¨ç°å‡ºæœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ |

| ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬8æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬8æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… For the COCO caption task, Image-Region-Pixel Model initially performs worse than Image-level Model and Image-Region Model but eventually achieve a final performance (133.4 CIDEr) that is only slightly worse than the other models (134.6 CIDEr). | âœ… å¯¹äº COCO å­—å¹•ä»»åŠ¡ï¼Œå›¾åƒåŒºåŸŸåƒç´ æ¨¡å‹æœ€åˆçš„è¡¨ç°æ¯”å›¾åƒçº§æ¨¡å‹å’Œå›¾åƒåŒºåŸŸæ¨¡å‹å·®ï¼Œä½†æœ€ç»ˆå®ç°äº†æœ€ç»ˆæ€§èƒ½ï¼ˆ133.4 CIDErï¼‰ï¼Œä»…æ¯”å…¶ä»–æ¨¡å‹ï¼ˆ134.6 CIDErï¼‰ç¨å·®ã€‚ |

| ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬9æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬9æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… For the COCO object detection task, Image-Region-Pixel Model outperforms Image-level Model by a significant margin (28.3 vs. | âœ… å¯¹äº COCO å¯¹è±¡æ£€æµ‹ä»»åŠ¡ï¼Œå›¾åƒåŒºåŸŸåƒç´ æ¨¡å‹çš„è¡¨ç°æ˜æ˜¾ä¼˜äºå›¾åƒçº§æ¨¡å‹ï¼ˆ28.3 vs. |
| âœ… 0.1) and was only slightly worse than Image-Region Model (29.7). | âœ… 0.1)ï¼Œä»…æ¯”å›¾åƒåŒºåŸŸæ¨¡å‹ (29.7) ç¨å·®ã€‚ |

| ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬10æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬10æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… For the Flickr30k grounding task, Image-Region-Pixel Model shows strong performance (78.1 recall@1), comparable to Image-Region Model (79.1 recall@1) and significantly better than Image-level Model (62.0 recall@1). | âœ… å¯¹äº Flickr30k åŸºç¡€ä»»åŠ¡ï¼Œå›¾åƒåŒºåŸŸåƒç´ æ¨¡å‹è¡¨ç°å‡ºè‰²ï¼ˆ78.1 å¬å›ç‡@1ï¼‰ï¼Œä¸å›¾åƒåŒºåŸŸæ¨¡å‹ï¼ˆ79.1 å¬å›ç‡@1ï¼‰ç›¸å½“ï¼Œå¹¶ä¸”æ˜æ˜¾ä¼˜äºå›¾åƒçº§æ¨¡å‹ï¼ˆ62.0 å¬å›ç‡@1ï¼‰ã€‚ |

| ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬11æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬11æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… For the RefCOCO referring segmentation task, Image-Region-Pixel Model clearly outperforms both Image-level Model and Image-Region Model, achieving the highest performance (31.6 mIoU) compared to the other models (28.4 and 18.2 mIoU). | âœ… å¯¹äº RefCOCO å‚ç…§åˆ†å‰²ä»»åŠ¡ï¼Œå›¾åƒåŒºåŸŸåƒç´ æ¨¡å‹æ˜æ˜¾ä¼˜äºå›¾åƒçº§æ¨¡å‹å’Œå›¾åƒåŒºåŸŸæ¨¡å‹ï¼Œä¸å…¶ä»–æ¨¡å‹ï¼ˆ28.4 å’Œ 18.2 mIoUï¼‰ç›¸æ¯”å®ç°äº†æœ€é«˜æ€§èƒ½ï¼ˆ31.6 mIoUï¼‰ã€‚ |

| ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬12æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.1èŠ‚ï¼Œç¬¬12æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Our findings suggest that the Image-Region-Pixel Model, which is pre-trained on tasks at the image, region, and pixel levels, is the most effective base model for transfer learning across various computer vision tasks. | âœ… æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨å›¾åƒã€åŒºåŸŸå’Œåƒç´ çº§åˆ«çš„ä»»åŠ¡ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„å›¾åƒåŒºåŸŸåƒç´ æ¨¡å‹æ˜¯è·¨å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡è¿›è¡Œè¿ç§»å­¦ä¹ çš„æœ€æœ‰æ•ˆçš„åŸºç¡€æ¨¡å‹ã€‚ |
| âœ… This model shows strong performance on all four downstream tasks we evaluated, and consistently outperforms the Image-level Model and matches or exceeds the Image-Region Model in performance. | âœ… è¯¥æ¨¡å‹åœ¨æˆ‘ä»¬è¯„ä¼°çš„æ‰€æœ‰å››ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºäº†å¼ºåŠ²çš„æ€§èƒ½ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºå›¾åƒçº§æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡äº†å›¾åƒåŒºåŸŸæ¨¡å‹ã€‚ |
| âœ… By pre-training a model on tasks at different levels of granularity, we can ensure that the base model is better prepared to handle a diverse range of downstream tasks, offering a versatile and robust solution for transfer learning in computer vision. | âœ… é€šè¿‡å¯¹ä¸åŒç²’åº¦çº§åˆ«çš„ä»»åŠ¡è¿›è¡Œæ¨¡å‹é¢„è®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿åŸºç¡€æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œä¸ºè®¡ç®—æœºè§†è§‰ä¸­çš„è¿ç§»å­¦ä¹ æä¾›å¤šåŠŸèƒ½ä¸”å¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚ |

#### 6.5.2 Model scaling.

| ã€ç¬¬6.5.2èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.2èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We aimed to investigate the impact of increasing model capacity on zero-shot performance on various downstream tasks in computer vision. | âœ… æˆ‘ä»¬çš„ç›®çš„æ˜¯ç ”ç©¶å¢åŠ æ¨¡å‹å®¹é‡å¯¹è®¡ç®—æœºè§†è§‰ä¸­å„ç§ä¸‹æ¸¸ä»»åŠ¡çš„é›¶æ ·æœ¬æ€§èƒ½çš„å½±å“ã€‚ |
| âœ… We compared two models: Florence-2-B and Florence-2-L , which have 232M and 771M parameters, respectively. | âœ… æˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ä¸ªæ¨¡å‹ï¼šFlorence-2-B å’Œ Florence-2-Lï¼Œå®ƒä»¬åˆ†åˆ«æœ‰ 232M å’Œ 771M ä¸ªå‚æ•°ã€‚ |
| âœ… The model architectures are described in TableÂ 15. | âœ… æ¨¡å‹æ¶æ„åœ¨TableÂ 15ä¸­æè¿°ã€‚ |
| âœ… We show the zero-shot performance on four downstream tasks in TableÂ 10. | âœ… æˆ‘ä»¬å±•ç¤ºäº† TableÂ 10 ä¸­å››ä¸ªä¸‹æ¸¸ä»»åŠ¡çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ |
| âœ… The large model clearly outperforms the base model across various downstream tasks. | âœ… å¤§å‹æ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜æ˜¾ä¼˜äºåŸºç¡€æ¨¡å‹ã€‚ |

<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T10.2"><tbody class="ltx_tbody"><tr class="ltx_tr" id="S6.T10.2.1.1"><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T10.2.1.1.1" rowspan="2" style="padding-left:3.3pt;padding-right:3.3pt;">Model</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T10.2.1.1.2" style="padding-left:3.3pt;padding-right:3.3pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T10.2.1.1.3" style="padding-left:3.3pt;padding-right:3.3pt;">Caption</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T10.2.1.1.4" style="padding-left:3.3pt;padding-right:3.3pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T10.2.1.1.5" style="padding-left:3.3pt;padding-right:3.3pt;">Detection</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T10.2.1.1.6" style="padding-left:3.3pt;padding-right:3.3pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T10.2.1.1.7" style="padding-left:3.3pt;padding-right:3.3pt;">Grounding</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T10.2.1.1.8" style="padding-left:3.3pt;padding-right:3.3pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S6.T10.2.1.1.9" style="padding-left:3.3pt;padding-right:3.3pt;">RES</th></tr><tr class="ltx_tr" id="S6.T10.2.2.2"><td class="ltx_td" id="S6.T10.2.2.2.1" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.2.2.2.2" style="padding-left:3.3pt;padding-right:3.3pt;">CIDEr</td><td class="ltx_td" id="S6.T10.2.2.2.3" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.2.2.2.4" style="padding-left:3.3pt;padding-right:3.3pt;">AP</td><td class="ltx_td" id="S6.T10.2.2.2.5" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.2.2.2.6" style="padding-left:3.3pt;padding-right:3.3pt;">Recall@1</td><td class="ltx_td" id="S6.T10.2.2.2.7" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.2.2.2.8" style="padding-left:3.3pt;padding-right:3.3pt;">mIOU</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.2.2.2.9" style="padding-left:3.3pt;padding-right:3.3pt;">oIOU</td></tr><tr class="ltx_tr" id="S6.T10.2.3.3"><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.2.3.3.1" style="padding-left:3.3pt;padding-right:3.3pt;">Base</th><th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S6.T10.2.3.3.2" style="padding-left:3.3pt;padding-right:3.3pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T10.2.3.3.3" style="padding-left:3.3pt;padding-right:3.3pt;">118.7</th><th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S6.T10.2.3.3.4" style="padding-left:3.3pt;padding-right:3.3pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T10.2.3.3.5" style="padding-left:3.3pt;padding-right:3.3pt;">19.7</th><th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S6.T10.2.3.3.6" style="padding-left:3.3pt;padding-right:3.3pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T10.2.3.3.7" style="padding-left:3.3pt;padding-right:3.3pt;">76.3</th><th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S6.T10.2.3.3.8" style="padding-left:3.3pt;padding-right:3.3pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T10.2.3.3.9" style="padding-left:3.3pt;padding-right:3.3pt;">18.6</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T10.2.3.3.10" style="padding-left:3.3pt;padding-right:3.3pt;">17.8</th></tr><tr class="ltx_tr" id="S6.T10.2.4.4"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T10.2.4.4.1" style="padding-left:3.3pt;padding-right:3.3pt;">Large</th><td class="ltx_td ltx_border_bb" id="S6.T10.2.4.4.2" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T10.2.4.4.3" style="padding-left:3.3pt;padding-right:3.3pt;">124.4</td><td class="ltx_td ltx_border_bb" id="S6.T10.2.4.4.4" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T10.2.4.4.5" style="padding-left:3.3pt;padding-right:3.3pt;">22.6</td><td class="ltx_td ltx_border_bb" id="S6.T10.2.4.4.6" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T10.2.4.4.7" style="padding-left:3.3pt;padding-right:3.3pt;">78.2</td><td class="ltx_td ltx_border_bb" id="S6.T10.2.4.4.8" style="padding-left:3.3pt;padding-right:3.3pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T10.2.4.4.9" style="padding-left:3.3pt;padding-right:3.3pt;">21.5</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T10.2.4.4.10" style="padding-left:3.3pt;padding-right:3.3pt;">19.1</td></tr></tbody></table>

| ã€è¡¨æ ‡é¢˜ã€‘åŸæ–‡ | ã€è¡¨æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Table 10:  Model scaling. Zero-shot performance on COCO caption and COCO object detection, Flickr30k grounding, RefCOCO referring expression segmentation(RES). | âœ… Table 10:  Model scaling. åœ¨ COCO æ ‡é¢˜å’Œ COCO å¯¹è±¡æ£€æµ‹ä¸Šçš„é›¶æ ·æœ¬æ€§èƒ½ï¼ŒFlickr30k åŸºç¡€ï¼ŒRefCOCO å‚è€ƒè¡¨æƒ…åˆ†å‰²ï¼ˆRESï¼‰ã€‚ |

#### 6.5.3 Data scaling.

| ã€ç¬¬6.5.3èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.3èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We conducted experiments to study how zero-shot performance on various computer vision tasks is affected by the scale of pre-training data. | âœ… æˆ‘ä»¬è¿›è¡Œäº†å®éªŒï¼Œç ”ç©¶é¢„è®­ç»ƒæ•°æ®è§„æ¨¡å¦‚ä½•å½±å“å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ |
| âœ… We used four different data sizes for pre-training: 0.12M, 0.36M, 1.2M, and 12M images. | âœ… æˆ‘ä»¬ä½¿ç”¨å››ç§ä¸åŒçš„æ•°æ®å¤§å°è¿›è¡Œé¢„è®­ç»ƒï¼š0.12Mã€0.36Mã€1.2M å’Œ 12M å›¾åƒã€‚ |
| âœ… All models were trained with the same effective sample size (72M) on a subset of FLD-5B data. | âœ… æ‰€æœ‰æ¨¡å‹å‡åœ¨ FLD-5B æ•°æ®å­é›†ä¸Šä½¿ç”¨ç›¸åŒçš„æœ‰æ•ˆæ ·æœ¬é‡ï¼ˆ72Mï¼‰è¿›è¡Œè®­ç»ƒã€‚ |

<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T11.2"><thead class="ltx_thead"><tr class="ltx_tr" id="S6.T11.2.1.1"><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T11.2.1.1.1" style="padding-left:3.1pt;padding-right:3.1pt;">Data</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T11.2.1.1.2" style="padding-left:3.1pt;padding-right:3.1pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T11.2.1.1.3" style="padding-left:3.1pt;padding-right:3.1pt;">Caption</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T11.2.1.1.4" style="padding-left:3.1pt;padding-right:3.1pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T11.2.1.1.5" style="padding-left:3.1pt;padding-right:3.1pt;">Detection</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T11.2.1.1.6" style="padding-left:3.1pt;padding-right:3.1pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T11.2.1.1.7" style="padding-left:3.1pt;padding-right:3.1pt;">Grounding</th><th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T11.2.1.1.8" style="padding-left:3.1pt;padding-right:3.1pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S6.T11.2.1.1.9" style="padding-left:3.1pt;padding-right:3.1pt;">RES</th></tr><tr class="ltx_tr" id="S6.T11.2.2.2"><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S6.T11.2.2.2.1" style="padding-left:3.1pt;padding-right:3.1pt;">size</th><th class="ltx_td ltx_th ltx_th_column" id="S6.T11.2.2.2.2" style="padding-left:3.1pt;padding-right:3.1pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T11.2.2.2.3" style="padding-left:3.1pt;padding-right:3.1pt;">CIDEr</th><th class="ltx_td ltx_th ltx_th_column" id="S6.T11.2.2.2.4" style="padding-left:3.1pt;padding-right:3.1pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T11.2.2.2.5" style="padding-left:3.1pt;padding-right:3.1pt;">AP</th><th class="ltx_td ltx_th ltx_th_column" id="S6.T11.2.2.2.6" style="padding-left:3.1pt;padding-right:3.1pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T11.2.2.2.7" style="padding-left:3.1pt;padding-right:3.1pt;">Recall@1</th><th class="ltx_td ltx_th ltx_th_column" id="S6.T11.2.2.2.8" style="padding-left:3.1pt;padding-right:3.1pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T11.2.2.2.9" style="padding-left:3.1pt;padding-right:3.1pt;">mIOU</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T11.2.2.2.10" style="padding-left:3.1pt;padding-right:3.1pt;">oIOU</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr" id="S6.T11.2.3.1"><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T11.2.3.1.1" style="padding-left:3.1pt;padding-right:3.1pt;">0.12M</th><td class="ltx_td ltx_border_t" id="S6.T11.2.3.1.2" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T11.2.3.1.3" style="padding-left:3.1pt;padding-right:3.1pt;">102.8</td><td class="ltx_td ltx_border_t" id="S6.T11.2.3.1.4" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T11.2.3.1.5" style="padding-left:3.1pt;padding-right:3.1pt;">16.1</td><td class="ltx_td ltx_border_t" id="S6.T11.2.3.1.6" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T11.2.3.1.7" style="padding-left:3.1pt;padding-right:3.1pt;">74.0</td><td class="ltx_td ltx_border_t" id="S6.T11.2.3.1.8" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T11.2.3.1.9" style="padding-left:3.1pt;padding-right:3.1pt;">15.9</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T11.2.3.1.10" style="padding-left:3.1pt;padding-right:3.1pt;">16.6</td></tr><tr class="ltx_tr" id="S6.T11.2.4.2"><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T11.2.4.2.1" style="padding-left:3.1pt;padding-right:3.1pt;">0.36M</th><td class="ltx_td" id="S6.T11.2.4.2.2" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center" id="S6.T11.2.4.2.3" style="padding-left:3.1pt;padding-right:3.1pt;">114.3</td><td class="ltx_td" id="S6.T11.2.4.2.4" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center" id="S6.T11.2.4.2.5" style="padding-left:3.1pt;padding-right:3.1pt;">18.7</td><td class="ltx_td" id="S6.T11.2.4.2.6" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center" id="S6.T11.2.4.2.7" style="padding-left:3.1pt;padding-right:3.1pt;">75.8</td><td class="ltx_td" id="S6.T11.2.4.2.8" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center" id="S6.T11.2.4.2.9" style="padding-left:3.1pt;padding-right:3.1pt;">16.6</td><td class="ltx_td ltx_align_center" id="S6.T11.2.4.2.10" style="padding-left:3.1pt;padding-right:3.1pt;">16.4</td></tr><tr class="ltx_tr" id="S6.T11.2.5.3"><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T11.2.5.3.1" style="padding-left:3.1pt;padding-right:3.1pt;">1.2M</th><td class="ltx_td" id="S6.T11.2.5.3.2" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center" id="S6.T11.2.5.3.3" style="padding-left:3.1pt;padding-right:3.1pt;">118.1</td><td class="ltx_td" id="S6.T11.2.5.3.4" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center" id="S6.T11.2.5.3.5" style="padding-left:3.1pt;padding-right:3.1pt;">18.9</td><td class="ltx_td" id="S6.T11.2.5.3.6" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center" id="S6.T11.2.5.3.7" style="padding-left:3.1pt;padding-right:3.1pt;">76.3</td><td class="ltx_td" id="S6.T11.2.5.3.8" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center" id="S6.T11.2.5.3.9" style="padding-left:3.1pt;padding-right:3.1pt;">19.3</td><td class="ltx_td ltx_align_center" id="S6.T11.2.5.3.10" style="padding-left:3.1pt;padding-right:3.1pt;">18.4</td></tr><tr class="ltx_tr" id="S6.T11.2.6.4"><th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T11.2.6.4.1" style="padding-left:3.1pt;padding-right:3.1pt;">12M</th><td class="ltx_td ltx_border_bb" id="S6.T11.2.6.4.2" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T11.2.6.4.3" style="padding-left:3.1pt;padding-right:3.1pt;">118.7</td><td class="ltx_td ltx_border_bb" id="S6.T11.2.6.4.4" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T11.2.6.4.5" style="padding-left:3.1pt;padding-right:3.1pt;">19.7</td><td class="ltx_td ltx_border_bb" id="S6.T11.2.6.4.6" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T11.2.6.4.7" style="padding-left:3.1pt;padding-right:3.1pt;">76.3</td><td class="ltx_td ltx_border_bb" id="S6.T11.2.6.4.8" style="padding-left:3.1pt;padding-right:3.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T11.2.6.4.9" style="padding-left:3.1pt;padding-right:3.1pt;">18.6</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T11.2.6.4.10" style="padding-left:3.1pt;padding-right:3.1pt;">17.8</td></tr></tbody></table>

| ã€è¡¨æ ‡é¢˜ã€‘åŸæ–‡ | ã€è¡¨æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Table 11:  Data scaling. Zero-shot performance on COCO caption, COCO object detection, Flickr30k grounding, COCORef referring segmentation. | âœ… Table 11:  Data scaling. åœ¨ COCO æ ‡é¢˜ã€COCO å¯¹è±¡æ£€æµ‹ã€Flickr30k åŸºç¡€ã€COCORef å¼•ç”¨åˆ†å‰²ä¸Šçš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ |

| ã€ç¬¬6.5.3èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.3èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… TableÂ 11 presents the zero-shot performance results on COCO caption, COCO object detection, Flickr30k grounding, and RefCoco referring segmentation (RES) tasks. | âœ… TableÂ 11 å±•ç¤ºäº† COCO æ ‡é¢˜ã€COCO å¯¹è±¡æ£€æµ‹ã€Flickr30k åŸºç¡€å’Œ RefCoco å¼•ç”¨åˆ†å‰² (RES) ä»»åŠ¡çš„é›¶æ ·æœ¬æ€§èƒ½ç»“æœã€‚ |
| âœ… We can observe a trend of improved zero-shot performance on the downstream tasks as the pre-training data size increases (except for RES, 1.2M data has slightly better performance compared to 12M). | âœ… æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œéšç€é¢„è®­ç»ƒæ•°æ®å¤§å°çš„å¢åŠ ï¼Œä¸‹æ¸¸ä»»åŠ¡çš„é›¶æ ·æœ¬æ€§èƒ½å‘ˆç°æé«˜çš„è¶‹åŠ¿ï¼ˆRES é™¤å¤–ï¼Œ1.2M æ•°æ®çš„æ€§èƒ½ç•¥ä¼˜äº 12Mï¼‰ã€‚ |

| ã€ç¬¬6.5.3èŠ‚ï¼Œç¬¬3æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.3èŠ‚ï¼Œç¬¬3æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Our experiments on data scaling demonstrate that larger pre-training data sizes generally lead to improved zero-shot performance across a variety of downstream tasks in computer vision. | âœ… æˆ‘ä»¬è¿›è¡Œæ•°æ®æ‰©å±•çš„å®éªŒè¡¨æ˜ï¼Œæ›´å¤§çš„é¢„è®­ç»ƒæ•°æ®é‡é€šå¸¸ä¼šæé«˜è®¡ç®—æœºè§†è§‰ä¸­å„ç§ä¸‹æ¸¸ä»»åŠ¡çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ |
| âœ… This finding suggests that investing in larger pre-training datasets can provide a more effective and versatile foundation for handling a wide range of downstream tasks. | âœ… è¿™ä¸€å‘ç°è¡¨æ˜ï¼ŒæŠ•èµ„æ›´å¤§çš„é¢„è®­ç»ƒæ•°æ®é›†å¯ä»¥ä¸ºå¤„ç†å¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡æä¾›æ›´æœ‰æ•ˆã€æ›´é€šç”¨çš„åŸºç¡€ã€‚ |

| ã€ç¬¬6.5.3èŠ‚ï¼Œç¬¬4æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.3èŠ‚ï¼Œç¬¬4æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Our approach to scaling data is significantly more efficient than relying solely on human annotations, as most of the annotation generation is performed using model inference. | âœ… æˆ‘ä»¬æ‰©å±•æ•°æ®çš„æ–¹æ³•æ¯”å•çº¯ä¾èµ–äººå·¥æ³¨é‡Šè¦é«˜æ•ˆå¾—å¤šï¼Œå› ä¸ºå¤§å¤šæ•°æ³¨é‡Šç”Ÿæˆéƒ½æ˜¯ä½¿ç”¨æ¨¡å‹æ¨ç†æ‰§è¡Œçš„ã€‚ |
| âœ… By leveraging specialist models to generate annotations, we can substantially reduce the time and cost associated with manual annotation efforts, which often involve labor-intensive processes and may be subject to human errors or inconsistencies. | âœ… é€šè¿‡åˆ©ç”¨ä¸“ä¸šæ¨¡å‹æ¥ç”Ÿæˆæ³¨é‡Šï¼Œæˆ‘ä»¬å¯ä»¥å¤§å¤§å‡å°‘ä¸æ‰‹åŠ¨æ³¨é‡Šå·¥ä½œç›¸å…³çš„æ—¶é—´å’Œæˆæœ¬ï¼Œæ‰‹åŠ¨æ³¨é‡Šå·¥ä½œé€šå¸¸æ¶‰åŠåŠ³åŠ¨å¯†é›†å‹è¿‡ç¨‹ï¼Œå¹¶ä¸”å¯èƒ½å—åˆ°äººä¸ºé”™è¯¯æˆ–ä¸ä¸€è‡´çš„å½±å“ã€‚ |

| ã€ç¬¬6.5.3èŠ‚ï¼Œç¬¬5æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.3èŠ‚ï¼Œç¬¬5æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Furthermore, utilizing model-generated annotations enables us to scale the pre-training datasets more rapidly and efficiently, allowing us to explore the impact of larger data sizes on model performance across various downstream tasks in computer vision. | âœ… æ­¤å¤–ï¼Œåˆ©ç”¨æ¨¡å‹ç”Ÿæˆçš„æ³¨é‡Šä½¿æˆ‘ä»¬èƒ½å¤Ÿæ›´å¿«ã€æ›´æœ‰æ•ˆåœ°æ‰©å±•é¢„è®­ç»ƒæ•°æ®é›†ï¼Œä»è€Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæ¢ç´¢æ›´å¤§çš„æ•°æ®é‡å¯¹è®¡ç®—æœºè§†è§‰ä¸­å„ç§ä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ |
| âœ… This not only facilitates the development of more effective and versatile foundation models but also ensures that the annotation process remains sustainable and scalable as the need for high-quality labeled data continues to grow. | âœ… è¿™ä¸ä»…æœ‰åˆ©äºå¼€å‘æ›´æœ‰æ•ˆã€æ›´é€šç”¨çš„åŸºç¡€æ¨¡å‹ï¼Œè€Œä¸”è¿˜ç¡®ä¿æ³¨é‡Šè¿‡ç¨‹åœ¨å¯¹é«˜è´¨é‡æ ‡è®°æ•°æ®çš„éœ€æ±‚ä¸æ–­å¢é•¿çš„æƒ…å†µä¸‹ä¿æŒå¯æŒç»­æ€§å’Œå¯æ‰©å±•æ€§ã€‚ |

| ã€ç¬¬6.5.3èŠ‚ï¼Œç¬¬6æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.3èŠ‚ï¼Œç¬¬6æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… In summary, our data scaling approach offers a more efficient alternative to traditional human annotation methods by harnessing the power of specialist models for annotation generation. | âœ… æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„æ•°æ®æ‰©å±•æ–¹æ³•åˆ©ç”¨ä¸“ä¸šæ¨¡å‹çš„åŠŸèƒ½è¿›è¡Œæ³¨é‡Šç”Ÿæˆï¼Œä¸ºä¼ ç»Ÿäººå·¥æ³¨é‡Šæ–¹æ³•æä¾›äº†æ›´æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚ |
| âœ… This strategy enables us to accelerate the pre-training process, optimize model performance, and effectively manage the ever-increasing demand for labeled data in the field of computer vision. | âœ… è¿™ä¸€ç­–ç•¥ä½¿æˆ‘ä»¬èƒ½å¤ŸåŠ é€Ÿé¢„è®­ç»ƒè¿‡ç¨‹ï¼Œä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆç®¡ç†è®¡ç®—æœºè§†è§‰é¢†åŸŸå¯¹æ ‡è®°æ•°æ®ä¸æ–­å¢é•¿çš„éœ€æ±‚ã€‚ |

#### 6.5.4 Training settings.

| ã€ç¬¬6.5.4èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.4èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We analyze the basic model training settings for the two primary components of our model, namely the vision encoder and the multi-modality encoder-decoder. | âœ… æˆ‘ä»¬åˆ†æäº†æ¨¡å‹çš„ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼Œå³è§†è§‰ç¼–ç å™¨å’Œå¤šæ¨¡æ€ç¼–ç å™¨-è§£ç å™¨çš„åŸºæœ¬æ¨¡å‹è®­ç»ƒè®¾ç½®ã€‚ |
| âœ… The experiment results are presented in TableÂ 12 | âœ… å®éªŒç»“æœå‘ˆç°åœ¨TableÂ 12ä¸­ |

<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T12.2"><tbody class="ltx_tbody"><tr class="ltx_tr" id="S6.T12.2.1.1"><th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S6.T12.2.1.1.1" style="padding-left:2.1pt;padding-right:2.1pt;"></th><th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S6.T12.2.1.1.2" style="padding-left:2.1pt;padding-right:2.1pt;"></th><td class="ltx_td ltx_border_tt" id="S6.T12.2.1.1.3" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T12.2.1.1.4" style="padding-left:2.1pt;padding-right:2.1pt;">Caption</td><td class="ltx_td ltx_border_tt" id="S6.T12.2.1.1.5" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T12.2.1.1.6" style="padding-left:2.1pt;padding-right:2.1pt;">Detection</td><td class="ltx_td ltx_border_tt" id="S6.T12.2.1.1.7" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T12.2.1.1.8" style="padding-left:2.1pt;padding-right:2.1pt;">Grounding</td><td class="ltx_td ltx_border_tt" id="S6.T12.2.1.1.9" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S6.T12.2.1.1.10" style="padding-left:2.1pt;padding-right:2.1pt;">RES</td></tr><tr class="ltx_tr" id="S6.T12.2.2.2"><th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T12.2.2.2.1" style="padding-left:2.1pt;padding-right:2.1pt;">V Pre</th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T12.2.2.2.2" style="padding-left:2.1pt;padding-right:2.1pt;">L Pre</th><td class="ltx_td" id="S6.T12.2.2.2.3" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.2.2.4" style="padding-left:2.1pt;padding-right:2.1pt;">CIDEr</td><td class="ltx_td" id="S6.T12.2.2.2.5" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.2.2.6" style="padding-left:2.1pt;padding-right:2.1pt;">AP</td><td class="ltx_td" id="S6.T12.2.2.2.7" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.2.2.8" style="padding-left:2.1pt;padding-right:2.1pt;">Recall@1</td><td class="ltx_td" id="S6.T12.2.2.2.9" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.2.2.10" style="padding-left:2.1pt;padding-right:2.1pt;">mIOU</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.2.2.11" style="padding-left:2.1pt;padding-right:2.1pt;">oIOU</td></tr><tr class="ltx_tr" id="S6.T12.2.3.3"><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="11" id="S6.T12.2.3.3.1" style="padding-left:2.1pt;padding-right:2.1pt;">Freeze Vision Encoder</th></tr><tr class="ltx_tr" id="S6.T12.2.4.4"><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T12.2.4.4.1" style="padding-left:2.1pt;padding-right:2.1pt;">âœ“</th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T12.2.4.4.2" style="padding-left:2.1pt;padding-right:2.1pt;">âœ“</th><td class="ltx_td ltx_border_t" id="S6.T12.2.4.4.3" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.4.4.4" style="padding-left:2.1pt;padding-right:2.1pt;">120.0</td><td class="ltx_td ltx_border_t" id="S6.T12.2.4.4.5" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.4.4.6" style="padding-left:2.1pt;padding-right:2.1pt;">6.9</td><td class="ltx_td ltx_border_t" id="S6.T12.2.4.4.7" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.4.4.8" style="padding-left:2.1pt;padding-right:2.1pt;">66.3</td><td class="ltx_td ltx_border_t" id="S6.T12.2.4.4.9" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.4.4.10" style="padding-left:2.1pt;padding-right:2.1pt;">9.9</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.4.4.11" style="padding-left:2.1pt;padding-right:2.1pt;">13.6</td></tr><tr class="ltx_tr" id="S6.T12.2.5.5"><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="11" id="S6.T12.2.5.5.1" style="padding-left:2.1pt;padding-right:2.1pt;">Unfreeze Vision Encoder</th></tr><tr class="ltx_tr" id="S6.T12.2.6.6"><th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S6.T12.2.6.6.1" style="padding-left:2.1pt;padding-right:2.1pt;"></th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T12.2.6.6.2" style="padding-left:2.1pt;padding-right:2.1pt;">âœ“</th><td class="ltx_td ltx_border_t" id="S6.T12.2.6.6.3" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.6.6.4" style="padding-left:2.1pt;padding-right:2.1pt;">81.3</td><td class="ltx_td ltx_border_t" id="S6.T12.2.6.6.5" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.6.6.6" style="padding-left:2.1pt;padding-right:2.1pt;">4.9</td><td class="ltx_td ltx_border_t" id="S6.T12.2.6.6.7" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.6.6.8" style="padding-left:2.1pt;padding-right:2.1pt;">69.0</td><td class="ltx_td ltx_border_t" id="S6.T12.2.6.6.9" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.6.6.10" style="padding-left:2.1pt;padding-right:2.1pt;">15.3</td><td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.6.6.11" style="padding-left:2.1pt;padding-right:2.1pt;">15.6</td></tr><tr class="ltx_tr" id="S6.T12.2.7.7"><th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T12.2.7.7.1" style="padding-left:2.1pt;padding-right:2.1pt;">âœ“</th><th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S6.T12.2.7.7.2" style="padding-left:2.1pt;padding-right:2.1pt;"></th><td class="ltx_td" id="S6.T12.2.7.7.3" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center" id="S6.T12.2.7.7.4" style="padding-left:2.1pt;padding-right:2.1pt;">117.4</td><td class="ltx_td" id="S6.T12.2.7.7.5" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center" id="S6.T12.2.7.7.6" style="padding-left:2.1pt;padding-right:2.1pt;">19.6</td><td class="ltx_td" id="S6.T12.2.7.7.7" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center" id="S6.T12.2.7.7.8" style="padding-left:2.1pt;padding-right:2.1pt;">75.2</td><td class="ltx_td" id="S6.T12.2.7.7.9" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center" id="S6.T12.2.7.7.10" style="padding-left:2.1pt;padding-right:2.1pt;">21.5</td><td class="ltx_td ltx_align_center" id="S6.T12.2.7.7.11" style="padding-left:2.1pt;padding-right:2.1pt;">19.3</td></tr><tr class="ltx_tr" id="S6.T12.2.8.8"><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S6.T12.2.8.8.1" style="padding-left:2.1pt;padding-right:2.1pt;">âœ“</th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T12.2.8.8.2" style="padding-left:2.1pt;padding-right:2.1pt;">âœ“</th><td class="ltx_td ltx_border_bb" id="S6.T12.2.8.8.3" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T12.2.8.8.4" style="padding-left:2.1pt;padding-right:2.1pt;">118.7</td><td class="ltx_td ltx_border_bb" id="S6.T12.2.8.8.5" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T12.2.8.8.6" style="padding-left:2.1pt;padding-right:2.1pt;">19.7</td><td class="ltx_td ltx_border_bb" id="S6.T12.2.8.8.7" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T12.2.8.8.8" style="padding-left:2.1pt;padding-right:2.1pt;">76.3</td><td class="ltx_td ltx_border_bb" id="S6.T12.2.8.8.9" style="padding-left:2.1pt;padding-right:2.1pt;"></td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T12.2.8.8.10" style="padding-left:2.1pt;padding-right:2.1pt;">18.6</td><td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T12.2.8.8.11" style="padding-left:2.1pt;padding-right:2.1pt;">17.8</td></tr></tbody></table>

| ã€è¡¨æ ‡é¢˜ã€‘åŸæ–‡ | ã€è¡¨æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Table 12:  Basic components. Zero-shot performance on COCO caption, COCO object detection, Flickr30k grounding, and COCORef referring segmentation. | âœ… Table 12:  Basic components. åœ¨ COCO æ ‡é¢˜ã€COCO å¯¹è±¡æ£€æµ‹ã€Flickr30k åŸºç¡€å’Œ COCORef å¼•ç”¨åˆ†å‰²ä¸Šçš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ |
| âœ… V Pre and L Pre indicate that using vision and language pre-training initialization, respectively. | âœ… V Preå’ŒL Preåˆ†åˆ«è¡¨ç¤ºä½¿ç”¨è§†è§‰å’Œè¯­è¨€é¢„è®­ç»ƒåˆå§‹åŒ–ã€‚ |

| ã€ç¬¬6.5.4èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.4èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We observe that freezing the vision encoders does not affect the performance on tasks that require image-level understanding, but it significantly degrades the performance on tasks that require region-level or pixel-level understanding (e.g., AP on COCO object detection drops from 19.7 to 6.9). | âœ… æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå†»ç»“è§†è§‰ç¼–ç å™¨ä¸ä¼šå½±å“éœ€è¦å›¾åƒçº§ç†è§£çš„ä»»åŠ¡çš„æ€§èƒ½ï¼Œä½†ä¼šæ˜¾è‘—é™ä½éœ€è¦åŒºåŸŸçº§æˆ–åƒç´ çº§ç†è§£çš„ä»»åŠ¡çš„æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼ŒCOCO å¯¹è±¡æ£€æµ‹çš„ AP ä» 19.7 ä¸‹é™åˆ° 6.9ï¼‰ã€‚ |
| âœ… Previous methods for pre-training vision foundation models mainly focus on image-level tasks (e.g., image classification ( **1. Imagenet classification with deep convolutional neural networks.** ï½œ **2. Deep residual learning for image recognition.** ) , image-text contrastive learning ( **1. Learning transferable visual models from natural language supervision.** ï½œ **2. Florence: A new foundation model for computer vision.** ) ), which may not provide them with sufficient region-level and pixel-level skills for downstream tasks. | âœ… å…ˆå‰çš„è§†è§‰åŸºç¡€æ¨¡å‹é¢„è®­ç»ƒæ–¹æ³•ä¸»è¦ä¾§é‡äºå›¾åƒçº§ä»»åŠ¡ï¼ˆä¾‹å¦‚å›¾åƒåˆ†ç±» ( **1. Imagenet classification with deep convolutional neural networks.** ï½œ **2. Deep residual learning for image recognition.** )ã€å›¾åƒæ–‡æœ¬å¯¹æ¯”å­¦ä¹  ( **1. Learning transferable visual models from natural language supervision.** ï½œ **2. Florence: A new foundation model for computer vision.** )ï¼‰ï¼Œè¿™å¯èƒ½æ— æ³•ä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›è¶³å¤Ÿçš„åŒºåŸŸçº§å’Œåƒç´ çº§æŠ€èƒ½ã€‚ |
| âœ… Therefore, it is important to unfreeze the vision backbone, enabling it to learn region-level and pixel-level features for various downstream tasks. | âœ… å› æ­¤ï¼Œè§£å†»è§†è§‰ä¸»å¹²éå¸¸é‡è¦ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åŒºåŸŸçº§å’Œåƒç´ çº§ç‰¹å¾ä»¥ç”¨äºå„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚ |

| ã€ç¬¬6.5.4èŠ‚ï¼Œç¬¬3æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.4èŠ‚ï¼Œç¬¬3æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… The effect of language pre-training weights on multi-modal encoder-decoder tasks varies depending on the task. | âœ… è¯­è¨€é¢„è®­ç»ƒæƒé‡å¯¹å¤šæ¨¡æ€ç¼–ç å™¨-è§£ç å™¨ä»»åŠ¡çš„å½±å“å› ä»»åŠ¡è€Œå¼‚ã€‚ |
| âœ… Tasks that require more text understanding, such as captioning and grounding, benefit slightly from using language pre-training weights (e.g., COCO caption, Flickr30k grounding). | âœ… å¯¹äºéœ€è¦æ›´å¤šæ–‡æœ¬ç†è§£çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚å­—å¹•å’ŒåŸºç¡€ï¼‰æ¥è¯´ï¼Œä½¿ç”¨è¯­è¨€é¢„è®­ç»ƒæƒé‡ï¼ˆä¾‹å¦‚ COCO å­—å¹•ã€Flickr30k åŸºç¡€ï¼‰ä¼šç•¥æœ‰å¥½å¤„ã€‚ |
| âœ… Tasks that are mostly vision-focused, such as object detection and region segmentation, do not gain much from using language pre-training weights (for COCO object detection, the gain is only 0.1; for RES tasks, which use only localization tokens, the drop is 2.91 mIOU). | âœ… å¯¹äºä¸»è¦ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼ˆä¾‹å¦‚å¯¹è±¡æ£€æµ‹å’ŒåŒºåŸŸåˆ†å‰²ï¼‰ï¼Œä½¿ç”¨è¯­è¨€é¢„è®­ç»ƒæƒé‡ä¸ä¼šå¸¦æ¥å¤ªå¤§çš„æ”¶ç›Šï¼ˆå¯¹äº COCO å¯¹è±¡æ£€æµ‹ï¼Œæ”¶ç›Šä»…ä¸º 0.1ï¼›å¯¹äºä»…ä½¿ç”¨å®šä½æ ‡è®°çš„ RES ä»»åŠ¡ï¼Œä¸‹é™å¹…åº¦ä¸º 2.91 mIOUï¼‰ã€‚ |

| ã€ç¬¬6.5.4èŠ‚ï¼Œç¬¬4æ®µã€‘åŸæ–‡ | ã€ç¬¬6.5.4èŠ‚ï¼Œç¬¬4æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We investigate the effects of different training configurations on the performance of a foundation model in region-level and pixel-level tasks. | âœ… æˆ‘ä»¬ç ”ç©¶äº†ä¸åŒçš„è®­ç»ƒé…ç½®å¯¹åŸºç¡€æ¨¡å‹åœ¨åŒºåŸŸçº§å’Œåƒç´ çº§ä»»åŠ¡ä¸­æ€§èƒ½çš„å½±å“ã€‚ |
| âœ… We find that unfreezing the vision backbone is crucial for enhancing the modelâ€™s ability to learn from regions and pixels, which is beneficial for transferring to various downstream tasks. | âœ… æˆ‘ä»¬å‘ç°è§£å†»è§†è§‰ä¸»å¹²å¯¹äºå¢å¼ºæ¨¡å‹ä»åŒºåŸŸå’Œåƒç´ å­¦ä¹ çš„èƒ½åŠ›è‡³å…³é‡è¦ï¼Œè¿™æœ‰åˆ©äºè½¬ç§»åˆ°å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚ |
| âœ… Moreover, we observe that using language pre-training weights can help the model in tasks that require text understanding, but have less impact on tasks that are purely vision-based. | âœ… æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä½¿ç”¨è¯­è¨€é¢„è®­ç»ƒæƒé‡å¯ä»¥å¸®åŠ©æ¨¡å‹å®Œæˆéœ€è¦æ–‡æœ¬ç†è§£çš„ä»»åŠ¡ï¼Œä½†å¯¹çº¯ç²¹åŸºäºè§†è§‰çš„ä»»åŠ¡å½±å“è¾ƒå°ã€‚ |
| âœ… These results offer useful guidance for choosing the best training settings for different computer vision tasks. | âœ… è¿™äº›ç»“æœä¸ºé€‰æ‹©ä¸åŒè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„æœ€ä½³è®­ç»ƒè®¾ç½®æä¾›äº†æœ‰ç”¨çš„æŒ‡å¯¼ã€‚ |

## 7 Related Works

### 7.1 Vision-Language Foundation Models

| ã€ç¬¬7.1èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬7.1èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Recent vision-language pre-training models ( **1. Learning transferable visual models from natural language supervision.** ï½œ **2. Scaling up visual and vision-language representation learning with noisy text supervision, 2021.** ï½œ **3. Florence: A new foundation model for computer vision.** ) have demonstrated impressive zero-shot transfer abilities to vision-language alignment and image classification tasks, thanks to the alignment of vision and text embeddings extracted from respective encoders through contrastive learning objectives ( **1. Improved deep metric learning with multi-class n-pair loss objective.** ï½œ **2. Representation learning with contrastive predictive coding.** ). | âœ… æœ€è¿‘çš„è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ ( **1. Learning transferable visual models from natural language supervision.** ï½œ **2. Scaling up visual and vision-language representation learning with noisy text supervision, 2021.** ï½œ **3. Florence: A new foundation model for computer vision.** ) å·²ç»å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ï¼Œå¯ç”¨äºè§†è§‰è¯­è¨€å¯¹é½å’Œå›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œè¿™è¦å½’åŠŸäºé€šè¿‡å¯¹æ¯”å­¦ä¹ ç›®æ ‡ ( **1. Improved deep metric learning with multi-class n-pair loss objective.** ï½œ **2. Representation learning with contrastive predictive coding.** ) ä»å„è‡ªçš„ç¼–ç å™¨æå–çš„è§†è§‰å’Œæ–‡æœ¬åµŒå…¥çš„å¯¹é½ã€‚ |
| âœ… These models ( e.g. | âœ… è¿™äº›æ¨¡å‹ï¼ˆe.g. |
| âœ…  , ( **Florence: A new foundation model for computer vision.** ) ), trained on weakly large-scale image-text data, have been further extended to more downstream tasks such as object detection, achieving state-of-the-art performance with task-specific adaptation heads. | âœ… ã€( **Florence: A new foundation model for computer vision.** )ï¼‰åœ¨å¼±å¤§è§„æ¨¡å›¾åƒæ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶è¿›ä¸€æ­¥æ‰©å±•åˆ°å¯¹è±¡æ£€æµ‹ç­‰æ›´ä¸‹æ¸¸çš„ä»»åŠ¡ï¼Œå¹¶é€šè¿‡ç‰¹å®šäºä»»åŠ¡çš„è‡ªé€‚åº”å¤´å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚ |

| ã€ç¬¬7.1èŠ‚ï¼Œç¬¬2æ®µã€‘åŸæ–‡ | ã€ç¬¬7.1èŠ‚ï¼Œç¬¬2æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… In contrast, other studies ( **1. Coca: Contrastive captioners are image-text foundation models, 2022.** ï½œ **2. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.** ï½œ **3. Git: A generative image-to-text transformer for vision and language, 2022.** ï½œ **4. Flamingo: a visual language model for few-shot learning.** ) propose using a multi-modality decoder to predict text in an autoregressive manner with language modeling pre-training objectives. | âœ… ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…¶ä»–ç ”ç©¶ ( **1. Coca: Contrastive captioners are image-text foundation models, 2022.** ï½œ **2. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.** ï½œ **3. Git: A generative image-to-text transformer for vision and language, 2022.** ï½œ **4. Flamingo: a visual language model for few-shot learning.** ) æå‡ºä½¿ç”¨å¤šæ¨¡æ€è§£ç å™¨ä»¥è¯­è¨€å»ºæ¨¡é¢„è®­ç»ƒç›®æ ‡ä»¥è‡ªå›å½’æ–¹å¼é¢„æµ‹æ–‡æœ¬ã€‚ |
| âœ… Techniques for fusing vision and language embeddings vary: GIT ( **Git: A generative image-to-text transformer for vision and language, 2022.** ) concatenates vision and text tokens as decoder input and designs a casual attention mask, CoCa ( **Coca: Contrastive captioners are image-text foundation models, 2022.** ) uses attentional poolers with learnable queries to select task-specific vision representations which are then cross-attended via the decoder, and Flamingo ( **Flamingo: a visual language model for few-shot learning.** ) pools a fixed number of vision tokens with a Perceiver Resampler and adds new learnable cross-attention layers to the decoder while freezing the pre-trained vision encoder and text decoder. | âœ… èåˆè§†è§‰å’Œè¯­è¨€åµŒå…¥çš„æŠ€æœ¯å„ä¸ç›¸åŒï¼šGIT ( **Git: A generative image-to-text transformer for vision and language, 2022.** ) å°†è§†è§‰å’Œæ–‡æœ¬æ ‡è®°è¿æ¥èµ·æ¥ä½œä¸ºè§£ç å™¨è¾“å…¥å¹¶è®¾è®¡ä¸€ä¸ªéšæ„æ³¨æ„æ©ç ï¼ŒCoCa ( **Coca: Contrastive captioners are image-text foundation models, 2022.** ) ä½¿ç”¨å…·æœ‰å¯å­¦ä¹ æŸ¥è¯¢çš„æ³¨æ„åŠ›æ± æ¥é€‰æ‹©ç‰¹å®šäºä»»åŠ¡çš„è§†è§‰è¡¨ç¤ºï¼Œç„¶åé€šè¿‡è§£ç å™¨è¿›è¡Œäº¤å‰æ³¨æ„ï¼Œè€Œ Flamingo ( **Flamingo: a visual language model for few-shot learning.** ) å°†å›ºå®šæ•°é‡çš„è§†è§‰æ ‡è®°ä¸æ„ŸçŸ¥å™¨é‡é‡‡æ ·å™¨æ± åŒ–ï¼Œå¹¶å‘è§£ç å™¨æ·»åŠ æ–°çš„å¯å­¦ä¹ çš„äº¤å‰æ³¨æ„å±‚ï¼ŒåŒæ—¶å†»ç»“é¢„å…ˆè®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å’Œæ–‡æœ¬è§£ç å™¨ã€‚ |

| ã€ç¬¬7.1èŠ‚ï¼Œç¬¬3æ®µã€‘åŸæ–‡ | ã€ç¬¬7.1èŠ‚ï¼Œç¬¬3æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Beyond image captioning pre-training task, some research ( **1. Unified-io: A unified model for vision, language, and multi-modal tasks, 2022.** ï½œ **2. Pali: A jointly-scaled multilingual language-image model, 2022.** ï½œ **3. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework, 2022.** ) attempts to formulate more vision tasks in a unified sequence-to-sequence learning paradigm, including object detection and image segmentation. | âœ… é™¤äº†å›¾åƒå­—å¹•é¢„è®­ç»ƒä»»åŠ¡ä¹‹å¤–ï¼Œä¸€äº›ç ”ç©¶ ( **1. Unified-io: A unified model for vision, language, and multi-modal tasks, 2022.** ï½œ **2. Pali: A jointly-scaled multilingual language-image model, 2022.** ï½œ **3. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework, 2022.** ) å°è¯•åœ¨ç»Ÿä¸€çš„åºåˆ—åˆ°åºåˆ—å­¦ä¹ èŒƒå¼ä¸­åˆ¶å®šæ›´å¤šçš„è§†è§‰ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¯¹è±¡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ã€‚ |
| âœ… Customized special tokens accommodate representations beyond pure text, such as bounding boxes ( **1. Unified-io: A unified model for vision, language, and multi-modal tasks, 2022.** ï½œ **2. Pix2seq: A language modeling framework for object detection, 2022.** ï½œ **3. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework, 2022.** ). | âœ… å®šåˆ¶çš„ç‰¹æ®Šæ ‡è®°å¯é€‚åº”çº¯æ–‡æœ¬ä»¥å¤–çš„è¡¨ç¤ºï¼Œä¾‹å¦‚è¾¹ç•Œæ¡† ( **1. Unified-io: A unified model for vision, language, and multi-modal tasks, 2022.** ï½œ **2. Pix2seq: A language modeling framework for object detection, 2022.** ï½œ **3. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework, 2022.** )ã€‚ |
| âœ… This approach uses the same architecture for pre-training and downstream tasks, potentially using the same set of weights for all tasks. | âœ… è¯¥æ–¹æ³•å¯¹é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨ç›¸åŒçš„æ¶æ„ï¼Œå¯èƒ½å¯¹æ‰€æœ‰ä»»åŠ¡ä½¿ç”¨ç›¸åŒçš„æƒé‡é›†ã€‚ |
| âœ… Our method, which falls into this category, aims to obtain foundation models that understand dense information beyond simple image-level captions. | âœ… æˆ‘ä»¬çš„æ–¹æ³•å±äºè¿™ä¸€ç±»ï¼Œæ—¨åœ¨è·å¾—èƒ½å¤Ÿç†è§£ç®€å•å›¾åƒçº§æ ‡é¢˜ä¹‹å¤–çš„å¯†é›†ä¿¡æ¯çš„åŸºç¡€æ¨¡å‹ã€‚ |
| âœ… It shares the same encoder-decoder design as other multi-modality encoder-decoder models ( **1. Pali: A jointly-scaled multilingual language-image model, 2022.** ï½œ **2. Unified-io: A unified model for vision, language, and multi-modal tasks, 2022.** ) adapted for sequence-to-sequence learning, but uses our built large-scale comprehensive annotation data instead of combining existing sparse annotated data. | âœ… å®ƒä¸å…¶ä»–é€‚ç”¨äºåºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„å¤šæ¨¡æ€ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ ( **1. Pali: A jointly-scaled multilingual language-image model, 2022.** ï½œ **2. Unified-io: A unified model for vision, language, and multi-modal tasks, 2022.** ) å…·æœ‰ç›¸åŒçš„ç¼–ç å™¨-è§£ç å™¨è®¾è®¡ï¼Œä½†ä½¿ç”¨æˆ‘ä»¬æ„å»ºçš„å¤§è§„æ¨¡ç»¼åˆæ³¨é‡Šæ•°æ®ï¼Œè€Œä¸æ˜¯ç»“åˆç°æœ‰çš„ç¨€ç–æ³¨é‡Šæ•°æ®ã€‚ |

### 7.2 Vision Datasets

#### 7.2.1 Comprehensive annotations.

| ã€ç¬¬7.2.1èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬7.2.1èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… The quest for comprehensive understanding of visual scenes, the holy grail of computer vision ( **Visual genome: Connecting language and vision using crowdsourced dense image annotations.** ) , has evolved from focusing on individual datasets each targeting a single perspective, e.g. | âœ… å¯¹è§†è§‰åœºæ™¯è¿›è¡Œå…¨é¢ç†è§£çš„è¿½æ±‚ï¼Œå³è®¡ç®—æœºè§†è§‰çš„ç»ˆæç›®æ ‡ ( **Visual genome: Connecting language and vision using crowdsourced dense image annotations.** )ï¼Œå·²ç»ä»å…³æ³¨æ¯ä¸ªé’ˆå¯¹å•ä¸€è§†è§’çš„å•ä¸ªæ•°æ®é›† e.g å‘å±•è€Œæ¥ã€‚ |
| âœ…  , image classification ( **Imagenet: A large-scale hierarchical image database.** ) , to providing multi-perspective ( **1. Microsoft coco: Common objects in context.** ï½œ **2. Visual genome: Connecting language and vision using crowdsourced dense image annotations.** ï½œ **3. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.** ) , comprehensive annotations for every visual data point. | âœ… ã€å›¾åƒåˆ†ç±»( **Imagenet: A large-scale hierarchical image database.** )ã€ä¸ºæ¯ä¸ªè§†è§‰æ•°æ®ç‚¹æä¾›å¤šè§†è§’( **1. Microsoft coco: Common objects in context.** ï½œ **2. Visual genome: Connecting language and vision using crowdsourced dense image annotations.** ï½œ **3. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.** )ã€å…¨é¢çš„æ³¨é‡Šã€‚ |
| âœ… Notable datasets like MS-COCO ( **1. Microsoft coco: Common objects in context.** ï½œ **2. Microsoft coco captions: Data collection and evaluation server.** ) and Visual Genome ( **Visual genome: Connecting language and vision using crowdsourced dense image annotations.** ) integrate various types of annotations, enabling richer understanding in spatial and semantic granularities and better model interactions across annotations. | âœ… è‘—åçš„æ•°æ®é›†å¦‚ MS-COCO ( **1. Microsoft coco: Common objects in context.** ï½œ **2. Microsoft coco captions: Data collection and evaluation server.** ) å’Œ Visual Genome ( **Visual genome: Connecting language and vision using crowdsourced dense image annotations.** ) é›†æˆäº†å„ç§ç±»å‹çš„æ³¨é‡Šï¼Œä»è€Œèƒ½å¤Ÿæ›´ä¸°å¯Œåœ°ç†è§£ç©ºé—´å’Œè¯­ä¹‰ç²’åº¦ï¼Œå¹¶å®ç°è·¨æ³¨é‡Šçš„æ›´å¥½çš„æ¨¡å‹äº¤äº’ã€‚ |
| âœ… However, due to the high cost of human verification, these annotations are limited in size. | âœ… ä½†ç”±äºäººå·¥éªŒè¯çš„æˆæœ¬è¾ƒé«˜ï¼Œè¿™äº›æ³¨é‡Šçš„å¤§å°å—åˆ°é™åˆ¶ã€‚ |
| âœ… Our datasets, while large-scale, maintain comprehensive annotations covering text, region-text pairs, and text-phrase-region triplets, with reduced human involvement. | âœ… æˆ‘ä»¬çš„æ•°æ®é›†è™½ç„¶è§„æ¨¡å¾ˆå¤§ï¼Œä½†ä»ä¿æŒäº†æ¶µç›–æ–‡æœ¬ã€åŒºåŸŸ-æ–‡æœ¬å¯¹å’Œæ–‡æœ¬-çŸ­è¯­-åŒºåŸŸä¸‰å…ƒç»„çš„å…¨é¢æ³¨é‡Šï¼ŒåŒæ—¶å‡å°‘äº†äººå·¥å‚ä¸ã€‚ |

#### 7.2.2 Scalable annotations.

| ã€ç¬¬7.2.2èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬7.2.2èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… : Over the past decade, vision datasets have rapidly scaled up from thousands ( **1. Mnist handwritten digit database.** ï½œ **2. Learning multiple layers of features from tiny images.** ) to billion examples ( **1. Scaling up visual and vision-language representation learning with noisy text supervision, 2021.** ï½œ **2. Scaling vision transformers.** ) to encompass more visual concepts for better generalization. | âœ… ï¼šåœ¨è¿‡å»åå¹´ä¸­ï¼Œè§†è§‰æ•°æ®é›†å·²ä»æ•°åƒä¸ª ( **1. Mnist handwritten digit database.** ï½œ **2. Learning multiple layers of features from tiny images.** ) è¿…é€Ÿæ‰©å¤§åˆ°åäº¿ä¸ªç¤ºä¾‹ ( **1. Scaling up visual and vision-language representation learning with noisy text supervision, 2021.** ï½œ **2. Scaling vision transformers.** )ï¼Œä»¥æ¶µç›–æ›´å¤šçš„è§†è§‰æ¦‚å¿µï¼Œä»è€Œå®ç°æ›´å¥½çš„æ³›åŒ–ã€‚ |
| âœ… This shift is evident in recent foundation models that employ massive quantities of data ( **On the opportunities and risks of foundation models.** ). | âœ… è¿™ç§è½¬å˜åœ¨æœ€è¿‘é‡‡ç”¨å¤§é‡æ•°æ® ( **On the opportunities and risks of foundation models.** ) çš„åŸºç¡€æ¨¡å‹ä¸­æ˜¾è€Œæ˜“è§ã€‚ |
| âœ… These large datasets typically collect images from the web and parse noisy annotations from the corresponding metadata, such as category label from query ( **1. Revisiting unreasonable effectiveness of data in deep learning era.** ï½œ **2. Scaling vision transformers.** ) , short description from alt-text ( **1. Learning transferable visual models from natural language supervision.** ï½œ **2. Scaling up visual and vision-language representation learning with noisy text supervision, 2021.** ) , as well as detailed description from interleaved text ( **1. Flamingo: a visual language model for few-shot learning.** ï½œ **2. Obelisc: An open web-scale filtered dataset of interleaved image-text documents.** ). | âœ… è¿™äº›å¤§å‹æ•°æ®é›†é€šå¸¸ä»ç½‘ç»œä¸Šæ”¶é›†å›¾åƒï¼Œå¹¶ä»ç›¸åº”çš„å…ƒæ•°æ®ä¸­è§£æå™ªå£°æ³¨é‡Šï¼Œä¾‹å¦‚æ¥è‡ªæŸ¥è¯¢ ( **1. Revisiting unreasonable effectiveness of data in deep learning era.** ï½œ **2. Scaling vision transformers.** ) çš„ç±»åˆ«æ ‡ç­¾ã€æ¥è‡ªæ›¿ä»£æ–‡æœ¬ ( **1. Learning transferable visual models from natural language supervision.** ï½œ **2. Scaling up visual and vision-language representation learning with noisy text supervision, 2021.** ) çš„ç®€çŸ­æè¿°ï¼Œä»¥åŠæ¥è‡ªäº¤é”™æ–‡æœ¬ ( **1. Flamingo: a visual language model for few-shot learning.** ï½œ **2. Obelisc: An open web-scale filtered dataset of interleaved image-text documents.** ) çš„è¯¦ç»†æè¿°ã€‚ |
| âœ… Despite their diversity, these annotations suffer from randomness and limited types ( i.e. | âœ… å°½ç®¡è¿™äº›æ³¨é‡Šå…·æœ‰å¤šæ ·æ€§ï¼Œä½†å®ƒä»¬å´å…·æœ‰éšæœºæ€§å’Œæœ‰é™ç±»å‹ï¼ˆi.eï¼‰ã€‚ |
| âœ…  , texts only). | âœ… ï¼Œä»…é™æ–‡æœ¬ï¼‰ã€‚ |
| âœ… Some works ( **1. Segment anything.** ï½œ **2. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.** ) attempt to scale up annotations using pseudo-label generation with iteratively trained models, which offer higher quality without significant diversity loss. | âœ… ä¸€äº›ç ”ç©¶ ( **1. Segment anything.** ï½œ **2. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.** ) å°è¯•ä½¿ç”¨ç»è¿‡è¿­ä»£è®­ç»ƒçš„æ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾æ¥æ‰©å¤§æ³¨é‡Šï¼Œè¿™å¯ä»¥æä¾›æ›´é«˜çš„è´¨é‡è€Œä¸ä¼šé€ æˆæ˜æ˜¾çš„å¤šæ ·æ€§æŸå¤±ã€‚ |
| âœ… Our data pipeline extends these large-scale, web-crawled noisy annotations with higher-quality, autonomous annotations generated from multiple specialist models. | âœ… æˆ‘ä»¬çš„æ•°æ®ç®¡é“åˆ©ç”¨ç”±å¤šä¸ªä¸“å®¶æ¨¡å‹ç”Ÿæˆçš„æ›´é«˜è´¨é‡ã€è‡ªä¸»çš„æ³¨é‡Šæ¥æ‰©å±•è¿™äº›å¤§è§„æ¨¡ã€ç½‘ç»œçˆ¬è¡Œçš„å™ªå£°æ³¨é‡Šã€‚ |
| âœ… The pipeline iteratively refines labels and completes missing pieces, resulting in a scalable and comprehensive dataset for learning a unified visual representation. | âœ… è¯¥ç®¡é“è¿­ä»£åœ°ç»†åŒ–æ ‡ç­¾å¹¶å®Œæˆç¼ºå¤±çš„éƒ¨åˆ†ï¼Œä»è€Œäº§ç”Ÿä¸€ä¸ªå¯æ‰©å±•ä¸”å…¨é¢çš„æ•°æ®é›†ï¼Œç”¨äºå­¦ä¹ ç»Ÿä¸€çš„è§†è§‰è¡¨ç¤ºã€‚ |

## 8 Conclusion

| ã€ç¬¬8èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬8èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… The Florence Project endeavors to develop a foundational vision model endowed with a diverse array of perceptual capabilities, encompassing spatial hierarchy and semantic granularity. | âœ… ä½›ç½—ä¼¦è¨é¡¹ç›®è‡´åŠ›äºå¼€å‘ä¸€ç§å…·æœ‰å¤šç§æ„ŸçŸ¥èƒ½åŠ›çš„åŸºç¡€è§†è§‰æ¨¡å‹ï¼Œæ¶µç›–ç©ºé—´å±‚æ¬¡å’Œè¯­ä¹‰ç²’åº¦ã€‚ |
| âœ… To this end, we construct FLD-5B dataset containing an extensive collection of 126M images paired with 5B comprehensive annotations, which are collected by the Florence data engine. | âœ… ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº† FLD-5B æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å« 126M å¼ å›¾åƒå’Œ 5B ä»½ç»¼åˆæ³¨é‡Šï¼Œç”± Florence æ•°æ®å¼•æ“æ”¶é›†ã€‚ |
| âœ… Subsequently, we pre-train Florence-2 on this rich dataset through comprehensive multitask learning in a unified manner. | âœ… éšåï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªä¸°å¯Œçš„æ•°æ®é›†ä¸Šé€šè¿‡ç»Ÿä¸€çš„æ–¹å¼è¿›è¡Œç»¼åˆå¤šä»»åŠ¡å­¦ä¹ å¯¹Florence-2è¿›è¡Œé¢„è®­ç»ƒã€‚ |
| âœ… Florence-2 has exhibited remarkable zero-shot capabilities that extend across a wide spectrum of visual tasks, such as captioning, object detection, visual grounding, and referring segmentation, among others. | âœ… Florence-2 è¡¨ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¯æ¶µç›–å¹¿æ³›çš„è§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚å­—å¹•ã€å¯¹è±¡æ£€æµ‹ã€è§†è§‰åŸºç¡€å’ŒæŒ‡ç§°åˆ†å‰²ç­‰ã€‚ |
| âœ… The experimental findings underscore the potency of the universal representation pre-trained by Florence-2 , revealing its substantial contributions to the enhancement of a multitude of downstream tasks. | âœ… å®éªŒç»“æœå¼ºè°ƒäº† Florence-2 é¢„è®­ç»ƒçš„é€šç”¨è¡¨ç¤ºçš„æ•ˆåŠ›ï¼Œæ­ç¤ºäº†å…¶å¯¹å¢å¼ºå¤§é‡ä¸‹æ¸¸ä»»åŠ¡çš„é‡å¤§è´¡çŒ®ã€‚ |

#### 8.1 Acknowledgment.

| ã€ç¬¬8.1èŠ‚ï¼Œç¬¬1æ®µã€‘åŸæ–‡ | ã€ç¬¬8.1èŠ‚ï¼Œç¬¬1æ®µã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… We would like to express our heartfelt gratitude to all the contributors from the Azure AI team who worked on the Florence project. | âœ… æˆ‘ä»¬æƒ³å‘ Azure AI å›¢é˜Ÿæ‰€æœ‰å‚ä¸ Florence é¡¹ç›®çš„è´¡çŒ®è€…è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ã€‚ |
| âœ… We sincerely appreciate Misha Bilenko for the invaluable guidance and support. | âœ… æˆ‘ä»¬çœŸè¯šæ„Ÿè°¢ Misha Bilenko çš„å®è´µæŒ‡å¯¼å’Œæ”¯æŒã€‚ |
| âœ… Our thanks are extended to Yi-Ling Chen, Mengchen Liu, Yen-Chun Chen and Dongdong Chen for engaging in helpful discussions and to Yunsheng Li for their assistance with segmentation annotations. | âœ… æˆ‘ä»¬æ„Ÿè°¢ Yi-Ling Chenã€Mengchen Liuã€Yen-Chun Chen å’Œ Dongdong Chen å‚ä¸çš„æœ‰ç›Šè®¨è®ºï¼Œä»¥åŠæ„Ÿè°¢ Yunsheng Li å¯¹åˆ†å‰²æ³¨é‡Šçš„å¸®åŠ©ã€‚ |
| âœ… Deep appreciation is also expressed to Qingfen Lin, Ryan Menezes, Kuan Lu, Gabe Blanco, Shohei Ono, Ping Jin, Jiahe Zhou, Xiong Qiao, Tong Bai, Xingchao Peng, Pei Guo, Lihang Li for providing valuable feedback in downstream applications discussions. | âœ… åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯¹é’èŠ¬æ— (Qingfen Lin)ã€ç‘å®‰æ¢…å†…æ³½æ–¯ (Ryan Menezes)ã€å®½ (Kuan Lu)ã€åŠ è´å¸ƒå…°ç§‘ (Gabe Blanco)ã€å°é‡ç¿”å¹³ (Shohei Ono)ã€é‡‘å¹³ (Ping Jin)ã€å‘¨å˜‰ç¦¾ (Jiahe Zhou)ã€ä¹”é›„ (Xiong Qiao)ã€ç™½æ¡ (Tong Bai)ã€å½­å…´è¶… (Xingchao Peng)ã€éƒ­åŸ¹ (Pei Guo)ã€æèˆª (Lihang Li) åœ¨ä¸‹æ¸¸åº”ç”¨è®¨è®ºä¸­æä¾›çš„å®è´µåé¦ˆè¡¨ç¤ºæ·±æ·±çš„æ„Ÿè°¢ã€‚ |
| âœ… Special thanks to Cha Zhang, Jinyu Li, Min Gao, Christina Sun, Oliver Ernst, Kevin Pan, Mei Gao for their work on data annotation support and insightful discussions in data pipeline. | âœ… ç‰¹åˆ«æ„Ÿè°¢ Cha Zhangã€Jinyu Liã€Min Gaoã€Christina Sunã€Oliver Ernstã€Kevin Pan å’Œ Mei Gao åœ¨æ•°æ®æ³¨é‡Šæ”¯æŒæ–¹é¢æ‰€åšçš„å·¥ä½œä»¥åŠåœ¨æ•°æ®ç®¡é“ä¸­çš„æ·±åˆ»è®¨è®ºã€‚ |
| âœ… Furthermore, we would like to thank Thomas Soemo, Nguyen Bach for their constructive feedback. | âœ… æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¦æ„Ÿè°¢ Thomas Soemo å’Œ Nguyen Bach çš„å»ºè®¾æ€§åé¦ˆã€‚ |

## 9 References

- 1
  - Azure ai services.
  - **https://azure.microsoft.com/en-us/products/ai-services?activetab=pivot:azureopenaiservicetab.**
  - Accessed: 2023-10-13.

- 2
  - Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, etÂ al.
  - **Flamingo: a visual language model for few-shot learning.**
  - Advances in Neural Information Processing Systems, 35:23716â€“23736, 2022.

- 3
  - JimmyÂ Lei Ba, JamieÂ Ryan Kiros, and GeoffreyÂ E. Hinton.
  - **Layer normalization, 2016.**

- 4
  - Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei.
  - **BEiT: BERT pre-training of image transformers.**
  - In International Conference on Learning Representations, 2022.

- 5
  - Rishi Bommasani, DrewÂ A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, MichaelÂ S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, etÂ al.
  - **On the opportunities and risks of foundation models.**
  - arXiv preprint arXiv:2108.07258, 2021.

- 6
  - Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
  - **Language models are few-shot learners.**
  - In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volumeÂ 33, pages 1877â€“1901. Curran Associates, Inc., 2020.

- 7
  - Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.
  - **End-to-end object detection with transformers.**
  - In European conference on computer vision, pages 213â€“229. Springer, 2020.

- 8
  - Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
  - **Unsupervised learning of visual features by contrasting cluster assignments.**
  - In Advances in Neural Information Processing Systems, volumeÂ 33, 2020.

- 9
  - Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
  - **A simple framework for contrastive learning of visual representations.**
  - In International conference on machine learning, pages 1597â€“1607. PMLR, 2020.

- 10
  - Ting Chen, Saurabh Saxena, Lala Li, DavidÂ J. Fleet, and Geoffrey Hinton.
  - **Pix2seq: A language modeling framework for object detection, 2022.**

- 11
  - Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, DavidÂ J Fleet, and GeoffreyÂ E Hinton.
  - **A unified sequence interface for vision tasks.**
  - Advances in Neural Information Processing Systems, 35:31333â€“31346, 2022.

- 12
  - Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, CarlosÂ Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, etÂ al.
  - **Pali-x: On scaling up a multilingual vision and language model.**
  - arXiv preprint arXiv:2305.18565, 2023.

- 13
  - Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr DollÃ¡r, and CÂ Lawrence Zitnick.
  - **Microsoft coco captions: Data collection and evaluation server.**
  - arXiv preprint arXiv:1504.00325, 2015.

- 14
  - Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Soricut.
  - **Pali-3 vision language models: Smaller, faster, stronger, 2023.**

- 15
  - Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, BurcuÂ Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut.
  - **Pali: A jointly-scaled multilingual language-image model, 2022.**

- 16
  - Bowen Cheng, Ishan Misra, AlexanderÂ G. Schwing, Alexander Kirillov, and Rohit Girdhar.
  - **Masked-attention mask transformer for universal image segmentation.**
  - 2022.

- 17
  - Kyunghyun Cho, Bart VanÂ MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
  - **Learning phrase representations using rnn encoder-decoder for statistical machine translation.**
  - arXiv preprint arXiv:1406.1078, 2014.

- 18
  - Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
  - **Imagenet: A large-scale hierarchical image database.**
  - In 2009 IEEE conference on computer vision and pattern recognition, pages 248â€“255. Ieee, 2009.

- 19
  - Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
  - **Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.**

- 20
  - Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, and Lu Yuan.
  - **Davit: Dual attention vision transformers.**
  - In Computer Visionâ€“ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XXIV, pages 74â€“92. Springer, 2022.

- 21
  - Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
  - **An image is worth 16x16 words: Transformers for image recognition at scale, 2021.**

- 22
  - Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
  - **Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering.**
  - In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.

- 23
  - Danna Gurari, Qing Li, AbigaleÂ J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and JeffreyÂ P Bigham.
  - **Vizwiz grand challenge: Answering visual questions from blind people.**
  - In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3608â€“3617, 2018.

- 24
  - Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick.
  - **Masked autoencoders are scalable vision learners.**
  - In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000â€“16009, 2022.

- 25
  - Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
  - **Momentum contrast for unsupervised visual representation learning.**
  - In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729â€“9738, 2020.

- 26
  - Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick.
  - **Mask r-cnn.**
  - In Proceedings of the IEEE international conference on computer vision, pages 2961â€“2969, 2017.

- 27
  - Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
  - **Deep residual learning for image recognition.**
  - In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770â€“778, 2016.

- 28
  - Matthew Honnibal, Ines Montani, Sofie VanÂ Landeghem, Adriane Boyd, etÂ al.
  - **spacy: Industrial-strength natural language processing in python.**
  - 2020.

- 29
  - Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, QuocÂ V. Le, Yunhsuan Sung, Zhen Li, and Tom Duerig.
  - **Scaling up visual and vision-language representation learning with noisy text supervision, 2021.**

- 30
  - Andrej Karpathy and Li Fei-Fei.
  - **Deep visual-semantic alignments for generating image descriptions.**
  - 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3128â€“3137, 2014.

- 31
  - Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg.
  - **Referitgame: Referring to objects in photographs of natural scenes.**
  - In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787â€“798, 2014.

- 32
  - Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, AlexanderÂ C Berg, Wan-Yen Lo, etÂ al.
  - **Segment anything.**
  - arXiv preprint arXiv:2304.02643, 2023.

- 33
  - Aniket Kittur, Ed Chi, BryanÂ A Pendleton, Bongwon Suh, and Todd Mytkowicz.
  - **Power of the few vs. wisdom of the crowd: Wikipedia and the rise of the bourgeoisie.**
  - World wide web, 1(2):19, 2007.

- 34
  - Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei.
  - **A hierarchical approach for generating descriptive image paragraphs.**
  - In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 317â€“325, 2017.

- 35
  - Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei.
  - **A hierarchical approach for generating descriptive image paragraphs.**
  - In Computer Vision and Patterm Recognition (CVPR), 2017.

- 36
  - Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, DavidÂ A Shamma, etÂ al.
  - **Visual genome: Connecting language and vision using crowdsourced dense image annotations.**
  - International journal of computer vision, 123:32â€“73, 2017.

- 37
  - Alex Krizhevsky, Geoffrey Hinton, etÂ al.
  - **Learning multiple layers of features from tiny images.**
  - 2009.

- 38
  - Alex Krizhevsky, Ilya Sutskever, and GeoffreyÂ E Hinton.
  - **Imagenet classification with deep convolutional neural networks.**
  - In Advances in neural information processing systems, pages 1097â€“1105, 2012.

- 39
  - Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari.
  - **The open images dataset v4.**
  - International Journal of Computer Vision, 128(7):1956â€“1981, mar 2020.

- 40
  - Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, etÂ al.
  - **The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.**
  - International Journal of Computer Vision, 128(7):1956â€“1981, 2020.

- 41
  - Hugo LaurenÃ§on, Lucile Saulnier, LÃ©o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, AlexanderÂ M Rush, Douwe Kiela, etÂ al.
  - **Obelisc: An open web-scale filtered dataset of interleaved image-text documents.**
  - arXiv preprint arXiv:2306.16527, 2023.

- 42
  - Yann LeCun, Corinna Cortes, and CJ Burges.
  - **Mnist handwritten digit database.**
  - ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.

- 43
  - Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
  - **Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019.**

- 44
  - Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
  - **Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.**
  - arXiv preprint arXiv:2301.12597, 2023.

- 45
  - Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
  - **Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.**
  - In International Conference on Machine Learning, pages 12888â€“12900. PMLR, 2022.

- 46
  - Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.
  - **Exploring plain vision transformer backbones for object detection.**
  - In European Conference on Computer Vision, pages 280â€“296. Springer, 2022.

- 47
  - Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C.Â Lawrence Zitnick, and Piotr DollÃ¡r.
  - **Microsoft coco: Common objects in context, 2015.**

- 48
  - Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and CÂ Lawrence Zitnick.
  - **Microsoft coco: Common objects in context.**
  - In Computer Visionâ€“ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740â€“755. Springer, 2014.

- 49
  - Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, RaviÂ Kumar Satzoda, Vijay Mahadevan, and R Manmatha.
  - **Polyformer: Referring image segmentation as sequential polygon generation.**
  - In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18653â€“18663, 2023.

- 50
  - Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, etÂ al.
  - **Grounding dino: Marrying dino with grounded pre-training for open-set object detection.**
  - arXiv preprint arXiv:2303.05499, 2023.

- 51
  - Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
  - **Swin transformer: Hierarchical vision transformer using shifted windows, 2021.**

- 52
  - Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
  - **A convnet for the 2020s.**
  - In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976â€“11986, 2022.

- 53
  - Ilya Loshchilov and Frank Hutter.
  - **Sgdr: Stochastic gradient descent with warm restarts, 2017.**

- 54
  - Ilya Loshchilov and Frank Hutter.
  - **Decoupled weight decay regularization, 2019.**

- 55
  - Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.
  - **Unified-io: A unified model for vision, language, and multi-modal tasks, 2022.**

- 56
  - Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, AlanÂ L Yuille, and Kevin Murphy.
  - **Generation and comprehension of unambiguous object descriptions.**
  - In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11â€“20, 2016.

- 57
  - Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
  - **Ok-vqa: A visual question answering benchmark requiring external knowledge, 2019.**

- 58
  - Aaron vanÂ den Oord, Yazhe Li, and Oriol Vinyals.
  - **Representation learning with contrastive predictive coding.**
  - arXiv preprint arXiv:1807.03748, 2018.

- 59
  - Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei.
  - **BEiT v2: Masked image modeling with vector-quantized visual tokenizers.**
  - 2022.

- 60
  - Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.
  - **Kosmos-2: Grounding multimodal large language models to the world.**
  - arXiv preprint arXiv:2306.14824, 2023.

- 61
  - BryanÂ A Plummer, Liwei Wang, ChrisÂ M Cervantes, JuanÂ C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.
  - **Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.**
  - In Proceedings of the IEEE international conference on computer vision, pages 2641â€“2649, 2015.

- 62
  - Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari.
  - **Connecting vision and language with localized narratives.**
  - In ECCV, 2020.

- 63
  - Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende, Yash Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Mahajan.
  - **Filtering, distillation, and hard negatives for vision-language pre-training.**
  - arXiv preprint arXiv:2301.02280, 2023.

- 64
  - Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, etÂ al.
  - **Learning transferable visual models from natural language supervision.**
  - In International conference on machine learning, pages 8748â€“8763. PMLR, 2021.

- 65
  - Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
  - **Language models are unsupervised multitask learners.**
  - 2019.

- 66
  - Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and PeterÂ J Liu.
  - **Exploring the limits of transfer learning with a unified text-to-text transformer.**
  - The Journal of Machine Learning Research, 21(1):5485â€“5551, 2020.

- 67
  - Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.
  - **Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.**
  - In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505â€“3506, 2020.

- 68
  - Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.
  - **Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.**
  - arXiv preprint arXiv:2111.02114, 2021.

- 69
  - Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.
  - **A-okvqa: A benchmark for visual question answering using world knowledge, 2022.**

- 70
  - Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun.
  - **Objects365: A large-scale, high-quality dataset for object detection.**
  - In Proceedings of the IEEE/CVF international conference on computer vision, pages 8430â€“8439, 2019.

- 71
  - Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
  - **Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.**
  - In Proceedings of ACL, 2018.

- 72
  - Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh.
  - **Textcaps: a dataset for image captioning with reading comprehension, 2020.**

- 73
  - Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
  - **Towards vqa models that can read.**
  - In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8317â€“8326, 2019.

- 74
  - Kihyuk Sohn.
  - **Improved deep metric learning with multi-class n-pair loss objective.**
  - Advances in neural information processing systems, 29, 2016.

- 75
  - Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.
  - **Revisiting unreasonable effectiveness of data in deep learning era.**
  - In Proceedings of the IEEE international conference on computer vision, pages 843â€“852, 2017.

- 76
  - Ilya Sutskever, Oriol Vinyals, and QuocÂ V Le.
  - **Sequence to sequence learning with neural networks.**
  - Advances in neural information processing systems, 27, 2014.

- 77
  - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin.
  - **Attention is all you need.**
  - In Advances in neural information processing systems, pages 5998â€“6008, 2017.

- 78
  - Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.
  - **Git: A generative image-to-text transformer for vision and language, 2022.**

- 79
  - Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang.
  - **Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework, 2022.**

- 80
  - NicÂ M Weststrate, Susan Bluck, and Judith GlÃ¼ck.
  - **Wisdom of the crowd.**
  - The Cambridge handbook of wisdom, pages 97â€“121, 2019.

- 81
  - Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, InÂ So Kweon, and Saining Xie.
  - **Convnext v2: Co-designing and scaling convnets with masked autoencoders.**
  - In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16133â€“16142, 2023.

- 82
  - Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun.
  - **Unified perceptual parsing for scene understanding.**
  - In Proceedings of the European conference on computer vision (ECCV), pages 418â€“434, 2018.

- 83
  - Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu.
  - **Simmim: A simple framework for masked image modeling.**
  - In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9653â€“9663, 2022.

- 84
  - Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu.
  - **Universal instance perception as object discovery and retrieval.**
  - In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15325â€“15336, 2023.

- 85
  - Jianwei Yang, Chunyuan Li, Xiyang Dai, and Jianfeng Gao.
  - **Focal modulation networks.**
  - Advances in Neural Information Processing Systems, 35:4203â€“4217, 2022.

- 86
  - Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao.
  - **Focal self-attention for local-global interactions in vision transformers.**
  - arXiv preprint arXiv:2107.00641, 2021.

- 87
  - Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, and Jianfeng Gao.
  - **Unified contrastive learning in image-text-label space, 2022.**

- 88
  - Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang.
  - **Unitab: Unifying text and box outputs for grounded vision-language modeling.**
  - In European Conference on Computer Vision, pages 521â€“539. Springer, 2022.

- 89
  - Sheng KungÂ Michael Yi, Mark Steyvers, MichaelÂ D Lee, and MatthewÂ J Dry.
  - **The wisdom of the crowd in combinatorial problems.**
  - Cognitive science, 36(3):452â€“470, 2012.

- 90
  - Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang.
  - **Ferret: Refer and ground anything anywhere at any granularity, 2023.**

- 91
  - Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.
  - **From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.**
  - Transactions of the Association for Computational Linguistics, 2:67â€“78, 2014.

- 92
  - Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.
  - **Coca: Contrastive captioners are image-text foundation models, 2022.**

- 93
  - Licheng Yu, Patrick Poirson, Shan Yang, AlexanderÂ C Berg, and TamaraÂ L Berg.
  - **Modeling context in referring expressions.**
  - In Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 69â€“85. Springer, 2016.

- 94
  - Licheng Yu, Patrick Poirson, Shan Yang, AlexanderÂ C. Berg, and TamaraÂ L. Berg.
  - **Modeling context in referring expressions.**
  - In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision â€“ ECCV 2016, pages 69â€“85, Cham, 2016. Springer International Publishing.

- 95
  - Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and Pengchuan Zhang.
  - **Florence: A new foundation model for computer vision.**
  - arXiv preprint arXiv:2111.11432, 2021.

- 96
  - Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
  - **Scaling vision transformers.**
  - In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104â€“12113, 2022.

- 97
  - Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, LionelÂ M Ni, and Heung-Yeung Shum.
  - **Dino: Detr with improved denoising anchor boxes for end-to-end object detection.**
  - arXiv preprint arXiv:2203.03605, 2022.

- 98
  - Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.
  - **Scene parsing through ade20k dataset.**
  - In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633â€“641, 2017.

- 99
  - Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo, Xingjia Pan, Mingbao Lin, Chao Chen, Liujuan Cao, Xiaoshuai Sun, and Rongrong Ji.
  - **Seqtr: A simple yet universal network for visual grounding.**
  - In European Conference on Computer Vision, pages 598â€“615. Springer, 2022.

- 100
  - Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
  - **Deformable detr: Deformable transformers for end-to-end object detection.**
  - arXiv preprint arXiv:2010.04159, 2020.

## 10 Appendix A Supported Tasks and Annotations in Florence-2

<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T13.2"><thead class="ltx_thead"><tr class="ltx_tr" id="A1.T13.2.1.1"><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T13.2.1.1.1" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Task</th><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T13.2.1.1.2" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Annotation Type</th><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T13.2.1.1.3" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Prompt Input</th><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T13.2.1.1.4" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Output</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr" id="A1.T13.2.2.1"><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T13.2.2.1.1" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Caption</td><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T13.2.2.1.2" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text</td><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T13.2.2.1.3" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Image, text</td><td class="ltx_td ltx_align_left ltx_border_t" id="A1.T13.2.2.1.4" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text</td></tr><tr class="ltx_tr" id="A1.T13.2.3.2"><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.3.2.1" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Detailed caption</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.3.2.2" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.3.2.3" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Image, text</td><td class="ltx_td ltx_align_left" id="A1.T13.2.3.2.4" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text</td></tr><tr class="ltx_tr" id="A1.T13.2.4.3"><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.4.3.1" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â More detailed caption</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.4.3.2" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.4.3.3" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Image, text</td><td class="ltx_td ltx_align_left" id="A1.T13.2.4.3.4" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text</td></tr><tr class="ltx_tr" id="A1.T13.2.5.4"><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.5.4.1" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Region proposal</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.5.4.2" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Region</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.5.4.3" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Image, text</td><td class="ltx_td ltx_align_left" id="A1.T13.2.5.4.4" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Region</td></tr><tr class="ltx_tr" id="A1.T13.2.6.5"><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.6.5.1" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Object detection</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.6.5.2" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Region-Text</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.6.5.3" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Image, text</td><td class="ltx_td ltx_align_left" id="A1.T13.2.6.5.4" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text, region</td></tr><tr class="ltx_tr" id="A1.T13.2.7.6"><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.7.6.1" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Dense region caption</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.7.6.2" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Region-Text</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.7.6.3" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Image, text</td><td class="ltx_td ltx_align_left" id="A1.T13.2.7.6.4" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text, region</td></tr><tr class="ltx_tr" id="A1.T13.2.8.7"><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.8.7.1" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Phrase grounding</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.8.7.2" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text-Phrase-Region</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.8.7.3" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Image, text</td><td class="ltx_td ltx_align_left" id="A1.T13.2.8.7.4" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text, region</td></tr><tr class="ltx_tr" id="A1.T13.2.9.8"><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.9.8.1" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Referring expression comprehension</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.9.8.2" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Region-Text</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.9.8.3" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Image, text</td><td class="ltx_td ltx_align_left" id="A1.T13.2.9.8.4" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text, region</td></tr><tr class="ltx_tr" id="A1.T13.2.10.9"><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.10.9.1" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Open vocabulary detection</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.10.9.2" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Region-Text</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.10.9.3" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Image, text</td><td class="ltx_td ltx_align_left" id="A1.T13.2.10.9.4" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text, region</td></tr><tr class="ltx_tr" id="A1.T13.2.11.10"><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.11.10.1" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Referring segmentation</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.11.10.2" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Region-Text</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.11.10.3" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Image, text</td><td class="ltx_td ltx_align_left" id="A1.T13.2.11.10.4" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text, region</td></tr><tr class="ltx_tr" id="A1.T13.2.12.11"><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.12.11.1" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Region to text</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.12.11.2" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Region-Text</td><td class="ltx_td ltx_align_left ltx_border_r" id="A1.T13.2.12.11.3" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Image, text, region</td><td class="ltx_td ltx_align_left" id="A1.T13.2.12.11.4" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text</td></tr><tr class="ltx_tr" id="A1.T13.2.13.12"><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="A1.T13.2.13.12.1" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text detection and recognition</td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="A1.T13.2.13.12.2" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Region-Text</td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="A1.T13.2.13.12.3" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Image, text</td><td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T13.2.13.12.4" style="padding-left:18.0pt;padding-right:18.0pt;">Â Â Â Â Â Â Text, region</td></tr></tbody></table>

| ã€è¡¨æ ‡é¢˜ã€‘åŸæ–‡ | ã€è¡¨æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Table 13:  Supported Tasks and annotations used for Florence-2 pretraining. | âœ… Table 13:  Supported Tasks and annotations used for Florence-2 pretraining. |

## 11 Appendix B Supervised Data Collection for Generalist Model Fine-tuning

<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T14.6"><tbody class="ltx_tbody"><tr class="ltx_tr" id="A2.T14.6.7.1"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A2.T14.6.7.1.1" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Task</th><td class="ltx_td ltx_align_left ltx_border_tt" id="A2.T14.6.7.1.2" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Dataset</td></tr><tr class="ltx_tr" id="A2.T14.6.8.2"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A2.T14.6.8.2.1" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Caption</th><td class="ltx_td ltx_align_left ltx_border_t" id="A2.T14.6.8.2.2" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â COCOÂ <html><body><p>( <strong>Microsoft coco captions: Data collection and evaluation server.</strong> )</p></body></html></td></tr><tr class="ltx_tr" id="A2.T14.6.9.3"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T14.6.9.3.1" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Text Caption</th><td class="ltx_td ltx_align_left" id="A2.T14.6.9.3.2" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â TextCapsÂ <html><body><p>( <strong>Textcaps: a dataset for image captioning with reading comprehension,2020.</strong> )</p></body></html></td></tr><tr class="ltx_tr" id="A2.T14.6.10.4"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T14.6.10.4.1" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Paragraph caption</th><td class="ltx_td ltx_align_left" id="A2.T14.6.10.4.2" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Standford Paragraph CaptionÂ <html><body><p>( <strong>A hierarchical approach for generating descriptive image paragraphs.</strong> )</p></body></html></td></tr><tr class="ltx_tr" id="A2.T14.6.11.5"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T14.6.11.5.1" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Detailed caption</th><td class="ltx_td ltx_align_left" id="A2.T14.6.11.5.2" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Localized NarrativesÂ <html><body><p>( <strong>Connecting vision and language with localized narratives.</strong> )</p></body></html></td></tr><tr class="ltx_tr" id="A2.T14.2.2"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T14.2.2.3" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Detection</th><td class="ltx_td ltx_align_left" id="A2.T14.2.2.2" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â COCOÂ <html><body><p>( <strong>Microsoft coco: Common objects in context, 2015.</strong> )</p></body></html>, Object365<sup class="ltx_sup" id="A2.T14.2.2.2.5">âˆ—</sup>Â <html><body><p>( <strong>Objects365: A large-scale, high-quality dataset for object detection.</strong> )</p></body></html>, Open Images<sup class="ltx_sup" id="A2.T14.2.2.2.10">âˆ—</sup>Â <html><body><p>( <strong>The open images dataset v4.</strong> )</p></body></html></td></tr><tr class="ltx_tr" id="A2.T14.4.4"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T14.4.4.3" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Phrase Grounding</th><td class="ltx_td ltx_align_left" id="A2.T14.4.4.2" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Flickr30k, Object365<sup class="ltx_sup" id="A2.T14.4.4.2.2">âˆ—</sup>Â <html><body><p>( <strong>Objects365: A large-scale, high-quality dataset for object detection.</strong> )</p></body></html>, Open Images<sup class="ltx_sup" id="A2.T14.4.4.2.7">âˆ—</sup>Â <html><body><p>( <strong>The open images dataset v4.</strong> )</p></body></html></td></tr><tr class="ltx_tr" id="A2.T14.6.12.6"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T14.6.12.6.1" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Referring expression</th><td class="ltx_td ltx_align_left" id="A2.T14.6.12.6.2" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â RefCOCO-mix (RefCOCO, RefCOCO+, RefCOCOg)Â <html><body><p>( <strong>1. Referitgame: Referring to objects in photographs of natural scenes.</strong> ï½œ <strong>2. Modeling context in referring expressions.</strong> ï½œ <strong>3. Generation and comprehension of unambiguous object descriptions.</strong> )</p></body></html></td></tr><tr class="ltx_tr" id="A2.T14.6.13.7"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T14.6.13.7.1" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Referring expression segmentation</th><td class="ltx_td ltx_align_left" id="A2.T14.6.13.7.2" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â RefCOCO-mix (RefCOCO, RefCOCO+, RefCOCOg)Â <html><body><p>( <strong>1. Referitgame: Referring to objects in photographs of natural scenes.</strong> ï½œ <strong>2. Modeling context in referring expressions.</strong> ï½œ <strong>3. Generation and comprehension of unambiguous object descriptions.</strong> )</p></body></html></td></tr><tr class="ltx_tr" id="A2.T14.6.6"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T14.6.6.3" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Region to category</th><td class="ltx_td ltx_align_left" id="A2.T14.6.6.2" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â COCOÂ <html><body><p>( <strong>Microsoft coco: Common objects in context, 2015.</strong> )</p></body></html>, Object365<sup class="ltx_sup" id="A2.T14.6.6.2.5">âˆ—</sup>Â <html><body><p>( <strong>Objects365: A large-scale, high-quality dataset for object detection.</strong> )</p></body></html>, Open Images<sup class="ltx_sup" id="A2.T14.6.6.2.10">âˆ—</sup>Â <html><body><p>( <strong>The open images dataset v4.</strong> )</p></body></html></td></tr><tr class="ltx_tr" id="A2.T14.6.14.8"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T14.6.14.8.1" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Region to polygon</th><td class="ltx_td ltx_align_left" id="A2.T14.6.14.8.2" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â COCOÂ <html><body><p>( <strong>Microsoft coco: Common objects in context, 2015.</strong> )</p></body></html> (after deduplicating RefCOCO-mix val)</td></tr><tr class="ltx_tr" id="A2.T14.6.15.9"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T14.6.15.9.1" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â VQA</th><td class="ltx_td ltx_align_left" id="A2.T14.6.15.9.2" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â VQAv2Â <html><body><p>( <strong>Making the V in VQA matter: Elevating the role of imageunderstanding in Visual Question Answering.</strong> )</p></body></html>, OKVQAÂ <html><body><p>( <strong>Ok-vqa: A visual question answering benchmark requiring externalknowledge, 2019.</strong> )</p></body></html>, AOKVQAÂ <html><body><p>( <strong>A-okvqa: A benchmark for visual question answering using worldknowledge, 2022.</strong> )</p></body></html>, TextVQAÂ <html><body><p>( <strong>Towards vqa models that can read.</strong> )</p></body></html>, ViZWiz VQAÂ <html><body><p>( <strong>Vizwiz grand challenge: Answering visual questions from blind people.</strong> )</p></body></html></td></tr><tr class="ltx_tr" id="A2.T14.6.16.10"><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A2.T14.6.16.10.1" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â OCR</th><td class="ltx_td ltx_align_left ltx_border_bb" id="A2.T14.6.16.10.2" style="padding-left:20.0pt;padding-right:20.0pt;">Â Â Â Â Â Â Subset from <em class="ltx_emph ltx_font_italic" id="A2.T14.6.16.10.2.2" style="font-size:90%;">FLD-5B</em> OCR (2 millon samples)</td></tr></tbody></table>

| ã€è¡¨æ ‡é¢˜ã€‘åŸæ–‡ | ã€è¡¨æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Table 14:  Collection of dataset for finetuning one single generalist model for downstream tasks evaluation. | âœ… Table 14:  ç”¨äºå¾®è°ƒå•ä¸€é€šç”¨æ¨¡å‹ä»¥ä¾›ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°çš„æ•°æ®é›†é›†åˆã€‚ |
| âœ… âˆ— indicates using the annotations from FLD-5B , which merges original annotations with ours. | âœ… âˆ— è¡¨ç¤ºä½¿ç”¨æ¥è‡ª FLD-5B çš„æ³¨é‡Šï¼Œå®ƒå°†åŸå§‹æ³¨é‡Šä¸æˆ‘ä»¬çš„æ³¨é‡Šåˆå¹¶ã€‚ |

## 12 Appendix C Model Configuration

<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T15.2"><thead class="ltx_thead"><tr class="ltx_tr" id="A3.T15.2.1.1"><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A3.T15.2.1.1.1" rowspan="2" style="padding-left:3.7pt;padding-right:3.7pt;">Model</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4" id="A3.T15.2.1.1.2" style="padding-left:3.7pt;padding-right:3.7pt;">Image Encoder (DaViT)</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="A3.T15.2.1.1.3" style="padding-left:3.7pt;padding-right:3.7pt;">Encoder-Decoder (Transformer)</th></tr><tr class="ltx_tr" id="A3.T15.2.2.2"><th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T15.2.2.2.1" style="padding-left:3.7pt;padding-right:3.7pt;">dimensions</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T15.2.2.2.2" style="padding-left:3.7pt;padding-right:3.7pt;">blocks</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T15.2.2.2.3" style="padding-left:3.7pt;padding-right:3.7pt;">heads/groups</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="A3.T15.2.2.2.4" style="padding-left:3.7pt;padding-right:3.7pt;">#params</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T15.2.2.2.5" style="padding-left:3.7pt;padding-right:3.7pt;">encoder layers</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T15.2.2.2.6" style="padding-left:3.7pt;padding-right:3.7pt;">decoder layers</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T15.2.2.2.7" style="padding-left:3.7pt;padding-right:3.7pt;">dimensions</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T15.2.2.2.8" style="padding-left:3.7pt;padding-right:3.7pt;">#params</th></tr></thead><tbody class="ltx_tbody"><tr class="ltx_tr" id="A3.T15.2.3.1"><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T15.2.3.1.1" style="padding-left:3.7pt;padding-right:3.7pt;"><em class="ltx_emph ltx_font_italic" id="A3.T15.2.3.1.1.1" style="font-size:90%;">Florence-2-B</em></th><td class="ltx_td ltx_align_center ltx_border_t" id="A3.T15.2.3.1.2" style="padding-left:3.7pt;padding-right:3.7pt;">[128, 256, 512, 1024]</td><td class="ltx_td ltx_align_center ltx_border_t" id="A3.T15.2.3.1.3" style="padding-left:3.7pt;padding-right:3.7pt;">[1, 1, 9, 1]</td><td class="ltx_td ltx_align_center ltx_border_t" id="A3.T15.2.3.1.4" style="padding-left:3.7pt;padding-right:3.7pt;">[4, 8, 16, 32]</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T15.2.3.1.5" style="padding-left:3.7pt;padding-right:3.7pt;">90M</td><td class="ltx_td ltx_align_center ltx_border_t" id="A3.T15.2.3.1.6" style="padding-left:3.7pt;padding-right:3.7pt;">6</td><td class="ltx_td ltx_align_center ltx_border_t" id="A3.T15.2.3.1.7" style="padding-left:3.7pt;padding-right:3.7pt;">6</td><td class="ltx_td ltx_align_center ltx_border_t" id="A3.T15.2.3.1.8" style="padding-left:3.7pt;padding-right:3.7pt;">768</td><td class="ltx_td ltx_align_center ltx_border_t" id="A3.T15.2.3.1.9" style="padding-left:3.7pt;padding-right:3.7pt;">140M</td></tr><tr class="ltx_tr" id="A3.T15.2.4.2"><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A3.T15.2.4.2.1" style="padding-left:3.7pt;padding-right:3.7pt;"><em class="ltx_emph ltx_font_italic" id="A3.T15.2.4.2.1.1" style="font-size:90%;">Florence-2-L</em></th><td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T15.2.4.2.2" style="padding-left:3.7pt;padding-right:3.7pt;">[256, 512, 1024, 2048]</td><td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T15.2.4.2.3" style="padding-left:3.7pt;padding-right:3.7pt;">[1, 1, 9, 1]</td><td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T15.2.4.2.4" style="padding-left:3.7pt;padding-right:3.7pt;">[8, 16, 32, 64]</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A3.T15.2.4.2.5" style="padding-left:3.7pt;padding-right:3.7pt;">360M</td><td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T15.2.4.2.6" style="padding-left:3.7pt;padding-right:3.7pt;">12</td><td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T15.2.4.2.7" style="padding-left:3.7pt;padding-right:3.7pt;">12</td><td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T15.2.4.2.8" style="padding-left:3.7pt;padding-right:3.7pt;">1024</td><td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T15.2.4.2.9" style="padding-left:3.7pt;padding-right:3.7pt;">410M</td></tr></tbody></table>

| ã€è¡¨æ ‡é¢˜ã€‘åŸæ–‡ | ã€è¡¨æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Table 15:  Model configuration of different size. | âœ… Table 15:  ä¸åŒå°ºå¯¸çš„æ¨¡å‹é…ç½®ã€‚ |

## 13 Appendix D More Examples of Annotations in FLD-5B

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/x13.png)



| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 8:  Examples of annotations in FLD-5B. | âœ… Figure 8:  Examples of annotations in FLD-5B. |

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/x14.png)



| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 9:  Examples of annotations in FLD-5B (continued). | âœ… Figure 9:  Examples of annotations in FLD-5B (continued). |

## 14 Appendix E Qualitative Evaluation and Visualization Results

### 14.1 E.1 Detailed Image Caption

Detailed Image Caption

Prompt

: Describe with a paragraph what is shown in the image.

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/cap_1.jpg)




The image is a hand-drawn illustration of a glass of gin and tonic. The glass is filled with ice cubes, lime slices, and a slice of lemon. There is a straw sticking out of the top of the glass. The background is a light green color with a grungy texture. On the right side of the image, there is a list of ingredients written in black ink. The text reads "Gin & Tonic" and "2oz gin, 5oz tonic water, 2-4 ice cubes lime slice for garnish".


![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/cap_2.jpg)




The image shows the interior of a grocery store with a large variety of fruits and vegetables on display. The store has a high ceiling with green and white striped awnings, and the floor is made of wood. There are rows of wooden crates filled with different types of fruits, including apples, oranges, lemons, limes, and limes. The crates are arranged in neat rows, and there are price tags attached to each crate. In the background, there are shelves stocked with various items such as fruits, vegetables, and other grocery items. The overall atmosphere of the store is bright and colorful, with a sense of freshness and abundance.


![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/cap_3.jpg)




The image is a movie poster for the film "Harry Potter and the Sorcererâ€™s Stone" starring Ron Weasley. The poster features a young boy, Ron Weasley, standing in front of a dark background with a castle-like structure in the background. He is wearing a school uniform with a Gryffindor crest on his jacket and a red tie. He has blonde hair and is looking directly at the camera with a serious expression on his face. The title of the film is written in white text at the top of the poster, with the tagline "20 years of movie magic" written in smaller text below.


![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/cap_4.jpg)




The image is a digital illustration of a girl hugging a white cat. The girl is wearing a pink sweater and has long brown hair. She is sitting on a green surface with several potted plants and flowers around her. The plants have green leaves and pink and white flowers. There are also two butterflies fluttering around the scene. The background is white. The overall style of the illustration is cartoon-like and playful.


| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 10:  Detailed captioning prediction results. | âœ… Figure 10:  Detailed captioning prediction results. |

### 14.2 E.2 Visual Grounding

Visual Grounding

Prompt

: Locate the phrases in the caption: {caption}

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/grounding_6.png)




               The image shows a group of five cartoon monsters. On the left side, there is 

a brown monster

1

 with horns and a big smile on its face. Next to it, there are two 

smaller monsters

2

, one black and one green. 

The black monster

3

 has two large horns on its head and is standing in the center of the group. 

The green monster

4

 on the right side is a green monster with big eyes and a long antennae. It is standing on its hind legs with its arms stretched out to the sides. In the middle of the image, there appears to be 

a small blue monster

5

 with a round head and two antennae on its back. The background is light beige with small green circles scattered around.


![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/grounding_7.png)




                 The image shows a cluttered room with a black and white checkered 

floor

1

. On the right side of the image, there is 

a small white cabinet

2

 with a 

television

3

 on top of it. Next to the cabinet, there are 

several items

4

 scattered on the floor, including a red 

blanket

5

, 

a wooden stool

6

, and a pile of trash. On top of the cabinet is 

a picture frame

7

 and a 

hat

8

. In the center of the room is 

a white refrigerator

9

 with a few items on top. 

The walls

10

 are painted white and there are 

a few clothes

11

 hanging on a 

rack

12

 on the left wall. The room appears to be in disarray, with some items strewn about and others scattered around.


![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/grounding_8.png)




           The image shows a kitchen countertop with various kitchen items on it. On the left side of the countertop, there is a microscope with a black body and a white 

lens

1

. Next to the microscope, there are two bottles of 

condiments

2

 - one with 

a red label

3

4

 and the other with green. On top of the microscope is 

a yellow banana

5

, 

a blue spatula

6

, 

a red plate

7

, and 

a yellow corn

8

9

 on the cob. In the center of the image, there appears to be 

a frying pan

10

 with a 

fried egg

11

 on it, and on the right side is 

a white sink

12

 with a white 

faucet

13

. 

The countertop

14

 is made of wood and has a gray tile backsplash.


| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 11:  Visual grounding prediction results. | âœ… Figure 11:  Visual grounding prediction results. |

Visual Grounding

Prompt

: Locate the phrases in the caption: {caption}

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/grounding_1.png)




   The image is a flat lay of various food items arranged on a white marble countertop. On the left side of the image, there is 

a piece of salmon

1

. Next to it, there are 

slices of cheese

2

, 

a glass of oil

3

, 

coffee beans

4

, 

a zucchini

5

, a bunch of 

strawberries

6

, two 

chicken breasts

7

, 

a avocado

8

 and 

a few whole spinach leaves

9

. In the center of the table, there appears to be  

a pile of ground beef

10

 on 

paper

11

, two 

eggs

12

, two 

orange bell peppers

13

, and 

some dark chocolate bars

14

. The items are arranged in a way that suggests they are being prepared for a meal.



![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/grounding_2.png)




             The image shows a modern kitchen with a large window on the left side. 

The window

1

 has a view of trees and greenery outside. On the left side of the image, there is 

a blue sofa

2

 with a wooden coffee table in front of it. Above the table, there are 

three copper pendant lights

3

 hanging from the ceiling. There is 

a large island

4

 with a white countertop. There are 

two bar stools

5

 next to the table. In the center of the kitchen, there is 

a bottle green plants

6

 on the table. 

The floor

7

 is made of light-colored wood and 

the walls

8

 are painted in a dark blue color.


%


![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/grounding_5.png)




         The image shows a 

man

1

 standing in a kitchen with a small dog. 

The man

1

 is wearing a plaid 

shirt

2

 and 

jeans

3

 and is holding a red 

cup

4

 in his hand. 

The dog

5

 is a light brown color and is standing on a tiled 

floor

6

. 

The kitchen

7

 has wooden 

cabinets

8

 and a 

countertop

9

 with various kitchen utensils hanging on the wall. There is 

a window

10

 with yellow 

curtains

11

 in the background. On the right side of the image, there is 

a wooden cutting board

12

 and a wooden 

stool

13

.


| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 12:  Visual grounding prediction results. (continued) | âœ… Figure 12:  Visual grounding prediction results. (continued) |

### 14.3 E.3 Dense Region Caption

Dense Region Caption

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/dense_cap_1.png)



![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/dense_cap_2.png)



![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/dense_cap_3.png)



![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/dense_cap_4.png)



![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/dense_cap_5.png)



![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/dense_cap_6.png)



| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 13:  Dense region caption prediction results. | âœ… Figure 13:  Dense region caption prediction results. |

### 14.4 E.4 Open Vocabulary Detection

Open Vocabulary Object Detection

Prompt

: Locate 

Five Alive juice box

 $\langle$ 

and

 $\rangle$ 

Colgate toothpaste

 in the image.

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/od_1.png)



Prompt

: Locate 

Chewbacca

 in the image.

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/od_2.png)



Prompt

:
Locate 

giraffe

 in the image.

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/od_3.png)



Prompt

:
Locate 

Mercedes-Benz

 $\langle$ 

and

 $\rangle$ 

M2

 $\langle$ 

and

 $\rangle$ 

Audi

 in the image.

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/od_4.png)



Prompt

: Locate the 

objects with category name

 in the image.

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/od_5.png)



Prompt

: Locate the 

objects with category name

 in the image.

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/od_6.png)



| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 14:  Open vocabulary object detection prediction results. | âœ… Figure 14:  Open vocabulary object detection prediction results. |

### 14.5 E.5 OCR

Ocr with region

Prompt

: What is the text in the image, with regions?

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/ocr_1.png)



Easy Stroganoff

1

Brown - 1 lb. ground beef in skillet

2

Add - 1 can beef broth

3

1 can cream of mushroom soup

4

Cut in squares & 2dld to above -

5

1/ Boz pkg. cream cheese

6

Simmer - 20-30 min.

7

Serve over hotrice /noodles.

8

Vintage. Recipes/Easy-Stroganof

9

Charlotte Miller

10

Tulsa

11

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/ocr_4.png)



COFFEE+TEA

1

BLENDED

2

$1.69/$1.89/$2.09

3

$3.49/$3.99

4

Hot Coffee/Tea

5

Taro

6

Iced Coffee/ Tea

7

Mango

8

Hot Chocolate

9

Honeydew

10

$3,49/$ 3.99

11

Strawberry

12

Mocha

14

Thai Iced Tea / Coffee

13

Caramel

15

$1,99/$2,29/$2:59

16

SPECIALTY Brew !!

17

Jasmine GreenTea

18

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/ocr_2.png)



LEONARDO

1

DiCAPRIO

2

ROBERT

3

DE NIRO

4

LILY

5

GLADSTONE

6

A MARTIN SCORSESE PICTURE

7

KILLERS

8

OF

9

FLOWER

10

MOON

11

SCREENLY ERIC ROTH AND MARTIIN SCORSESE DIRECTED BYMARTIN SORSESE

12

ONLY IN THEATRES OCTOBER 20

13

| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 15:  OCR with region prediction results. | âœ… Figure 15:  OCR with region prediction results. |

### 14.6 E.6 Region to segmentation

Region to Segmentation

Prompt

:
What is the polygon mask of region


 $\langle$ 

loc_586

 $\rangle$ 

 $\langle$ 

loc_294

 $\rangle$ 

 $\langle$ 

loc_929

 $\rangle$ 

 $\langle$ 

loc_814

 $\rangle$ 

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/seg_1.png)



Prompt

:
What is the polygon mask of region 

 $\langle$ 

loc_317

 $\rangle$ 

 $\langle$ 

loc_314

 $\rangle$ 

 $\langle$ 

loc_893

 $\rangle$ 

 $\langle$ 

loc_904

 $\rangle$ 

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/seg_2.png)



Prompt

:
What is the polygon mask of region


 $\langle$ 

loc_541

 $\rangle$ 

 $\langle$ 

loc_266

 $\rangle$ 

 $\langle$ 

loc_692

 $\rangle$ 

 $\langle$ 

loc_627

 $\rangle$ 

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/seg_3.png)



Prompt

:
What is the polygon mask of region


 $\langle$ 

loc_583

 $\rangle$ 

 $\langle$ 

loc_66

 $\rangle$ 

 $\langle$ 

loc_794

 $\rangle$ 

 $\langle$ 

loc_331

 $\rangle$ 

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/seg_4.png)



Prompt

:
What is the polygon mask of region 

 $\langle$ 

loc_386

 $\rangle$ 

 $\langle$ 

loc_53

 $\rangle$ 

 $\langle$ 

loc_759

 $\rangle$ 

 $\langle$ 

loc_998

 $\rangle$ 

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/seg_5.png)



Prompt

:
What is the polygon mask of region 

 $\langle$ 

loc_102

 $\rangle$ 

 $\langle$ 

loc_7

 $\rangle$ 

 $\langle$ 

loc_375

 $\rangle$ 

 $\langle$ 

loc_648

 $\rangle$ 

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/pred_results/seg_6.png)



| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 16:  Region to segmentation prediction results. | âœ… Figure 16:  Region to segmentation prediction results. |

## 15 Appendix F Comparision with LMMs on Detailed Caption Task

Comparison with LMMs on Detailed Image Caption Task

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/x15.jpg)



Florence-2

: The image is a photograph of a handwritten note from a person named Angelica. The note is written in black ink on a white background. The handwriting is neat and legible, and it appears to be a handwritten letter. The personâ€™s name, Jonathan, is written at the top of the note in a cursive font. Below the name, there is a note that reads, "Iâ€™ve got to admit, Londonâ€™s lights have nothing on our family get together. I miss that, I miss you guys too. Letâ€™s not get too happy about it though Iâ€™m keeping my fingers crossed for a trip back stateside. Give a big hug to Amelia for me. Will you?"


GPT-4V

1

1

1

https://chat.openai.com

: The image shows a handwritten note addressed to Jonathan. The note conveys a personal message from the sender, Angelica, who admits to missing family gatherings and the feeling of home, contrasting it with the lights of London. Angelica expresses a longing to return stateside and asks Jonathan to give Amelia a big hug on her behalf. The tone of the note is casual and warm, with a sense of fondness and a hint of homesickness. The note is written informally, suggesting a close relationship between the sender and the recipients.


LLaVA

2

2

2

https://llava.hliu.cc

:

 The image is a handwritten letter on a piece of paper, written by a person named Jonathan. The letter is addressed to someone named Angela, and it appears to be a heartfelt message expressing love and affection. The letter is written in cursive, giving it a personal and intimate touch.


MiniGPT-4

3

3

3

https://huggingface.co/spaces/Vision-CAIR/minigpt4

:

 The image is a handwritten letter from an unknown person to someone named Angel. The letter is written in black ink on a piece of white paper. The letter is addressed to Angel and begins with the words, â€œDear Angel,â€. The letter goes on to describe the writerâ€™s feelings about a recent event or situation, but the details are unclear. The letter ends with the words, â€œSincerely, [Writerâ€™s Name].â€



| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 17:  Comparison with LMMs on detailed caption task. | âœ… Figure 17:  Comparison with LMMs on detailed caption task. |

Comparison with LMMs on Detailed Image Caption Task

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/comparison/com_5.jpg)



Florence-2

: The image is a close-up of a red apple with a green label on it. The label has the text "FUJI 94131 USA" and a barcode on it, indicating that the apple is from the United States. The apple appears to be fresh and has a shiny texture. The background is white, making the apple stand out.


GPT-4V

4

4

4

https://chat.openai.com

: The image displays a Fuji apple with a sticker indicating it is organic, along with the identifying number 94131, and specifying that it is from the USA. The apple has a reddish-pink skin with some yellow and green coloration, which is characteristic of Fuji apples. The apple is placed on a white background, which appears to be a textured fabric.


LLaVA

5

5

5

https://llava.hliu.cc

:

 The image features a large, fresh, and ripe red apple sitting on a white surface. The apple has a sticker on it, which is likely a barcode or a label providing information about the product. The apple is prominently displayed, taking up a significant portion of the image.


MiniGPT-4

6

6

6

https://huggingface.co/spaces/Vision-CAIR/minigpt4

:

 The image shows a red apple with a label on it that reads â€œOrganicâ€ in white letters. The apple is sitting on a white surface.


| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 18:  Comparison with LMMs on detailed caption task (continued). | âœ… Figure 18:  Comparison with LMMs on detailed caption task (continued). |

Comparison with LMMs on Detailed Image Caption Task

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/comparison/com_2.jpg)



Florence-2

: The image shows a highway with multiple lanes. There are several vehicles on the road, including a black van and a red truck. On the left side of the image, there is a white semi-truck with the word "Martini" written on it. Above the highway, there are several bridges with cars and trucks on them. The sky is blue and there are trees and grass on the right side. In the center of the highway is a green sign that reads "Exit 94, Chamblee Tucker Rd, Mercer Univ."


GPT-4V

7

7

7

https://chat.openai.com

: The image shows a highway scene with several lanes of traffic. There are overpasses with cars and trucks traveling over them, and the highway itself appears to be busy with vehicles, including a prominent black van in the foreground.

A road sign indicates that Exit 94 for Chamblee-Tucker Rd and Mercer University is 1 1/4 miles ahead. The presence of the sign suggests that drivers in the right lanes will soon have the opportunity to take the exit to reach these destinations.

The infrastructure suggests a well-traveled interstate or highway system, possibly in an urban or suburban area given the overpass complexity and road signage for local roads and institutions. The vehicles on the road are diverse, indicating a mix of commercial and personal transport.


LLaVA

8

8

8

https://llava.hliu.cc

:

 The image depicts a busy highway with multiple lanes of traffic, including cars and trucks. There is a large truck driving on the highway, and several other trucks are also visible in the scene. The highway is filled with cars, and there is a traffic sign above the lanes, indicating the direction to Tucker Road. The scene captures the hustle and bustle of a typical day on a highway filled with vehicles.


MiniGPT-4

9

9

9

https://huggingface.co/spaces/Vision-CAIR/minigpt4

:

 The image shows a highway with multiple lanes of traffic moving in both directions. There are several large bridges spanning the highway, and in the distance, there are mountains and a blue sky. There are also several large trucks and cars on the highway, including a black van in the center of the image. The road surface is smooth and well-maintained.


| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 19:  Comparison with LMMs on detailed caption task (continued). | âœ… Figure 19:  Comparison with LMMs on detailed caption task (continued). |

Comparison with LMMs on Detailed Image Caption Task

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/comparison/com_3.jpg)



Florence-2

: The image shows a kitchen countertop with a laptop on it. The laptop is open and the screen is displaying a blue abstract design. On the countertop, there are various kitchen utensils hanging on a metal rack. The rack is attached to the wall above the sink and has a variety of kitchen tools hanging on it, including a grater, a spatula, a ladle, a spoon, and a whisk. There are also a few other kitchen items scattered around the counter. The countertop is made of wood and there is a wire rack at the bottom of the image.


GPT-4V

10

10

10

https://chat.openai.com

: The image shows a modern kitchen setting where culinary tools are organized neatly on a rack against the wall. These tools include a variety of utensils such as a grater, tongs, a whisk, ladles, and spatulas. On the counter, there is an open laptop with a vibrant blue wallpaper on the screen, which suggests that someone might be using it for a recipe or as entertainment while cooking.
In front of the laptop, there appears to be a cutting board with some chopped ingredients and a knife, indicating that food preparation is underway or has recently taken place. There are also some dried chilies and a plastic container that might contain other ingredients or spices, further suggesting cooking activities.
The counter also has an electrical outlet and some covered sockets, and below the main counter, there are storage bins that likely hold additional kitchen supplies. The environment looks lived-in and practical, with a mix of technology and traditional cooking tools.


LLaVA

11

11

11

https://llava.hliu.cc

:

 The image features a wooden kitchen counter with a laptop computer sitting on top of it. The laptop is open and displaying a picture of a cake. The counter is also cluttered with various kitchen utensils, including multiple spoons and knives, as well as a bowl. The scene suggests that someone might be using the laptop for cooking or baking purposes, possibly following a recipe or watching a tutorial.


MiniGPT-4

12

12

12

https://huggingface.co/spaces/Vision-CAIR/minigpt4

:

 The image shows a kitchen counter with a laptop computer on it, a set of cooking utensils hanging from a rack above it, and a knife block on the counter. The laptop appears to be open, with a blue screen displaying an image of a kitchen. The utensils hanging from the rack include a spatula, a ladle, a whisk, and a cutting board. The knife block contains several knives, including a chefâ€™s knife, a paring knife, and a bread knife.


| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 20:  Comparison with LMMs on detailed caption task (continued). | âœ… Figure 20:  Comparison with LMMs on detailed caption task (continued). |

## 16 Appendix G Comarison with Kosmos-2 on Detailed Caption and Grounding Tasks

Comparison on detailed caption and grounding tasks.

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/comparison/ouput_1_kosmos2.jpg)



 The image features a home office with 

two white desks

, one on the left and the other on the right. The desks are connected by  

a white metal frame

, creating a functional and stylish workspace. 

A laptop

  is placed on the desk on the far left, and   

a keyboard

  is visible on the other desk.   

Two chairs

  are placed in front of the desks, one closer to the left desk and the second one on its right side.


(a) Result from Kosmos-2.

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/comparison/output1_fld2.jpg)




      The image shows a modern home office setup with two white 

desks

1

 and 

two black chairs

2

. 

The desks

3

 are arranged in a corner of the room with a large window on the left side. On the right side of the desk, there is 

a computer monitor

4

, a 

keyboard

5

, a mouse, 

a printer

6

, and a 

laptop

7

. Above the computer monitor and keyboard, there are 

two framed pictures

8

 hanging on the wall. 

The walls

9

 are painted in a light green color and 

the floor

10

 is made of light-colored wood. 

The floor

11

 is covered with a beige area rug with a geometric pattern. The overall style of the space is minimal and contemporary.


(b) Result from 

Florence-2

.

| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 21:  Systematic comparison with Kosmos-2Â [60] on detailed caption and grounding tasks. The models generate both the detailed caption and grounding results. The results of Kosmos-2 are from https://huggingface.co/spaces/ydshieh/Kosmos-2. | âœ… Figure 21:  Systematic comparison with Kosmos-2Â [60] on detailed caption and grounding tasks. The models generate both the detailed caption and grounding results. The results of Kosmos-2 are from https://huggingface.co/spaces/ydshieh/Kosmos-2. |

Comparison on detailed caption and grounding tasks.

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/comparison/output_2_kosmos2.jpg)



 The image features a white dining table with a variety of food items on it. On the table, there is a bowl of bread, a bowl with a salad, and a plate with a side of yogurt. In addition to the food, there are   

two cups

  placed on the table. One cup is located near the left side of the table and the other cup is situated on the right side. The table is set with a fork and a knife, ready for a meal.


(c) Result from Kosmos-2.

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/comparison/output2_fld2.jpg)




  The image shows a table with a basket of bread and a plate of 

salad

1

. 

The basket

2

 is made of woven straw and has several slices of bread in it. Next to the basket, there is 

a small bowl

1

 of salad with a variety of vegetables, including 

lettuce

3

, 

tomatoes

4

, 

cucumbers

5

, and 

feta cheese

6

. There are also 

two salt

7

 and pepper 

shakers

7

 on the table. On the right side of the table, there are 

two white plates

8

9

 with a dollop of white sauce on them. The table is covered with a white tablecloth and there are 

a few other dishes

9

 and 

utensils

10

 scattered around. 

A person"s hand

11

 can be seen in the top right corner of the image.


(d) Result from 

Florence-2

.

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/comparison/output_3_kosmos2.jpg)



  The image features a family of   

elephants

  walking together in a lush green forest. The  

elephants

 are walking in a line, with one of them walking in front of the rest of the family. The family consists of a mother and her two calves, with the mother and the two calves walking side by side. The forest is filled with green trees, adding to the natural beauty of the scene.


(e) Result from Kosmos-2.

![figure](https://ar5iv.labs.arxiv.org/html/2311.06242/assets/figures/appendix/comparison/output3_fld2.jpg)




  The image shows a group of three elephants standing in a dirt field with trees and bushes in the background. 

The elephants

1

 are standing close together, with the largest elephant in the center and two smaller ones on either side. 

The largest elephant

1

2

  on the left is standing with its 

trunk

3

 extended, while 

the smaller one

4

 is standing next to it. 

All three elephants

1

 have 

tusks

5

 and appear to be in their natural habitat. 

The ground

6

 is covered in dirt and there is 

a small pile of dirt

7

 in front of them. The overall mood of the image is peaceful and serene.


(f) Result from 

Florence-2

.

| ã€å›¾æ ‡é¢˜ã€‘åŸæ–‡ | ã€å›¾æ ‡é¢˜ã€‘ç¿»è¯‘ |
| ---- | ---- |
| âœ… Figure 22:  Systematic comparison with Kosmos-2Â [60] on detailed caption and grounding tasks. The models generate both the detailed caption and grounding results. The results of Kosmos-2 are from https://huggingface.co/spaces/ydshieh/Kosmos-2. (continued) | âœ… Figure 22:  Systematic comparison with Kosmos-2Â [60] on detailed caption and grounding tasks. The models generate both the detailed caption and grounding results. The results of Kosmos-2 are from https://huggingface.co/spaces/ydshieh/Kosmos-2. (continued) |